# Study of the effects of pre-trained neural network sparsity and dispersion methods

Applying pruning on a neural network decreases the complexity of the internal representation, thus allowing an increase in the speed of training and inference. In addition, being a simpler model, it can improve its generalization capacity, which is why applying pruning techniques on pre-trained networks can help to improve the application of transfer learning and achieve better models.

By reducing the complexity of the model, we also reduce the amount of parameters needed to train on the new task and therefore reduce the amount of VRAM memory needed, facilitating the application of transfer learning on devices with lower computational capacity.

Our work will be based on a hypothesis described in the study and we will conduct experiments to determine its veracity.