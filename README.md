# Study of the effects of pre-trained neural network sparsity and dispersion methods

Applying pruning on a neural network decreases the complexity of the inter- nal representation, allowing an increase in the speed of training and inference. In addition, producing a simpler model, it can improve its generalization capacity, which is why applying pruning techniques on pre-trained neural networks can help improve the application of transfer learning and achieve better models. 

By reducing the complexity of the model, we also reduce the number of pa- rameters needed to train on the new task, therefore reducing the computational capacity required, and improving the application of transfer learning. 

Our work will be based on applying multiple pruning methods on pre-trained models in order to know how it affects the application of transfer learning.
