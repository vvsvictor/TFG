{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gW7H6FygFyzr"
   },
   "source": [
    "Importaciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l5NI0wgQFkRD"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import copy\n",
    "import time\n",
    "import pandas as pd\n",
    "from torchvision.io import read_image\n",
    "from torchvision.datasets.folder import default_loader\n",
    "from torchvision.datasets.utils import download_url\n",
    "from torch.utils.data import Dataset\n",
    "from torch import autograd\n",
    "import torchvision.transforms as T\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision.models import vgg16\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import torch.nn.utils.prune as prune\n",
    "from heapq import nsmallest\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MYXA-zOLeuHO"
   },
   "outputs": [],
   "source": [
    "import nni\n",
    "from nni.compression.pytorch.pruning import MovementPruner\n",
    "from nni.compression.pytorch import TorchEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7WaQVjQOFya-",
    "outputId": "48f6fda0-f5ef-417c-aeaf-24ba8b92a91d"
   },
   "outputs": [],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clean vram\n",
    "torch.cuda.empty_cache() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XhYfK8f5F5aM",
    "outputId": "557dfbab-21c0-4504-91fb-e84c6fd8676d"
   },
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bhWM4ImbF7MQ"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qF0S6lUiGZYH"
   },
   "source": [
    "Función de carga del dataset CUB (El dataset se descarga automáticamente al ejecutar las siguientes celdas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ux5xbkoEGEIJ"
   },
   "outputs": [],
   "source": [
    "class Cub2011(Dataset):\n",
    "    base_folder = 'CUB_200_2011/images'\n",
    "    url = 'https://data.caltech.edu/records/65de6-vp158/files/CUB_200_2011.tgz?download=1'\n",
    "    filename = 'CUB_200_2011.tgz'\n",
    "    tgz_md5 = '97eceeb196236b17998738112f37df78'\n",
    "\n",
    "    def __init__(self, root, train=True, transform=None, loader=default_loader, download=True):\n",
    "        self.root = os.path.expanduser(root)\n",
    "        self.transform = transform\n",
    "        self.loader = default_loader\n",
    "        self.train = train\n",
    "\n",
    "        if download:\n",
    "            self._download()\n",
    "\n",
    "        if not self._check_integrity():\n",
    "            raise RuntimeError('Dataset not found or corrupted.' +\n",
    "                               ' You can use download=True to download it')\n",
    "\n",
    "    def _load_metadata(self):\n",
    "        images = pd.read_csv(os.path.join(self.root, 'CUB_200_2011', 'images.txt'), sep=' ',\n",
    "                             names=['img_id', 'filepath'])\n",
    "        image_class_labels = pd.read_csv(os.path.join(self.root, 'CUB_200_2011', 'image_class_labels.txt'),\n",
    "                                         sep=' ', names=['img_id', 'target'])\n",
    "        train_test_split = pd.read_csv(os.path.join(self.root, 'CUB_200_2011', 'train_test_split.txt'),\n",
    "                                       sep=' ', names=['img_id', 'is_training_img'])\n",
    "\n",
    "        data = images.merge(image_class_labels, on='img_id')\n",
    "        self.data = data.merge(train_test_split, on='img_id')\n",
    "\n",
    "        if self.train:\n",
    "            self.data = self.data[self.data.is_training_img == 1]\n",
    "        else:\n",
    "            self.data = self.data[self.data.is_training_img == 0]\n",
    "\n",
    "    def _check_integrity(self):\n",
    "        try:\n",
    "            self._load_metadata()\n",
    "        except Exception:\n",
    "            return False\n",
    "\n",
    "        for index, row in self.data.iterrows():\n",
    "            filepath = os.path.join(self.root, self.base_folder, row.filepath)\n",
    "            if not os.path.isfile(filepath):\n",
    "                print(filepath)\n",
    "                return False\n",
    "        return True\n",
    "\n",
    "    def _download(self):\n",
    "        import tarfile\n",
    "\n",
    "        if self._check_integrity():\n",
    "            print('Files already downloaded and verified')\n",
    "            return\n",
    "\n",
    "        download_url(self.url, self.root, self.filename, self.tgz_md5)\n",
    "\n",
    "        with tarfile.open(os.path.join(self.root, self.filename), \"r:gz\") as tar:\n",
    "            tar.extractall(path=self.root)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.data.iloc[idx]\n",
    "        path = os.path.join(self.root, self.base_folder, sample.filepath)\n",
    "        target = sample.target - 1 \n",
    "        img = self.loader(path)\n",
    "\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        return img, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "15JoU_PQGjAO"
   },
   "outputs": [],
   "source": [
    "transform = T.Compose([\n",
    "    T.RandomResizedCrop(224),\n",
    "    T.RandomHorizontalFlip(),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Xb6IhrWfGtih",
    "outputId": "ad74cb4f-82b1-40fb-c062-6ccf3e802dcf"
   },
   "outputs": [],
   "source": [
    "train_ds = Cub2011('.', train=True, transform = transform)\n",
    "val_ds = Cub2011('.s', train=False, transform = transform)\n",
    "\n",
    "ds = {'train': DataLoader(train_ds, batch_size = BATCH_SIZE, shuffle=True),\n",
    "      'val': DataLoader(val_ds, batch_size = BATCH_SIZE, shuffle=False)}\n",
    "\n",
    "\n",
    "ds_sizes = {'train': len(train_ds),\n",
    "      'val': len(val_ds)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1zGK0VmaH9pF"
   },
   "source": [
    "Función para mostrar los parámetros de la capa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6FCSNfOOIT-K"
   },
   "source": [
    "Carga el modelo preentrenado (He utilizado una VGG16 para simplificar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f1WkQ3ujHP9y"
   },
   "outputs": [],
   "source": [
    "model = vgg16(weights='IMAGENET1K_V1')\n",
    "model.classifier[6] = nn.Linear(4096, 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "397fgKE6qqEN"
   },
   "outputs": [],
   "source": [
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zxNK32Ku6K_2"
   },
   "outputs": [],
   "source": [
    "val_bal_acc = []\n",
    "val_acc = []\n",
    "val_loss = []\n",
    "\n",
    "train_bal_acc = []\n",
    "train_acc = []\n",
    "train_loss = []\n",
    "NCLAS = 200\n",
    "\n",
    "\n",
    "def training_model(model, optimizer, criterion, lr_scheduler, max_steps, max_epochs, *args, **kwargs):\n",
    "  for epoch in range(max_epochs):\n",
    "    for phase in ['train', 'val']:\n",
    "      if phase == 'train':\n",
    "        model.train() \n",
    "      else:\n",
    "        model.eval() \n",
    "      running_loss = 0.0\n",
    "      running_corrects = 0\n",
    "      CF = np.zeros((NCLAS,NCLAS)) # Confusion matrix\n",
    "      for inputs,labels in ds[phase]:\n",
    "          inputs, labels = inputs.to(device), labels.to(device)\n",
    "          optimizer.zero_grad()\n",
    "          with torch.set_grad_enabled(phase == 'train'):\n",
    "            output = model(inputs)\n",
    "            _, preds = torch.max(output, 1)\n",
    "            loss = criterion(output, labels)\n",
    "          if phase == 'train':\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "          running_loss += loss.item() * inputs.size(0)\n",
    "          running_corrects += torch.sum(preds == labels.data)\n",
    "          for i in range(len(labels.data)):\n",
    "            CF[labels.data[i]][preds[i]] +=1\n",
    "      if phase == 'train':\n",
    "        lr_scheduler.step()\n",
    "      epoch_loss = running_loss / ds_sizes[phase]\n",
    "      epoch_acc = running_corrects.double() / ds_sizes[phase]\n",
    "      recalli = 0\n",
    "      for i in range(NCLAS):\n",
    "          TP = CF[i][i]\n",
    "          FN = 0\n",
    "          for j in range(NCLAS):\n",
    "              if i!=j:\n",
    "                  FN+=CF[i][j]\n",
    "          if (TP+FN) !=0:\n",
    "              recalli+= TP/(TP+FN)\n",
    "      epoch_bal_acc = recalli/NCLAS\n",
    "      \n",
    "      if phase == 'val':\n",
    "          val_bal_acc.append(epoch_bal_acc)\n",
    "          val_acc.append(epoch_acc)\n",
    "          val_loss.append(epoch_loss)\n",
    "          print(f'Val Acc: {epoch_acc:.4f}')\n",
    "      else:\n",
    "          train_bal_acc.append(epoch_bal_acc)\n",
    "          train_acc.append(epoch_acc)\n",
    "          train_loss.append(epoch_loss)\n",
    "          print(f'Train Acc: {epoch_acc:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7glwjZiueuHW",
    "outputId": "d2baa36c-3180-45e4-dc92-44bf550e67d1"
   },
   "outputs": [],
   "source": [
    "\n",
    "traced_optimizer = nni.trace(optim.SGD)(model.parameters(), lr=0.001, momentum=0.9)\n",
    "config_list = [{'op_types': ['Conv2d','Linear'], \n",
    "'sparsity_per_layer': 0.2}]\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "lr_scheduler = nni.trace(torch.optim.lr_scheduler.StepLR)(traced_optimizer, step_size=7, gamma=0.1)\n",
    "evaluator = TorchEvaluator(training_func=training_model, optimizers=traced_optimizer,criterion=criterion, lr_schedulers=lr_scheduler)\n",
    "# warm_up_step – The total optimizer.step() number before start pruning for warm up. Make sure warm_up_step is smaller than cool_down_beginning_step.\n",
    "# cool_down_beginning_step – The number of steps at which sparsity stops growing, note that the sparsity stop growing doesn’t mean masks not changed.\n",
    "warm_up_step = len(train_ds) // BATCH_SIZE * 6\n",
    "cool_down_begin_step = len(train_ds) // BATCH_SIZE * 8\n",
    "pruner = MovementPruner(model, config_list, evaluator, warm_up_step=warm_up_step, cool_down_beginning_step=cool_down_begin_step, training_epochs=70,  movement_mode='hard')\n",
    "_, masks = pruner.compress()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "gpuClass": "premium",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7 (tags/v3.10.7:6cc6b13, Sep  5 2022, 14:08:36) [MSC v.1933 64 bit (AMD64)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "7cb176315d1b334180b6af9fe4839184c771682056334f245fd0d7c99b98cf46"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
