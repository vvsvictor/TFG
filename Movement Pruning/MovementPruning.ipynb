{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gW7H6FygFyzr"
   },
   "source": [
    "Importaciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "l5NI0wgQFkRD"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Victor\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import copy\n",
    "import time\n",
    "import pandas as pd\n",
    "from torchvision.io import read_image\n",
    "from torchvision.datasets.folder import default_loader\n",
    "from torchvision.datasets.utils import download_url\n",
    "from torch.utils.data import Dataset\n",
    "from torch import autograd\n",
    "import torchvision.transforms as T\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision.models import vgg16\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import torch.nn.utils.prune as prune\n",
    "from heapq import nsmallest\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "MYXA-zOLeuHO"
   },
   "outputs": [],
   "source": [
    "import nni\n",
    "from nni.compression.pytorch.pruning import MovementPruner\n",
    "from nni.compression.pytorch import TorchEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7WaQVjQOFya-",
    "outputId": "48f6fda0-f5ef-417c-aeaf-24ba8b92a91d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clean vram\n",
    "torch.cuda.empty_cache() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XhYfK8f5F5aM",
    "outputId": "557dfbab-21c0-4504-91fb-e84c6fd8676d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Nov 14 23:09:42 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 522.06       Driver Version: 522.06       CUDA Version: 11.8     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name            TCC/WDDM | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA GeForce ... WDDM  | 00000000:04:00.0  On |                  N/A |\n",
      "| 44%   31C    P8    13W /  95W |    320MiB /  2048MiB |      1%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  Tesla M40 24GB     TCC   | 00000000:2B:00.0 Off |           1112277029 |\n",
      "| N/A   37C    P8    16W / 250W |      9MiB / 23040MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A      1476    C+G   ...s\\LibreWolf\\librewolf.exe    N/A      |\n",
      "|    0   N/A  N/A      2260    C+G   ...e\\PhoneExperienceHost.exe    N/A      |\n",
      "|    0   N/A  N/A      7012    C+G   C:\\Windows\\explorer.exe         N/A      |\n",
      "|    0   N/A  N/A      7504    C+G   ...ge\\Application\\msedge.exe    N/A      |\n",
      "|    0   N/A  N/A      8244    C+G   ...5n1h2txyewy\\SearchApp.exe    N/A      |\n",
      "|    0   N/A  N/A      9456    C+G   ...2txyewy\\TextInputHost.exe    N/A      |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "bhWM4ImbF7MQ"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qF0S6lUiGZYH"
   },
   "source": [
    "Función de carga del dataset CUB (El dataset se descarga automáticamente al ejecutar las siguientes celdas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "Ux5xbkoEGEIJ"
   },
   "outputs": [],
   "source": [
    "class Cub2011(Dataset):\n",
    "    base_folder = 'CUB_200_2011/images'\n",
    "    url = 'https://data.caltech.edu/records/65de6-vp158/files/CUB_200_2011.tgz?download=1'\n",
    "    filename = 'CUB_200_2011.tgz'\n",
    "    tgz_md5 = '97eceeb196236b17998738112f37df78'\n",
    "\n",
    "    def __init__(self, root, train=True, transform=None, loader=default_loader, download=True):\n",
    "        self.root = os.path.expanduser(root)\n",
    "        self.transform = transform\n",
    "        self.loader = default_loader\n",
    "        self.train = train\n",
    "\n",
    "        if download:\n",
    "            self._download()\n",
    "\n",
    "        if not self._check_integrity():\n",
    "            raise RuntimeError('Dataset not found or corrupted.' +\n",
    "                               ' You can use download=True to download it')\n",
    "\n",
    "    def _load_metadata(self):\n",
    "        images = pd.read_csv(os.path.join(self.root, 'CUB_200_2011', 'images.txt'), sep=' ',\n",
    "                             names=['img_id', 'filepath'])\n",
    "        image_class_labels = pd.read_csv(os.path.join(self.root, 'CUB_200_2011', 'image_class_labels.txt'),\n",
    "                                         sep=' ', names=['img_id', 'target'])\n",
    "        train_test_split = pd.read_csv(os.path.join(self.root, 'CUB_200_2011', 'train_test_split.txt'),\n",
    "                                       sep=' ', names=['img_id', 'is_training_img'])\n",
    "\n",
    "        data = images.merge(image_class_labels, on='img_id')\n",
    "        self.data = data.merge(train_test_split, on='img_id')\n",
    "\n",
    "        if self.train:\n",
    "            self.data = self.data[self.data.is_training_img == 1]\n",
    "        else:\n",
    "            self.data = self.data[self.data.is_training_img == 0]\n",
    "\n",
    "    def _check_integrity(self):\n",
    "        try:\n",
    "            self._load_metadata()\n",
    "        except Exception:\n",
    "            return False\n",
    "\n",
    "        for index, row in self.data.iterrows():\n",
    "            filepath = os.path.join(self.root, self.base_folder, row.filepath)\n",
    "            if not os.path.isfile(filepath):\n",
    "                print(filepath)\n",
    "                return False\n",
    "        return True\n",
    "\n",
    "    def _download(self):\n",
    "        import tarfile\n",
    "\n",
    "        if self._check_integrity():\n",
    "            print('Files already downloaded and verified')\n",
    "            return\n",
    "\n",
    "        download_url(self.url, self.root, self.filename, self.tgz_md5)\n",
    "\n",
    "        with tarfile.open(os.path.join(self.root, self.filename), \"r:gz\") as tar:\n",
    "            tar.extractall(path=self.root)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.data.iloc[idx]\n",
    "        path = os.path.join(self.root, self.base_folder, sample.filepath)\n",
    "        target = sample.target - 1 \n",
    "        img = self.loader(path)\n",
    "\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        return img, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "15JoU_PQGjAO"
   },
   "outputs": [],
   "source": [
    "transform = T.Compose([\n",
    "    T.RandomResizedCrop(224),\n",
    "    T.RandomHorizontalFlip(),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Xb6IhrWfGtih",
    "outputId": "ad74cb4f-82b1-40fb-c062-6ccf3e802dcf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "train_ds = Cub2011('.', train=True, transform = transform)\n",
    "val_ds = Cub2011('.s', train=False, transform = transform)\n",
    "\n",
    "ds = {'train': DataLoader(train_ds, batch_size = BATCH_SIZE, shuffle=True),\n",
    "      'val': DataLoader(val_ds, batch_size = BATCH_SIZE, shuffle=False)}\n",
    "\n",
    "\n",
    "ds_sizes = {'train': len(train_ds),\n",
    "      'val': len(val_ds)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1zGK0VmaH9pF"
   },
   "source": [
    "Función para mostrar los parámetros de la capa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6FCSNfOOIT-K"
   },
   "source": [
    "Carga el modelo preentrenado (He utilizado una VGG16 para simplificar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "f1WkQ3ujHP9y"
   },
   "outputs": [],
   "source": [
    "model = vgg16(weights='IMAGENET1K_V1')\n",
    "model.classifier[6] = nn.Linear(4096, 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "397fgKE6qqEN"
   },
   "outputs": [],
   "source": [
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "zxNK32Ku6K_2"
   },
   "outputs": [],
   "source": [
    "val_bal_acc = []\n",
    "val_acc = []\n",
    "val_loss = []\n",
    "\n",
    "train_bal_acc = []\n",
    "train_acc = []\n",
    "train_loss = []\n",
    "NCLAS = 200\n",
    "\n",
    "\n",
    "def training_model(model, optimizer, criterion, lr_scheduler, max_steps, max_epochs, *args, **kwargs):\n",
    "  for epoch in range(max_epochs):\n",
    "    for phase in ['train', 'val']:\n",
    "      if phase == 'train':\n",
    "        model.train() \n",
    "      else:\n",
    "        model.eval() \n",
    "      running_loss = 0.0\n",
    "      running_corrects = 0\n",
    "      CF = np.zeros((NCLAS,NCLAS)) # Confusion matrix\n",
    "      for inputs,labels in ds[phase]:\n",
    "          inputs, labels = inputs.to(device), labels.to(device)\n",
    "          optimizer.zero_grad()\n",
    "          with torch.set_grad_enabled(phase == 'train'):\n",
    "            output = model(inputs)\n",
    "            _, preds = torch.max(output, 1)\n",
    "            loss = criterion(output, labels)\n",
    "          if phase == 'train':\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "          running_loss += loss.item() * inputs.size(0)\n",
    "          running_corrects += torch.sum(preds == labels.data)\n",
    "          for i in range(len(labels.data)):\n",
    "            CF[labels.data[i]][preds[i]] +=1\n",
    "      if phase == 'train':\n",
    "        lr_scheduler.step()\n",
    "      epoch_loss = running_loss / ds_sizes[phase]\n",
    "      epoch_acc = running_corrects.double() / ds_sizes[phase]\n",
    "      recalli = 0\n",
    "      for i in range(NCLAS):\n",
    "          TP = CF[i][i]\n",
    "          FN = 0\n",
    "          for j in range(NCLAS):\n",
    "              if i!=j:\n",
    "                  FN+=CF[i][j]\n",
    "          if (TP+FN) !=0:\n",
    "              recalli+= TP/(TP+FN)\n",
    "      epoch_bal_acc = recalli/NCLAS\n",
    "      \n",
    "      if phase == 'val':\n",
    "          val_bal_acc.append(epoch_bal_acc)\n",
    "          val_acc.append(epoch_acc)\n",
    "          val_loss.append(epoch_loss)\n",
    "          print(f'Val Acc: {epoch_acc:.4f}')\n",
    "      else:\n",
    "          train_bal_acc.append(epoch_bal_acc)\n",
    "          train_acc.append(epoch_acc)\n",
    "          train_loss.append(epoch_loss)\n",
    "          print(f'Train Acc: {epoch_acc:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7glwjZiueuHW",
    "outputId": "d2baa36c-3180-45e4-dc92-44bf550e67d1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022-11-14 23:09:52] \u001b[33mWARNING: Did not bind any model, no need to unbind model.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Victor\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:124: UserWarning: Seems like `optimizer.step()` has been overridden after learning rate scheduler initialization. Please, make sure to call `optimizer.step()` before `lr_scheduler.step()`. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Seems like `optimizer.step()` has been overridden after learning rate scheduler \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Acc: 0.1039\n",
      "Val Acc: 0.2931\n",
      "Train Acc: 0.3205\n",
      "Val Acc: 0.4292\n",
      "Train Acc: 0.4266\n",
      "Val Acc: 0.4726\n",
      "Train Acc: 0.4985\n",
      "Val Acc: 0.5140\n",
      "Train Acc: 0.5367\n",
      "Val Acc: 0.5268\n",
      "Train Acc: 0.5811\n",
      "Val Acc: 0.5154\n",
      "Train Acc: 0.3894\n",
      "Val Acc: 0.3811\n",
      "Train Acc: 0.4583\n",
      "Val Acc: 0.4760\n",
      "Train Acc: 0.5325\n",
      "Val Acc: 0.4972\n",
      "Train Acc: 0.5649\n",
      "Val Acc: 0.5302\n",
      "Train Acc: 0.5787\n",
      "Val Acc: 0.5352\n",
      "Train Acc: 0.6138\n",
      "Val Acc: 0.5525\n",
      "Train Acc: 0.6360\n",
      "Val Acc: 0.5497\n",
      "Train Acc: 0.6456\n",
      "Val Acc: 0.5333\n",
      "Train Acc: 0.6368\n",
      "Val Acc: 0.5557\n",
      "Train Acc: 0.6713\n",
      "Val Acc: 0.5677\n",
      "Train Acc: 0.6770\n",
      "Val Acc: 0.5739\n",
      "Train Acc: 0.6850\n",
      "Val Acc: 0.5784\n",
      "Train Acc: 0.6854\n",
      "Val Acc: 0.5958\n",
      "Train Acc: 0.6947\n",
      "Val Acc: 0.6061\n",
      "Train Acc: 0.7047\n",
      "Val Acc: 0.6001\n",
      "Train Acc: 0.7189\n",
      "Val Acc: 0.5949\n",
      "Train Acc: 0.7177\n",
      "Val Acc: 0.5970\n",
      "Train Acc: 0.7177\n",
      "Val Acc: 0.5994\n",
      "Train Acc: 0.7289\n",
      "Val Acc: 0.5992\n",
      "Train Acc: 0.7394\n",
      "Val Acc: 0.5875\n",
      "Train Acc: 0.7332\n",
      "Val Acc: 0.6105\n",
      "Train Acc: 0.7574\n",
      "Val Acc: 0.6072\n",
      "Train Acc: 0.7664\n",
      "Val Acc: 0.6179\n",
      "Train Acc: 0.7452\n",
      "Val Acc: 0.6111\n",
      "Train Acc: 0.7583\n",
      "Val Acc: 0.6165\n",
      "Train Acc: 0.7639\n",
      "Val Acc: 0.6137\n",
      "Train Acc: 0.7696\n",
      "Val Acc: 0.6143\n",
      "Train Acc: 0.7783\n",
      "Val Acc: 0.6001\n",
      "Train Acc: 0.7688\n",
      "Val Acc: 0.5948\n",
      "Train Acc: 0.7783\n",
      "Val Acc: 0.6231\n",
      "Train Acc: 0.7871\n",
      "Val Acc: 0.6262\n",
      "Train Acc: 0.7866\n",
      "Val Acc: 0.6360\n",
      "Train Acc: 0.7843\n",
      "Val Acc: 0.6212\n",
      "Train Acc: 0.7850\n",
      "Val Acc: 0.6172\n",
      "Train Acc: 0.7975\n",
      "Val Acc: 0.6346\n",
      "Train Acc: 0.7983\n",
      "Val Acc: 0.6210\n",
      "Train Acc: 0.7976\n",
      "Val Acc: 0.6258\n",
      "Train Acc: 0.8036\n",
      "Val Acc: 0.6326\n",
      "Train Acc: 0.8013\n",
      "Val Acc: 0.6350\n",
      "Train Acc: 0.8070\n",
      "Val Acc: 0.6275\n",
      "Train Acc: 0.8133\n",
      "Val Acc: 0.6227\n",
      "Train Acc: 0.8090\n",
      "Val Acc: 0.6282\n",
      "Train Acc: 0.8101\n",
      "Val Acc: 0.6156\n",
      "Train Acc: 0.8150\n",
      "Val Acc: 0.6374\n",
      "Train Acc: 0.8138\n",
      "Val Acc: 0.6457\n",
      "Train Acc: 0.8141\n",
      "Val Acc: 0.6267\n",
      "Train Acc: 0.8275\n",
      "Val Acc: 0.6289\n",
      "Train Acc: 0.8240\n",
      "Val Acc: 0.6193\n",
      "Train Acc: 0.8190\n",
      "Val Acc: 0.6293\n"
     ]
    }
   ],
   "source": [
    "\n",
    "traced_optimizer = nni.trace(optim.SGD)(model.parameters(), lr=0.001, momentum=0.9)\n",
    "# Operation types to be pruned and operation partial names to be pruned in vgg16\n",
    "#Prune 10% of the filters \n",
    "config_list = [{'op_types': ['Conv2d','Linear'], \n",
    "'sparsity_per_layer': 0.2}]\n",
    "#criterion (Callable[[Tensor, Tensor], Tensor]) – The criterion function used in trainer. Take model output and target value as input, and return the loss.\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "lr_scheduler = nni.trace(torch.optim.lr_scheduler.StepLR)(traced_optimizer, step_size=7, gamma=0.1)\n",
    "evaluator = TorchEvaluator(training_func=training_model, optimizers=traced_optimizer,criterion=criterion, lr_schedulers=lr_scheduler)\n",
    "# warm_up_step – The total optimizer.step() number before start pruning for warm up. Make sure warm_up_step is smaller than cool_down_beginning_step.\n",
    "# cool_down_beginning_step – The number of steps at which sparsity stops growing, note that the sparsity stop growing doesn’t mean masks not changed.\n",
    "warm_up_step = len(train_ds) // BATCH_SIZE * 6\n",
    "cool_down_begin_step = len(train_ds) // BATCH_SIZE * 8\n",
    "pruner = MovementPruner(model, config_list, evaluator, warm_up_step=warm_up_step, cool_down_beginning_step=cool_down_begin_step, training_epochs=70,  movement_mode='hard')\n",
    "\n",
    "_, masks = pruner.compress()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XFe3sysBjpOD",
    "outputId": "16c0687d-ce40-441c-996c-7578f8bd334d"
   },
   "outputs": [],
   "source": [
    "print('Validation:')\n",
    "print('Val_bal_acc:', val_bal_acc)\n",
    "print('Val_acc:', val_acc)\n",
    "print('Val_loss:', val_loss)\n",
    "\n",
    "print('Training:')\n",
    "print('Train_bal_acc:', train_bal_acc)\n",
    "print('Train_acc:', train_acc)\n",
    "print('Train_loss:', train_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KEpXuwUWeuHY"
   },
   "outputs": [],
   "source": [
    "model = vgg16(weights='IMAGENET1K_V1')\n",
    "model.classifier[6] = nn.Linear(4096, 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clean vram\n",
    "torch.cuda.empty_cache() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_bal_acc = []\n",
    "val_acc = []\n",
    "val_loss = []\n",
    "\n",
    "train_bal_acc = []\n",
    "train_acc = []\n",
    "train_loss = []\n",
    "NCLAS = 200\n",
    "\n",
    "\n",
    "def training_model(model, optimizer, criterion, lr_scheduler, max_steps, max_epochs, *args, **kwargs):\n",
    "  for epoch in range(max_epochs):\n",
    "    for phase in ['train', 'val']:\n",
    "      if phase == 'train':\n",
    "        model.train() \n",
    "      else:\n",
    "        model.eval() \n",
    "      running_loss = 0.0\n",
    "      running_corrects = 0\n",
    "      CF = np.zeros((NCLAS,NCLAS)) # Confusion matrix\n",
    "      for inputs,labels in ds[phase]:\n",
    "          inputs, labels = inputs.to(device), labels.to(device)\n",
    "          optimizer.zero_grad()\n",
    "          with torch.set_grad_enabled(phase == 'train'):\n",
    "            output = model(inputs)\n",
    "            _, preds = torch.max(output, 1)\n",
    "            loss = criterion(output, labels)\n",
    "          if phase == 'train':\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "          running_loss += loss.item() * inputs.size(0)\n",
    "          running_corrects += torch.sum(preds == labels.data)\n",
    "          for i in range(len(labels.data)):\n",
    "            CF[labels.data[i]][preds[i]] +=1\n",
    "      if phase == 'train':\n",
    "        lr_scheduler.step()\n",
    "      epoch_loss = running_loss / ds_sizes[phase]\n",
    "      epoch_acc = running_corrects.double() / ds_sizes[phase]\n",
    "      recalli = 0\n",
    "      for i in range(NCLAS):\n",
    "          TP = CF[i][i]\n",
    "          FN = 0\n",
    "          for j in range(NCLAS):\n",
    "              if i!=j:\n",
    "                  FN+=CF[i][j]\n",
    "          if (TP+FN) !=0:\n",
    "              recalli+= TP/(TP+FN)\n",
    "      epoch_bal_acc = recalli/NCLAS\n",
    "      \n",
    "      if phase == 'val':\n",
    "          val_bal_acc.append(epoch_bal_acc)\n",
    "          val_acc.append(epoch_acc)\n",
    "          val_loss.append(epoch_loss)\n",
    "          print(f'Val Acc: {epoch_acc:.4f}')\n",
    "      else:\n",
    "          train_bal_acc.append(epoch_bal_acc)\n",
    "          train_acc.append(epoch_acc)\n",
    "          train_loss.append(epoch_loss)\n",
    "          print(f'Train Acc: {epoch_acc:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "traced_optimizer = nni.trace(optim.SGD)(model.parameters(), lr=0.001, momentum=0.9)\n",
    "# Operation types to be pruned and operation partial names to be pruned in vgg16\n",
    "#Prune 10% of the filters \n",
    "config_list = [{'op_types': ['Conv2d','Linear'], \n",
    "'sparsity_per_layer': 0.3}]\n",
    "#criterion (Callable[[Tensor, Tensor], Tensor]) – The criterion function used in trainer. Take model output and target value as input, and return the loss.\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "lr_scheduler = nni.trace(torch.optim.lr_scheduler.StepLR)(traced_optimizer, step_size=7, gamma=0.1)\n",
    "evaluator = TorchEvaluator(training_func=training_model, optimizers=traced_optimizer,criterion=criterion, lr_schedulers=lr_scheduler)\n",
    "# warm_up_step – The total optimizer.step() number before start pruning for warm up. Make sure warm_up_step is smaller than cool_down_beginning_step.\n",
    "# cool_down_beginning_step – The number of steps at which sparsity stops growing, note that the sparsity stop growing doesn’t mean masks not changed.\n",
    "warm_up_step = len(train_ds) // BATCH_SIZE * 6\n",
    "cool_down_begin_step = len(train_ds) // BATCH_SIZE * 8\n",
    "pruner = MovementPruner(model, config_list, evaluator, warm_up_step=warm_up_step, cool_down_beginning_step=cool_down_begin_step, training_epochs=70,  movement_mode='hard')\n",
    "\n",
    "_, masks = pruner.compress()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Validation:')\n",
    "print('Val_bal_acc:', val_bal_acc)\n",
    "print('Val_acc:', val_acc)\n",
    "print('Val_loss:', val_loss)\n",
    "\n",
    "print('Training:')\n",
    "print('Train_bal_acc:', train_bal_acc)\n",
    "print('Train_acc:', train_acc)\n",
    "print('Train_loss:', train_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "gpuClass": "premium",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "7cb176315d1b334180b6af9fe4839184c771682056334f245fd0d7c99b98cf46"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
