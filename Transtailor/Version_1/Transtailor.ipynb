{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gW7H6FygFyzr"
   },
   "source": [
    "Importaciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l5NI0wgQFkRD"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import copy\n",
    "import time\n",
    "import pandas as pd\n",
    "from torchvision.io import read_image\n",
    "from torchvision.datasets.folder import default_loader\n",
    "from torchvision.datasets.utils import download_url\n",
    "from torch.utils.data import Dataset\n",
    "import torchvision.transforms as T\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision.models import vgg16\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import torch.nn.utils.prune as prune\n",
    "from heapq import nsmallest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6yihMHI_GBUX"
   },
   "source": [
    "Activar acceleración por hardware -> GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7WaQVjQOFya-",
    "outputId": "d72a12d8-584d-4566-fb85-ebe14e707f97"
   },
   "outputs": [],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XhYfK8f5F5aM",
    "outputId": "6466241f-85ec-4e65-cb2a-54461ab6b238"
   },
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bhWM4ImbF7MQ"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qF0S6lUiGZYH"
   },
   "source": [
    "Función de carga del dataset CUB (El dataset se descarga automáticamente al ejecutar las siguientes celdas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ux5xbkoEGEIJ"
   },
   "outputs": [],
   "source": [
    "class Cub2011(Dataset):\n",
    "    base_folder = 'CUB_200_2011/images'\n",
    "    url = 'https://data.caltech.edu/records/65de6-vp158/files/CUB_200_2011.tgz?download=1'\n",
    "    filename = 'CUB_200_2011.tgz'\n",
    "    tgz_md5 = '97eceeb196236b17998738112f37df78'\n",
    "\n",
    "    def __init__(self, root, train=True, transform=None, loader=default_loader, download=True):\n",
    "        self.root = os.path.expanduser(root)\n",
    "        self.transform = transform\n",
    "        self.loader = default_loader\n",
    "        self.train = train\n",
    "\n",
    "        if download:\n",
    "            self._download()\n",
    "\n",
    "        if not self._check_integrity():\n",
    "            raise RuntimeError('Dataset not found or corrupted.' +\n",
    "                               ' You can use download=True to download it')\n",
    "\n",
    "    def _load_metadata(self):\n",
    "        images = pd.read_csv(os.path.join(self.root, 'CUB_200_2011', 'images.txt'), sep=' ',\n",
    "                             names=['img_id', 'filepath'])\n",
    "        image_class_labels = pd.read_csv(os.path.join(self.root, 'CUB_200_2011', 'image_class_labels.txt'),\n",
    "                                         sep=' ', names=['img_id', 'target'])\n",
    "        train_test_split = pd.read_csv(os.path.join(self.root, 'CUB_200_2011', 'train_test_split.txt'),\n",
    "                                       sep=' ', names=['img_id', 'is_training_img'])\n",
    "\n",
    "        data = images.merge(image_class_labels, on='img_id')\n",
    "        self.data = data.merge(train_test_split, on='img_id')\n",
    "\n",
    "        if self.train:\n",
    "            self.data = self.data[self.data.is_training_img == 1]\n",
    "        else:\n",
    "            self.data = self.data[self.data.is_training_img == 0]\n",
    "\n",
    "    def _check_integrity(self):\n",
    "        try:\n",
    "            self._load_metadata()\n",
    "        except Exception:\n",
    "            return False\n",
    "\n",
    "        for index, row in self.data.iterrows():\n",
    "            filepath = os.path.join(self.root, self.base_folder, row.filepath)\n",
    "            if not os.path.isfile(filepath):\n",
    "                print(filepath)\n",
    "                return False\n",
    "        return True\n",
    "\n",
    "    def _download(self):\n",
    "        import tarfile\n",
    "\n",
    "        if self._check_integrity():\n",
    "            print('Files already downloaded and verified')\n",
    "            return\n",
    "\n",
    "        download_url(self.url, self.root, self.filename, self.tgz_md5)\n",
    "\n",
    "        with tarfile.open(os.path.join(self.root, self.filename), \"r:gz\") as tar:\n",
    "            tar.extractall(path=self.root)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.data.iloc[idx]\n",
    "        path = os.path.join(self.root, self.base_folder, sample.filepath)\n",
    "        target = sample.target - 1 \n",
    "        img = self.loader(path)\n",
    "\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        return img, target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "23qqQFV1GjWS"
   },
   "source": [
    "Preprocesado de las imágenes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "15JoU_PQGjAO"
   },
   "outputs": [],
   "source": [
    "transform = T.Compose([\n",
    "    T.RandomResizedCrop(224),\n",
    "    T.RandomHorizontalFlip(),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dxz6ldGoG0bE"
   },
   "source": [
    "Carga del dataset (Colab no soporta un batch_size muy alto así que lo dejo en 64 para la prueba)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Xb6IhrWfGtih",
    "outputId": "d6dced00-85f3-49b8-a68f-300439360fcf"
   },
   "outputs": [],
   "source": [
    "train_ds = Cub2011('.', train=True, transform = transform)\n",
    "val_ds = Cub2011('.s', train=False, transform = transform)\n",
    "\n",
    "ds = {'train': DataLoader(train_ds, batch_size = 64, shuffle=True),\n",
    "      'val': DataLoader(val_ds, batch_size = 64, shuffle=False)}\n",
    "\n",
    "\n",
    "ds_sizes = {'train': len(train_ds),\n",
    "      'val': len(val_ds)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8At2Z94CHCPL"
   },
   "source": [
    "Función de entrenamiento del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uD5xkrWCG_op"
   },
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, num_epochs=40, nclas=200, patience=8):\n",
    "    since = time.time()\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    #best_acc = 0.0\n",
    "    best_bal_acc = 0.0\n",
    "\n",
    "    #early stopping\n",
    "    best_epoch = 0\n",
    "    \n",
    "    epochs_bal_acc = []\n",
    "\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'Epoch {epoch}/{num_epochs - 1}')\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "            CF = np.zeros((nclas,nclas)) # Confusion matrix\n",
    "\n",
    "            # Iterate over data.\n",
    "            for inputs, labels in ds[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "                for i in range(len(labels.data)):\n",
    "                    CF[labels.data[i]][preds[i]] +=1\n",
    "                \n",
    "            #if phase == 'train':\n",
    "            #    scheduler.step()\n",
    "\n",
    "            epoch_loss = running_loss / ds_sizes[phase]\n",
    "            epoch_acc = running_corrects.double() / ds_sizes[phase]\n",
    "            recalli = 0\n",
    "            for i in range(nclas):\n",
    "                TP = CF[i][i]\n",
    "                FN = 0\n",
    "                for j in range(nclas):\n",
    "                    if i!=j:\n",
    "                        FN+=CF[i][j]\n",
    "                if (TP+FN) !=0:\n",
    "                    recalli+= TP/(TP+FN)\n",
    "            epoch_bal_acc = recalli/nclas\n",
    "            print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f} Balanced Acc: {epoch_bal_acc:.4f}')\n",
    "            if phase == 'val':\n",
    "                epochs_bal_acc.append(epoch_bal_acc)\n",
    "            # deep copy the model\n",
    "            if phase == 'val' and epoch_bal_acc > best_bal_acc:\n",
    "                best_bal_acc = epoch_bal_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "                best_epoch = epoch\n",
    "            \n",
    "            if phase == 'val' and epoch - best_epoch > patience:\n",
    "                print('Early stopping')\n",
    "                break\n",
    "\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print(f'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n",
    "    print(f'Best val Balanced Acc: {best_bal_acc:4f}')\n",
    "    print(epochs_bal_acc)\n",
    "\n",
    "    # load best model weights\n",
    "    #model.load_state_dict(best_model_wts)\n",
    "    return model, best_bal_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2jOfrDdgHtT3"
   },
   "outputs": [],
   "source": [
    "class AlphaConv2d(nn.Conv2d):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True):\n",
    "        super(AlphaConv2d, self).__init__(in_channels, out_channels, kernel_size, stride, padding, dilation, groups, bias)\n",
    "        #One alpha per filter\n",
    "        #self.alpha = nn.Parameter(torch.ones(out_channels, 1, 1, 1))\n",
    "        self.alpha = nn.Parameter(torch.rand(out_channels))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        #return super(AlphaConv2d, self).forward(x) * self.alpha\n",
    "        return super(AlphaConv2d, self).forward(x) * self.alpha.unsqueeze(1).unsqueeze(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1zGK0VmaH9pF"
   },
   "source": [
    "Función para mostrar los parámetros de la capa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D9zPfuPHIJwO"
   },
   "outputs": [],
   "source": [
    "def check_weights(m):\n",
    "    if type(m) == AlphaConv2d:\n",
    "        for module in m.named_parameters():\n",
    "            print(module)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6FCSNfOOIT-K"
   },
   "source": [
    "Carga el modelo preentrenado (He utilizado una VGG16 para simplificar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f1WkQ3ujHP9y"
   },
   "outputs": [],
   "source": [
    "model = vgg16(weights='IMAGENET1K_V1')\n",
    "model.classifier[6] = nn.Linear(4096, 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2CcrU51nm7sK"
   },
   "outputs": [],
   "source": [
    "for name, module in model.named_modules():\n",
    "  if type(module) == nn.Conv2d:\n",
    "    new_module = AlphaConv2d(module.in_channels, module.out_channels, module.kernel_size, module.stride, module.padding, module.dilation, module.groups, True)\n",
    "    new_module.weight = module.weight\n",
    "    new_module.bias = module.bias\n",
    "    model.features[int(name.split('.')[1])] = new_module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V2uu3kh6tXXH"
   },
   "outputs": [],
   "source": [
    "conv = [] #Conv layers\n",
    "fc = [] #FC layers\n",
    "\n",
    "for name, module in model.named_modules():\n",
    "    if type(module) == AlphaConv2d:\n",
    "        conv.append(module.alpha)\n",
    "    elif type(module) == nn.Linear:\n",
    "        fc.append(module.weight)\n",
    "        fc.append(module.bias)\n",
    "\n",
    "optimizer = torch.optim.SGD([\n",
    "                {'params': conv},\n",
    "                {'params': fc, 'lr': 0.005}\n",
    "          ], weight_decay  = 0.005, momentum = 0.9, lr = 0.0005)\n",
    "\n",
    "adam_optimizer = torch.optim.Adam([\n",
    "                {'params': conv},\n",
    "                {'params': fc, 'lr': 0.01}\n",
    "            ], weight_decay  = 0.005, lr = 0.001)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xQtMDUeOWmsG",
    "outputId": "a8babb51-005f-452e-b5d4-af8f9f6d49fb"
   },
   "outputs": [],
   "source": [
    "\n",
    "optimizer_last = torch.optim.SGD(model.classifier[6].parameters(), lr=0.005, momentum=0.9, weight_decay=0.005)\n",
    "adam_optimizer_last = torch.optim.Adam(model.classifier[6].parameters(), lr=0.01, weight_decay=0.005)\n",
    "model = model.to(device)\n",
    "model, _ = train_model(model, criterion, optimizer_last, num_epochs=60, nclas=200)\n",
    "torch.save(model, '.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CQi_lUyJtJPF",
    "outputId": "575340b0-381c-477e-9081-2e0f41878c64"
   },
   "outputs": [],
   "source": [
    "\n",
    "betterAcc = True\n",
    "previousAcc = 0.0\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "while betterAcc:\n",
    "    # Train the factors alpha by Eq.[3]\n",
    "   \n",
    "    for name, module in model.named_modules():\n",
    "      if type(module) == AlphaConv2d:\n",
    "          for param in module.parameters():\n",
    "              param.requires_grad = False\n",
    "          module.alpha.requires_grad = True\n",
    "\n",
    "\n",
    "    model_ft,_ = train_model(model, criterion, optimizer, num_epochs=60, nclas =200) # Set to 40 in the paper\n",
    "    \n",
    "    #Calculo del gradiente de los parámetros alpha\n",
    "    \n",
    "    alpha_grad = {}\n",
    "    for inputs, labels in ds['train']:\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        #Get the gradient of all alpha parameters in the vgg16 model\n",
    "        for name, param in model.named_parameters():\n",
    "            if 'alpha' in name:\n",
    "                if name.split('.')[0]+'.'+name.split('.')[1] not in alpha_grad:\n",
    "                    alpha_grad[name.split('.')[0]+'.'+name.split('.')[1]] = (param.grad/len(ds['train']))\n",
    "                else:\n",
    "                    alpha_grad[name.split('.')[0]+'.'+name.split('.')[1]] += (param.grad/len(ds['train']))\n",
    "    \n",
    "    betas = []\n",
    "    for name, module in model_ft.named_modules():\n",
    "        if type(module) == AlphaConv2d:\n",
    "            module.alpha.data = torch.abs(alpha_grad[name] * module.alpha.data) #Transform to beta\n",
    "            betas.extend(module.alpha)\n",
    "    \n",
    "    PERC = 0.10\n",
    "    pruneVal = max(nsmallest(int(len(betas)*PERC),betas))\n",
    "    \n",
    "    for name, module in model_ft.named_modules():\n",
    "        if type(module) == AlphaConv2d:\n",
    "            mask = module.alpha > pruneVal\n",
    "            print(f'Pruned {torch.sum((mask) == 0)} filters' ) \n",
    "            mask=mask.unsqueeze(1).unsqueeze(2).unsqueeze(3).expand_as(module.weight.data)\n",
    "            prune.custom_from_mask(module, 'weight', mask)\n",
    "            \n",
    "            \n",
    "\n",
    "    #Fine tune the model\n",
    "    for name, module in model_ft.named_modules():\n",
    "        if type(module) == AlphaConv2d:\n",
    "            for param in module.parameters():\n",
    "                param.requires_grad = True\n",
    "            module.alpha.requires_grad = False\n",
    "\n",
    "            \n",
    "    model_ft = model_ft.to(device)\n",
    "    model_ft,current_acc = train_model(model_ft, criterion, optimizer, num_epochs=60, nclas =200) # Set to 40 in the paper\n",
    "    if current_acc-previousAcc > 0.003: #tau = 0.3\n",
    "        previousAcc = current_acc\n",
    "        model = model_ft\n",
    "    else:\n",
    "        betterAcc = False\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1 (tags/v3.11.1:a7a450f, Dec  6 2022, 19:58:39) [MSC v.1934 64 bit (AMD64)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "5238573367df39f7286bb46f9ff5f08f63a01a80960060ce41e3c79b190280fa"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
