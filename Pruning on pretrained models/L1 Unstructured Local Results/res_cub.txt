10%:

Validation:
Val_bal_acc: [0.21101149425287363, 0.4427586206896554, 0.5512183908045983, 0.6070804597701152, 0.634885057471264, 0.6512643678160915, 0.6882873563218395, 0.7006149425287362, 0.7281896551724141, 0.7198908045977013, 0.759103448275862, 0.7538850574712643, 0.7694310344827591, 0.8014137931034484, 0.778948275862069, 0.7980977011494262, 0.8011724137931043, 0.814454022988506, 0.8192528735632184, 0.8227931034482772, 0.8264367816091963, 0.8184482758620697, 0.8273563218390815, 0.8416896551724145, 0.8413620689655181, 0.8386666666666681, 0.8383390804597706, 0.8523735632183915, 0.8562068965517254, 0.8505402298850586, 0.8568160919540246, 0.8522298850574725, 0.8643850574712656, 0.8645402298850587, 0.8633620689655183, 0.8675919540229899, 0.869367816091955, 0.8752183908045992, 0.8687701149425304, 0.867919540229886, 0.8808735632183923, 0.8850517241379328, 0.8907356321839096, 0.8727068965517253, 0.8816264367816105, 0.8915689655172433, 0.8835402298850589, 0.8810977011494266, 0.8897356321839098, 0.8854367816091968]
Val_acc: [tensor(0.2183, device='cuda:0', dtype=torch.float64), tensor(0.4581, device='cuda:0', dtype=torch.float64), tensor(0.5704, device='cuda:0', dtype=torch.float64), tensor(0.6281, device='cuda:0', dtype=torch.float64), tensor(0.6569, device='cuda:0', dtype=torch.float64), tensor(0.6738, device='cuda:0', dtype=torch.float64), tensor(0.7121, device='cuda:0', dtype=torch.float64), tensor(0.7249, device='cuda:0', dtype=torch.float64), tensor(0.7534, device='cuda:0', dtype=torch.float64), tensor(0.7447, device='cuda:0', dtype=torch.float64), tensor(0.7853, device='cuda:0', dtype=torch.float64), tensor(0.7799, device='cuda:0', dtype=torch.float64), tensor(0.7960, device='cuda:0', dtype=torch.float64), tensor(0.8291, device='cuda:0', dtype=torch.float64), tensor(0.8058, device='cuda:0', dtype=torch.float64), tensor(0.8257, device='cuda:0', dtype=torch.float64), tensor(0.8288, device='cuda:0', dtype=torch.float64), tensor(0.8426, device='cuda:0', dtype=torch.float64), tensor(0.8476, device='cuda:0', dtype=torch.float64), tensor(0.8512, device='cuda:0', dtype=torch.float64), tensor(0.8550, device='cuda:0', dtype=torch.float64), tensor(0.8467, device='cuda:0', dtype=torch.float64), tensor(0.8559, device='cuda:0', dtype=torch.float64), tensor(0.8707, device='cuda:0', dtype=torch.float64), tensor(0.8704, device='cuda:0', dtype=torch.float64), tensor(0.8676, device='cuda:0', dtype=torch.float64), tensor(0.8673, device='cuda:0', dtype=torch.float64), tensor(0.8818, device='cuda:0', dtype=torch.float64), tensor(0.8857, device='cuda:0', dtype=torch.float64), tensor(0.8799, device='cuda:0', dtype=torch.float64), tensor(0.8864, device='cuda:0', dtype=torch.float64), tensor(0.8816, device='cuda:0', dtype=torch.float64), tensor(0.8942, device='cuda:0', dtype=torch.float64), tensor(0.8944, device='cuda:0', dtype=torch.float64), tensor(0.8932, device='cuda:0', dtype=torch.float64), tensor(0.8975, device='cuda:0', dtype=torch.float64), tensor(0.8994, device='cuda:0', dtype=torch.float64), tensor(0.9054, device='cuda:0', dtype=torch.float64), tensor(0.8987, device='cuda:0', dtype=torch.float64), tensor(0.8978, device='cuda:0', dtype=torch.float64), tensor(0.9113, device='cuda:0', dtype=torch.float64), tensor(0.9156, device='cuda:0', dtype=torch.float64), tensor(0.9215, device='cuda:0', dtype=torch.float64), tensor(0.9028, device='cuda:0', dtype=torch.float64), tensor(0.9120, device='cuda:0', dtype=torch.float64), tensor(0.9223, device='cuda:0', dtype=torch.float64), tensor(0.9140, device='cuda:0', dtype=torch.float64), tensor(0.9115, device='cuda:0', dtype=torch.float64), tensor(0.9204, device='cuda:0', dtype=torch.float64), tensor(0.9159, device='cuda:0', dtype=torch.float64)]
Val_loss: [3.8138094953228205, 2.4277941650631596, 1.878983623289346, 1.6077875148686287, 1.4880043608919276, 1.403640858378952, 1.2952416943071792, 1.1821303417816793, 1.0803202612876563, 1.1281019066957758, 0.9825087052030237, 0.9826354201925512, 0.9359317906033717, 0.8181816138890352, 0.8937072090786574, 0.8207904830405249, 0.813524779982924, 0.7501345422706565, 0.7524679921487714, 0.711236706428541, 0.7152928682413848, 0.7410553317050256, 0.731125542859929, 0.6547746927235346, 0.6471258698350856, 0.6681113134057595, 0.656060842592057, 0.61905568179732, 0.5893270485887537, 0.6102413887149678, 0.5978558621387626, 0.6140655352199411, 0.5678212439598278, 0.57729912633356, 0.5753734737871267, 0.531743686632078, 0.5575711517281313, 0.5247564868224174, 0.5437853554219347, 0.5359437421740932, 0.49702001787647526, 0.4712346440600987, 0.45958481272788637, 0.5030251297409221, 0.49976668857400647, 0.4456922021412545, 0.4844463449698051, 0.47404377257614083, 0.45520803191392717, 0.4635437140186614]
Training:
Train_bal_acc: [0.04152873563218388, 0.2206609195402301, 0.36063793103448283, 0.439425287356322, 0.5034482758620688, 0.5208045977011491, 0.5676609195402298, 0.5845229885057475, 0.6125747126436782, 0.6303965517241377, 0.6329367816091958, 0.6626264367816093, 0.671649425287356, 0.6901264367816095, 0.6968390804597702, 0.6986609195402302, 0.7043333333333339, 0.7125402298850578, 0.7155517241379318, 0.7367701149425294, 0.7508965517241386, 0.7486034482758623, 0.7512701149425289, 0.758057471264368, 0.757155172413793, 0.7742471264367823, 0.7764137931034485, 0.7784137931034482, 0.7752528735632188, 0.801614942528736, 0.7991149425287364, 0.7992643678160931, 0.80430459770115, 0.8143333333333344, 0.8046206896551726, 0.8126666666666675, 0.7962931034482765, 0.811931034482759, 0.8210057471264374, 0.8229482758620699, 0.8184770114942538, 0.8218103448275867, 0.8273448275862075, 0.8381551724137946, 0.82796551724138, 0.8364770114942539, 0.837011494252874, 0.8366781609195411, 0.829494252873564, 0.838522988505748]
Train_acc: [tensor(0.0415, device='cuda:0', dtype=torch.float64), tensor(0.2207, device='cuda:0', dtype=torch.float64), tensor(0.3607, device='cuda:0', dtype=torch.float64), tensor(0.4394, device='cuda:0', dtype=torch.float64), tensor(0.5035, device='cuda:0', dtype=torch.float64), tensor(0.5209, device='cuda:0', dtype=torch.float64), tensor(0.5677, device='cuda:0', dtype=torch.float64), tensor(0.5846, device='cuda:0', dtype=torch.float64), tensor(0.6126, device='cuda:0', dtype=torch.float64), tensor(0.6305, device='cuda:0', dtype=torch.float64), tensor(0.6330, device='cuda:0', dtype=torch.float64), tensor(0.6627, device='cuda:0', dtype=torch.float64), tensor(0.6717, device='cuda:0', dtype=torch.float64), tensor(0.6902, device='cuda:0', dtype=torch.float64), tensor(0.6969, device='cuda:0', dtype=torch.float64), tensor(0.6987, device='cuda:0', dtype=torch.float64), tensor(0.7044, device='cuda:0', dtype=torch.float64), tensor(0.7125, device='cuda:0', dtype=torch.float64), tensor(0.7155, device='cuda:0', dtype=torch.float64), tensor(0.7367, device='cuda:0', dtype=torch.float64), tensor(0.7509, device='cuda:0', dtype=torch.float64), tensor(0.7486, device='cuda:0', dtype=torch.float64), tensor(0.7513, device='cuda:0', dtype=torch.float64), tensor(0.7581, device='cuda:0', dtype=torch.float64), tensor(0.7573, device='cuda:0', dtype=torch.float64), tensor(0.7743, device='cuda:0', dtype=torch.float64), tensor(0.7764, device='cuda:0', dtype=torch.float64), tensor(0.7784, device='cuda:0', dtype=torch.float64), tensor(0.7753, device='cuda:0', dtype=torch.float64), tensor(0.8016, device='cuda:0', dtype=torch.float64), tensor(0.7991, device='cuda:0', dtype=torch.float64), tensor(0.7993, device='cuda:0', dtype=torch.float64), tensor(0.8043, device='cuda:0', dtype=torch.float64), tensor(0.8143, device='cuda:0', dtype=torch.float64), tensor(0.8046, device='cuda:0', dtype=torch.float64), tensor(0.8126, device='cuda:0', dtype=torch.float64), tensor(0.7963, device='cuda:0', dtype=torch.float64), tensor(0.8120, device='cuda:0', dtype=torch.float64), tensor(0.8210, device='cuda:0', dtype=torch.float64), tensor(0.8230, device='cuda:0', dtype=torch.float64), tensor(0.8185, device='cuda:0', dtype=torch.float64), tensor(0.8218, device='cuda:0', dtype=torch.float64), tensor(0.8273, device='cuda:0', dtype=torch.float64), tensor(0.8382, device='cuda:0', dtype=torch.float64), tensor(0.8280, device='cuda:0', dtype=torch.float64), tensor(0.8365, device='cuda:0', dtype=torch.float64), tensor(0.8370, device='cuda:0', dtype=torch.float64), tensor(0.8367, device='cuda:0', dtype=torch.float64), tensor(0.8295, device='cuda:0', dtype=torch.float64), tensor(0.8385, device='cuda:0', dtype=torch.float64)]
Train_loss: [4.99899831468915, 3.3532267691574376, 2.573616886043453, 2.196796639426215, 1.928411775165134, 1.827393931112649, 1.6276442803738314, 1.5338742513834813, 1.467471526748624, 1.3813816709123852, 1.3818179252030733, 1.2624466358362376, 1.2250114732238901, 1.1556217582137496, 1.106050743832364, 1.1118790735035369, 1.0858724353151954, 1.0729728540580274, 1.0530543431545203, 0.9810256075174919, 0.9406738942728307, 0.9528994799097816, 0.9279180079012423, 0.8871932418934297, 0.9113849414202385, 0.8383098768320806, 0.832312269492431, 0.8310503152596541, 0.8347965409288735, 0.7489070010992699, 0.7838262702808565, 0.7398731734580026, 0.7255787120328413, 0.7150995989501338, 0.7436012521679496, 0.7249411248070898, 0.7653255005816758, 0.7230762148006861, 0.6924866971708673, 0.6634400916529132, 0.6770291379940363, 0.6935704481534095, 0.6572151242751935, 0.6289758080436979, 0.6441364281568123, 0.6473651880974526, 0.6198381947444843, 0.6179709140563114, 0.6373686270989057, 0.6090729818727559]

30%:

Validation:
Val_bal_acc: [0.34975862068965496, 0.5059195402298851, 0.5613735632183908, 0.6040574712643683, 0.6571551724137927, 0.6862873563218393, 0.6955862068965516, 0.7319195402298851, 0.7352758620689654, 0.7474195402298855, 0.7624425287356325, 0.7421954022988512, 0.7806206896551727, 0.7951091954022995, 0.8085057471264372, 0.8059942528735636, 0.8127931034482764, 0.8082126436781618, 0.8146206896551733, 0.8338333333333341, 0.8193275862068972, 0.8157988505747131, 0.8274885057471273, 0.8178735632183914, 0.8358505747126448, 0.8318505747126443, 0.8440747126436784, 0.8484655172413799, 0.8443218390804609, 0.8466666666666676, 0.8522241379310356, 0.8540747126436793, 0.8598505747126447, 0.8481781609195416, 0.8601839080459776, 0.8544022988505753, 0.8542241379310355, 0.853494252873564, 0.8692241379310357, 0.8728390804597711, 0.8480402298850587, 0.8645229885057485, 0.8755344827586222, 0.8533390804597716, 0.8571724137931045, 0.8680057471264377, 0.8639827586206912, 0.8687126436781617, 0.871718390804599, 0.8764942528735645]
Val_acc: [tensor(0.3619, device='cuda:0', dtype=torch.float64), tensor(0.5235, device='cuda:0', dtype=torch.float64), tensor(0.5808, device='cuda:0', dtype=torch.float64), tensor(0.6250, device='cuda:0', dtype=torch.float64), tensor(0.6798, device='cuda:0', dtype=torch.float64), tensor(0.7100, device='cuda:0', dtype=torch.float64), tensor(0.7197, device='cuda:0', dtype=torch.float64), tensor(0.7572, device='cuda:0', dtype=torch.float64), tensor(0.7606, device='cuda:0', dtype=torch.float64), tensor(0.7732, device='cuda:0', dtype=torch.float64), tensor(0.7887, device='cuda:0', dtype=torch.float64), tensor(0.7679, device='cuda:0', dtype=torch.float64), tensor(0.8076, device='cuda:0', dtype=torch.float64), tensor(0.8226, device='cuda:0', dtype=torch.float64), tensor(0.8364, device='cuda:0', dtype=torch.float64), tensor(0.8338, device='cuda:0', dtype=torch.float64), tensor(0.8409, device='cuda:0', dtype=torch.float64), tensor(0.8362, device='cuda:0', dtype=torch.float64), tensor(0.8428, device='cuda:0', dtype=torch.float64), tensor(0.8626, device='cuda:0', dtype=torch.float64), tensor(0.8476, device='cuda:0', dtype=torch.float64), tensor(0.8440, device='cuda:0', dtype=torch.float64), tensor(0.8561, device='cuda:0', dtype=torch.float64), tensor(0.8460, device='cuda:0', dtype=torch.float64), tensor(0.8647, device='cuda:0', dtype=torch.float64), tensor(0.8605, device='cuda:0', dtype=torch.float64), tensor(0.8731, device='cuda:0', dtype=torch.float64), tensor(0.8778, device='cuda:0', dtype=torch.float64), tensor(0.8735, device='cuda:0', dtype=torch.float64), tensor(0.8759, device='cuda:0', dtype=torch.float64), tensor(0.8816, device='cuda:0', dtype=torch.float64), tensor(0.8835, device='cuda:0', dtype=torch.float64), tensor(0.8895, device='cuda:0', dtype=torch.float64), tensor(0.8775, device='cuda:0', dtype=torch.float64), tensor(0.8899, device='cuda:0', dtype=torch.float64), tensor(0.8838, device='cuda:0', dtype=torch.float64), tensor(0.8837, device='cuda:0', dtype=torch.float64), tensor(0.8830, device='cuda:0', dtype=torch.float64), tensor(0.8992, device='cuda:0', dtype=torch.float64), tensor(0.9030, device='cuda:0', dtype=torch.float64), tensor(0.8773, device='cuda:0', dtype=torch.float64), tensor(0.8944, device='cuda:0', dtype=torch.float64), tensor(0.9058, device='cuda:0', dtype=torch.float64), tensor(0.8828, device='cuda:0', dtype=torch.float64), tensor(0.8868, device='cuda:0', dtype=torch.float64), tensor(0.8980, device='cuda:0', dtype=torch.float64), tensor(0.8939, device='cuda:0', dtype=torch.float64), tensor(0.8987, device='cuda:0', dtype=torch.float64), tensor(0.9018, device='cuda:0', dtype=torch.float64), tensor(0.9068, device='cuda:0', dtype=torch.float64)]
Val_loss: [2.9120261977778084, 2.060608708286516, 1.8220420721937634, 1.5447306514280106, 1.3610152826664734, 1.2461937672934698, 1.2241756869547524, 1.0989648318512917, 1.0206738633042909, 0.9994543497423324, 0.9463997913468571, 0.9887298561804946, 0.8711288451819243, 0.8261297685876977, 0.759356901877742, 0.7819142719775098, 0.7459390581291957, 0.767945071829571, 0.7339575341087726, 0.6972294946065803, 0.7355117636546621, 0.7257069090460349, 0.6922560513204733, 0.7218482120925079, 0.703883183143408, 0.6937003331551438, 0.6258471934475238, 0.6434381223975851, 0.6413563127800807, 0.6233422572258056, 0.603347772081062, 0.6006603381722847, 0.580221371688553, 0.637361649345397, 0.576829437350338, 0.6016815085945189, 0.5921909797590768, 0.6047782626158622, 0.519132288262212, 0.5413237715543119, 0.6143336837399529, 0.5495728825469407, 0.5242882236596589, 0.588851881254314, 0.5751960909403642, 0.5654342750001867, 0.5560195094240012, 0.5259720535269268, 0.5310712830132355, 0.5193391805646499]
Training:
Train_bal_acc: [0.10491954022988516, 0.3241379310344829, 0.41658045977011476, 0.4889712643678161, 0.531086206896552, 0.5642183908045978, 0.6003678160919536, 0.6113735632183909, 0.6436494252873559, 0.6487931034482757, 0.674844827586207, 0.686965517241379, 0.6956494252873567, 0.7028735632183906, 0.7118390804597701, 0.720195402298851, 0.7361954022988509, 0.7414252873563224, 0.7498678160919543, 0.7512356321839083, 0.7550919540229889, 0.7675517241379315, 0.7722931034482761, 0.7758965517241387, 0.776413793103449, 0.7909597701149436, 0.7861206896551732, 0.7898045977011505, 0.7959425287356333, 0.7965862068965524, 0.7902586206896552, 0.8029137931034489, 0.804126436781611, 0.805270114942529, 0.80762643678161, 0.8086321839080465, 0.820505747126438, 0.8156494252873567, 0.8226494252873569, 0.8139827586206906, 0.8101149425287361, 0.8219942528735639, 0.8250172413793109, 0.8231551724137944, 0.8196379310344826, 0.8249540229885065, 0.8330000000000005, 0.8313275862068977, 0.8310172413793113, 0.8263620689655178]
Train_acc: [tensor(0.1049, device='cuda:0', dtype=torch.float64), tensor(0.3242, device='cuda:0', dtype=torch.float64), tensor(0.4166, device='cuda:0', dtype=torch.float64), tensor(0.4890, device='cuda:0', dtype=torch.float64), tensor(0.5312, device='cuda:0', dtype=torch.float64), tensor(0.5642, device='cuda:0', dtype=torch.float64), tensor(0.6004, device='cuda:0', dtype=torch.float64), tensor(0.6114, device='cuda:0', dtype=torch.float64), tensor(0.6436, device='cuda:0', dtype=torch.float64), tensor(0.6488, device='cuda:0', dtype=torch.float64), tensor(0.6748, device='cuda:0', dtype=torch.float64), tensor(0.6870, device='cuda:0', dtype=torch.float64), tensor(0.6957, device='cuda:0', dtype=torch.float64), tensor(0.7029, device='cuda:0', dtype=torch.float64), tensor(0.7119, device='cuda:0', dtype=torch.float64), tensor(0.7202, device='cuda:0', dtype=torch.float64), tensor(0.7362, device='cuda:0', dtype=torch.float64), tensor(0.7414, device='cuda:0', dtype=torch.float64), tensor(0.7499, device='cuda:0', dtype=torch.float64), tensor(0.7513, device='cuda:0', dtype=torch.float64), tensor(0.7551, device='cuda:0', dtype=torch.float64), tensor(0.7676, device='cuda:0', dtype=torch.float64), tensor(0.7723, device='cuda:0', dtype=torch.float64), tensor(0.7759, device='cuda:0', dtype=torch.float64), tensor(0.7764, device='cuda:0', dtype=torch.float64), tensor(0.7910, device='cuda:0', dtype=torch.float64), tensor(0.7861, device='cuda:0', dtype=torch.float64), tensor(0.7898, device='cuda:0', dtype=torch.float64), tensor(0.7960, device='cuda:0', dtype=torch.float64), tensor(0.7966, device='cuda:0', dtype=torch.float64), tensor(0.7903, device='cuda:0', dtype=torch.float64), tensor(0.8030, device='cuda:0', dtype=torch.float64), tensor(0.8041, device='cuda:0', dtype=torch.float64), tensor(0.8053, device='cuda:0', dtype=torch.float64), tensor(0.8076, device='cuda:0', dtype=torch.float64), tensor(0.8086, device='cuda:0', dtype=torch.float64), tensor(0.8205, device='cuda:0', dtype=torch.float64), tensor(0.8156, device='cuda:0', dtype=torch.float64), tensor(0.8227, device='cuda:0', dtype=torch.float64), tensor(0.8140, device='cuda:0', dtype=torch.float64), tensor(0.8101, device='cuda:0', dtype=torch.float64), tensor(0.8220, device='cuda:0', dtype=torch.float64), tensor(0.8250, device='cuda:0', dtype=torch.float64), tensor(0.8232, device='cuda:0', dtype=torch.float64), tensor(0.8197, device='cuda:0', dtype=torch.float64), tensor(0.8250, device='cuda:0', dtype=torch.float64), tensor(0.8330, device='cuda:0', dtype=torch.float64), tensor(0.8313, device='cuda:0', dtype=torch.float64), tensor(0.8310, device='cuda:0', dtype=torch.float64), tensor(0.8263, device='cuda:0', dtype=torch.float64)]
Train_loss: [4.406225369459476, 2.79945959653463, 2.2898411090110673, 1.967067951992189, 1.8140753163947716, 1.673808015502609, 1.5343468601320998, 1.4163380156249097, 1.339407931000223, 1.3274474430370617, 1.2031159251381407, 1.1837377208926736, 1.160421465132926, 1.0898581014938022, 1.0687842501932119, 1.0255290954797953, 0.9959514044824345, 0.9662063899818245, 0.9300503470637538, 0.9272924949019441, 0.9279008513098365, 0.8777798597200894, 0.846011822327877, 0.8322462333135697, 0.8543105939248422, 0.8015297236425064, 0.8130737318171634, 0.7985156506827007, 0.780089852529086, 0.7762543112487048, 0.7726871275607451, 0.7421920761331782, 0.7447197986595783, 0.7458332333120856, 0.7395888658257218, 0.7267710814550196, 0.6871842019431464, 0.6951597180810418, 0.6715911331552046, 0.7023456280614123, 0.7300863764340461, 0.691022287319611, 0.6789221554574447, 0.6887377504511678, 0.6827019678965126, 0.6701509057978292, 0.6218539392466859, 0.6497376552513531, 0.6512080299921899, 0.6541548002303681]


__________________________________________

Updated code:

traced_optimizer = nni.trace(optim.SGD)(model.parameters(), lr=0.001, momentum=0.9)
# Operation types to be pruned and operation partial names to be pruned in vgg16
#Prune 10% of the filters 
config_list = [{'op_types': ['Conv2d','Linear'], 
'sparsity_per_layer': 0.1}]
#criterion (Callable[[Tensor, Tensor], Tensor]) – The criterion function used in trainer. Take model output and target value as input, and return the loss.
criterion = nn.CrossEntropyLoss()
evaluator = TorchEvaluator(training_func=training_model, optimizers=traced_optimizer,criterion=criterion)
# warm_up_step – The total optimizer.step() number before start pruning for warm up. Make sure warm_up_step is smaller than cool_down_beginning_step.
# cool_down_beginning_step – The number of steps at which sparsity stops growing, note that the sparsity stop growing doesn’t mean masks not changed.
warm_up_step = len(train_ds) // BATCH_SIZE * 1
cool_down_begin_step = len(train_ds) // BATCH_SIZE * 2
pruner = MovementPruner(model, config_list, evaluator, warm_up_step=warm_up_step, cool_down_beginning_step=cool_down_begin_step, training_epochs=50, regular_scale=15, movement_mode='soft')

_, masks = pruner.compress()


Val <-> Train

Results: 
[2022-11-11 16:25:19] WARNING: Did not bind any model, no need to unbind model.
Val Acc: 0.1073
Train Acc: 0.3101
Val Acc: 0.3300
Train Acc: 0.4180
Val Acc: 0.4284
Train Acc: 0.4625
Val Acc: 0.4842
Train Acc: 0.5119
Val Acc: 0.5370
Train Acc: 0.5250
Val Acc: 0.5712
Train Acc: 0.5456
Val Acc: 0.6063
Train Acc: 0.5613
Val Acc: 0.6306
Train Acc: 0.5723
Val Acc: 0.6303
Train Acc: 0.5889
Val Acc: 0.6675
Train Acc: 0.5816
Val Acc: 0.6665
Train Acc: 0.5860
Val Acc: 0.6722
Train Acc: 0.5994
Val Acc: 0.6952
Train Acc: 0.6017
Val Acc: 0.7199
Train Acc: 0.6056
Val Acc: 0.7269
Train Acc: 0.5973
Val Acc: 0.7307
Train Acc: 0.6151
Val Acc: 0.7289
Train Acc: 0.6051
Val Acc: 0.7508
Train Acc: 0.6103
Val Acc: 0.7596
Train Acc: 0.6063
Val Acc: 0.7538
Train Acc: 0.6225
Val Acc: 0.7589
Train Acc: 0.6042
Val Acc: 0.7726
Train Acc: 0.6105
Val Acc: 0.7648
Train Acc: 0.6120
Val Acc: 0.7840
Train Acc: 0.6241
Val Acc: 0.7993
Train Acc: 0.6044
Val Acc: 0.7951
Train Acc: 0.6215
Val Acc: 0.7975
Train Acc: 0.6236
Val Acc: 0.8038
Train Acc: 0.6099
Val Acc: 0.8053
Train Acc: 0.6049

__________________________________________

Different warm up


traced_optimizer = nni.trace(optim.SGD)(model.parameters(), lr=0.001, momentum=0.9)
# Operation types to be pruned and operation partial names to be pruned in vgg16
#Prune 10% of the filters 
config_list = [{'op_types': ['Conv2d','Linear'], 
'sparsity_per_layer': 0.1}]
#criterion (Callable[[Tensor, Tensor], Tensor]) – The criterion function used in trainer. Take model output and target value as input, and return the loss.
criterion = nn.CrossEntropyLoss()
evaluator = TorchEvaluator(training_func=training_model, optimizers=traced_optimizer,criterion=criterion)
# warm_up_step – The total optimizer.step() number before start pruning for warm up. Make sure warm_up_step is smaller than cool_down_beginning_step.
# cool_down_beginning_step – The number of steps at which sparsity stops growing, note that the sparsity stop growing doesn’t mean masks not changed.
warm_up_step = len(train_ds) // BATCH_SIZE * 6
cool_down_begin_step = len(train_ds) // BATCH_SIZE * 8
pruner = MovementPruner(model, config_list, evaluator, warm_up_step=warm_up_step, cool_down_beginning_step=cool_down_begin_step, training_epochs=50, regular_scale=15, movement_mode='soft')

_, masks = pruner.compress()

Results:

Train Acc: 0.0943
Val Acc: 0.2829
Train Acc: 0.3158
Val Acc: 0.4197
Train Acc: 0.4274
Val Acc: 0.4589
Train Acc: 0.4955
Val Acc: 0.4979
Train Acc: 0.5370
Val Acc: 0.5290
Train Acc: 0.5686
Val Acc: 0.5652
Train Acc: 0.5996
Val Acc: 0.5611
Train Acc: 0.6208
Val Acc: 0.5744
Train Acc: 0.6458
Val Acc: 0.5604
Train Acc: 0.6593
Val Acc: 0.5929
Train Acc: 0.6713
Val Acc: 0.5830
Train Acc: 0.6907
Val Acc: 0.5972
Train Acc: 0.6884
Val Acc: 0.5851
Train Acc: 0.7145
Val Acc: 0.6058
Train Acc: 0.7079
Val Acc: 0.5958
Train Acc: 0.7227
Val Acc: 0.6136
Train Acc: 0.7344
Val Acc: 0.6191
Train Acc: 0.7419
Val Acc: 0.6215
Train Acc: 0.7514
Val Acc: 0.6155
Train Acc: 0.7508
Val Acc: 0.6148
Train Acc: 0.7624
Val Acc: 0.6101
Train Acc: 0.7749
Val Acc: 0.6231
Train Acc: 0.7828
Val Acc: 0.6206
Train Acc: 0.7898
Val Acc: 0.6248
Train Acc: 0.7910
Val Acc: 0.6262
Train Acc: 0.7826
Val Acc: 0.6136

__________________________________________



traced_optimizer = nni.trace(optim.SGD)(model.parameters(), lr=0.001, momentum=0.9)
# Operation types to be pruned and operation partial names to be pruned in vgg16
#Prune 10% of the filters 
config_list = [{'op_types': ['Conv2d','Linear'], 
'sparsity_per_layer': 0.1}]
#criterion (Callable[[Tensor, Tensor], Tensor]) – The criterion function used in trainer. Take model output and target value as input, and return the loss.
criterion = nn.CrossEntropyLoss()
evaluator = TorchEvaluator(training_func=training_model, optimizers=traced_optimizer,criterion=criterion)
# warm_up_step – The total optimizer.step() number before start pruning for warm up. Make sure warm_up_step is smaller than cool_down_beginning_step.
# cool_down_beginning_step – The number of steps at which sparsity stops growing, note that the sparsity stop growing doesn’t mean masks not changed.
warm_up_step = len(train_ds) // BATCH_SIZE * 6
cool_down_begin_step = len(train_ds) // BATCH_SIZE * 8
pruner = MovementPruner(model, config_list, evaluator, warm_up_step=warm_up_step, cool_down_beginning_step=cool_down_begin_step, training_epochs=50,  movement_mode='hard')

_, masks = pruner.compress()

Results:
[2022-11-11 21:22:45] WARNING: Did not bind any model, no need to unbind model.
Train Acc: 0.0954
Val Acc: 0.2972
Train Acc: 0.3100
Val Acc: 0.4089
Train Acc: 0.4169
Val Acc: 0.4684
Train Acc: 0.4887
Val Acc: 0.5209
Train Acc: 0.5340
Val Acc: 0.5330
Train Acc: 0.5706
Val Acc: 0.5445
Train Acc: 0.5013
Val Acc: 0.4845
Train Acc: 0.5379
Val Acc: 0.4983
Train Acc: 0.5794
Val Acc: 0.5475
Train Acc: 0.6233
Val Acc: 0.5639
Train Acc: 0.6520
Val Acc: 0.5882
Train Acc: 0.6486
Val Acc: 0.5789
Train Acc: 0.6815
Val Acc: 0.5899
Train Acc: 0.6834
Val Acc: 0.5865
Train Acc: 0.6989
Val Acc: 0.5866
Train Acc: 0.7014
Val Acc: 0.5742
Train Acc: 0.7187
Val Acc: 0.5972
Train Acc: 0.7277
Val Acc: 0.5906
Train Acc: 0.7287
Val Acc: 0.5991
Train Acc: 0.7394
Val Acc: 0.6077
Train Acc: 0.7364
Val Acc: 0.5782
Train Acc: 0.7509
Val Acc: 0.6129
Train Acc: 0.7603
Val Acc: 0.6110
Train Acc: 0.7619
Val Acc: 0.6303
Train Acc: 0.7624
Val Acc: 0.6072
Train Acc: 0.7713
Val Acc: 0.6098
__________________________________________


traced_optimizer = nni.trace(optim.SGD)(model.parameters(), lr=0.001, momentum=0.9)
# Operation types to be pruned and operation partial names to be pruned in vgg16
#Prune 10% of the filters 
config_list = [{'op_types': ['Conv2d','Linear'], 
'sparsity_per_layer': 0.1}]
#criterion (Callable[[Tensor, Tensor], Tensor]) – The criterion function used in trainer. Take model output and target value as input, and return the loss.
criterion = nn.CrossEntropyLoss()
lr_scheduler = nni.trace(torch.optim.lr_scheduler.StepLR)(traced_optimizer, step_size=7, gamma=0.1)
evaluator = TorchEvaluator(training_func=training_model, optimizers=traced_optimizer,criterion=criterion, lr_schedulers=lr_scheduler)
# warm_up_step – The total optimizer.step() number before start pruning for warm up. Make sure warm_up_step is smaller than cool_down_beginning_step.
# cool_down_beginning_step – The number of steps at which sparsity stops growing, note that the sparsity stop growing doesn’t mean masks not changed.
warm_up_step = len(train_ds) // BATCH_SIZE * 6
cool_down_begin_step = len(train_ds) // BATCH_SIZE * 8
pruner = MovementPruner(model, config_list, evaluator, warm_up_step=warm_up_step, cool_down_beginning_step=cool_down_begin_step, training_epochs=50,  movement_mode='hard')

_, masks = pruner.compress()

Train Acc: 0.0933
Val Acc: 0.2699
Train Acc: 0.3063
Val Acc: 0.3876
Train Acc: 0.4087
Val Acc: 0.4905
Train Acc: 0.4922
Val Acc: 0.4971
Train Acc: 0.5394
Val Acc: 0.5271
Train Acc: 0.5667
Val Acc: 0.5390
Train Acc: 0.5234
Val Acc: 0.4822
Train Acc: 0.5984
Val Acc: 0.5376
Train Acc: 0.6321
Val Acc: 0.5651
Train Acc: 0.6648
Val Acc: 0.5987
Train Acc: 0.6727
Val Acc: 0.5929
Train Acc: 0.6857
Val Acc: 0.5906
Train Acc: 0.6997
Val Acc: 0.6151
Train Acc: 0.7110
Val Acc: 0.5989
Train Acc: 0.7077
Val Acc: 0.6129
Train Acc: 0.7264
Val Acc: 0.6222
Train Acc: 0.7336
Val Acc: 0.5975
Train Acc: 0.7429
Val Acc: 0.6201
Train Acc: 0.7372
Val Acc: 0.6274
Train Acc: 0.7489
Val Acc: 0.6248
Train Acc: 0.7499
Val Acc: 0.6124
Train Acc: 0.7457
Val Acc: 0.6246
Train Acc: 0.7689
Val Acc: 0.6193
Train Acc: 0.7718
Val Acc: 0.6239
Train Acc: 0.7769
Val Acc: 0.6124
Train Acc: 0.7778
Val Acc: 0.6350
Train Acc: 0.7885
Val Acc: 0.6372
Train Acc: 0.7741
Val Acc: 0.6300
Train Acc: 0.7920
Val Acc: 0.6324
Train Acc: 0.7908
Val Acc: 0.6491
Train Acc: 0.7923
Val Acc: 0.6377
Train Acc: 0.7936
Val Acc: 0.6398
Train Acc: 0.8010
Val Acc: 0.6357
Train Acc: 0.7943
Val Acc: 0.6474
Train Acc: 0.8060
Val Acc: 0.6472
Train Acc: 0.8051
Val Acc: 0.6362
Train Acc: 0.8061
Val Acc: 0.6329
Train Acc: 0.7971
Val Acc: 0.6479
Train Acc: 0.8098
Val Acc: 0.6346
Train Acc: 0.8095
Val Acc: 0.6495
Train Acc: 0.8166
Val Acc: 0.6365
Train Acc: 0.8146
Val Acc: 0.6446
Train Acc: 0.8165
Val Acc: 0.6474
Train Acc: 0.8262
Val Acc: 0.6512
Train Acc: 0.8150
Val Acc: 0.6388
Train Acc: 0.8227
Val Acc: 0.6457


__________________________________________


traced_optimizer = nni.trace(optim.SGD)(model.parameters(), lr=0.001, momentum=0.9)
# Operation types to be pruned and operation partial names to be pruned in vgg16
#Prune 10% of the filters 
config_list = [{'op_types': ['Conv2d','Linear'], 
'sparsity_per_layer': 0.4}]
#criterion (Callable[[Tensor, Tensor], Tensor]) – The criterion function used in trainer. Take model output and target value as input, and return the loss.
criterion = nn.CrossEntropyLoss()
lr_scheduler = nni.trace(torch.optim.lr_scheduler.StepLR)(traced_optimizer, step_size=7, gamma=0.1)
evaluator = TorchEvaluator(training_func=training_model, optimizers=traced_optimizer,criterion=criterion, lr_schedulers=lr_scheduler)
# warm_up_step – The total optimizer.step() number before start pruning for warm up. Make sure warm_up_step is smaller than cool_down_beginning_step.
# cool_down_beginning_step – The number of steps at which sparsity stops growing, note that the sparsity stop growing doesn’t mean masks not changed.
warm_up_step = len(train_ds) // BATCH_SIZE * 6
cool_down_begin_step = len(train_ds) // BATCH_SIZE * 8
pruner = MovementPruner(model, config_list, evaluator, warm_up_step=warm_up_step, cool_down_beginning_step=cool_down_begin_step, training_epochs=50,  movement_mode='hard')

_, masks = pruner.compress()



Train Acc: 0.1003
Val Acc: 0.2957
Train Acc: 0.3076
Val Acc: 0.4329
Train Acc: 0.4117
Val Acc: 0.4822
Train Acc: 0.4902
Val Acc: 0.5064
Train Acc: 0.5214
Val Acc: 0.5212
Train Acc: 0.5552
Val Acc: 0.4839
Train Acc: 0.1990
Val Acc: 0.1921
Train Acc: 0.2377
Val Acc: 0.3207
Train Acc: 0.3170
Val Acc: 0.3460
Train Acc: 0.3637
Val Acc: 0.3773
Train Acc: 0.4092
Val Acc: 0.4239
Train Acc: 0.4351
Val Acc: 0.4196
Train Acc: 0.4696
Val Acc: 0.3940
Train Acc: 0.4912
Val Acc: 0.4455
Train Acc: 0.5117
Val Acc: 0.4803
Train Acc: 0.5175
Val Acc: 0.4776
Train Acc: 0.5269
Val Acc: 0.4955
Train Acc: 0.5459
Val Acc: 0.4984
Train Acc: 0.5607
Val Acc: 0.5186
Train Acc: 0.5861
Val Acc: 0.5211
Train Acc: 0.5794
Val Acc: 0.5354
Train Acc: 0.6068
Val Acc: 0.5207
Train Acc: 0.6116
Val Acc: 0.5166
Train Acc: 0.6173
Val Acc: 0.5323
Train Acc: 0.6153
Val Acc: 0.5388
Train Acc: 0.6401
Val Acc: 0.5459
Train Acc: 0.6411
Val Acc: 0.5407
Train Acc: 0.6496
Val Acc: 0.5312
Train Acc: 0.6488
Val Acc: 0.5540
Train Acc: 0.6692
Val Acc: 0.5627
Train Acc: 0.6813
Val Acc: 0.5689
Train Acc: 0.6800
Val Acc: 0.5537
Train Acc: 0.6900
Val Acc: 0.5466
Train Acc: 0.6860
Val Acc: 0.5718
Train Acc: 0.6999
Val Acc: 0.5637
Train Acc: 0.7095
Val Acc: 0.5576
Train Acc: 0.6975
Val Acc: 0.5647
Train Acc: 0.7075
Val Acc: 0.5880
Train Acc: 0.7147
Val Acc: 0.5632
Train Acc: 0.7097
Val Acc: 0.5846
Train Acc: 0.7100
Val Acc: 0.5773
Train Acc: 0.7381
Val Acc: 0.5892
Train Acc: 0.7341
Val Acc: 0.5818
Train Acc: 0.7362
Val Acc: 0.5935
Train Acc: 0.7317
Val Acc: 0.5630
Train Acc: 0.7344
Val Acc: 0.5828
Train Acc: 0.7326
Val Acc: 0.5808
Train Acc: 0.7541
Val Acc: 0.5870
Train Acc: 0.7509
Val Acc: 0.5835
Train Acc: 0.7578
Val Acc: 0.5823

Validation:
Val_bal_acc: [0.29720419964594585, 0.4336122982377223, 0.48416588938633837, 0.5098718660070157, 0.5229827470612803, 0.48457595764908545, 0.19144581125319732, 0.31990448621755335, 0.347290870113722, 0.3775261353051734, 0.42601003012815547, 0.42024117131542377, 0.3958166725360765, 0.4457934274263281, 0.4819289307709573, 0.4773855292195463, 0.4963118802816784, 0.4984635750788397, 0.5188365190619194, 0.5217482477374572, 0.5368571476117042, 0.5227965190373657, 0.5198168872095724, 0.5334549884878733, 0.5376535720176728, 0.5492324269998661, 0.5431148104009702, 0.5319956717941805, 0.5556067319700242, 0.5647576991207148, 0.5693032696770366, 0.5543303684853041, 0.5484601461004835, 0.5739380077598534, 0.5642930665511545, 0.5588611214632682, 0.56581055280061, 0.5888632991145619, 0.5662153298026018, 0.5853520308551875, 0.579718233839101, 0.5893309534555098, 0.5814133786522327, 0.5963441836270291, 0.5661558940312984, 0.5840062657721855, 0.5819999586680128, 0.5891280305057235, 0.5847871116144612, 0.5831626085935249]
Val_acc: [tensor(0.2957, device='cuda:0', dtype=torch.float64), tensor(0.4329, device='cuda:0', dtype=torch.float64), tensor(0.4822, device='cuda:0', dtype=torch.float64), tensor(0.5064, device='cuda:0', dtype=torch.float64), tensor(0.5212, device='cuda:0', dtype=torch.float64), tensor(0.4839, device='cuda:0', dtype=torch.float64), tensor(0.1921, device='cuda:0', dtype=torch.float64), tensor(0.3207, device='cuda:0', dtype=torch.float64), tensor(0.3460, device='cuda:0', dtype=torch.float64), tensor(0.3773, device='cuda:0', dtype=torch.float64), tensor(0.4239, device='cuda:0', dtype=torch.float64), tensor(0.4196, device='cuda:0', dtype=torch.float64), tensor(0.3940, device='cuda:0', dtype=torch.float64), tensor(0.4455, device='cuda:0', dtype=torch.float64), tensor(0.4803, device='cuda:0', dtype=torch.float64), tensor(0.4776, device='cuda:0', dtype=torch.float64), tensor(0.4955, device='cuda:0', dtype=torch.float64), tensor(0.4984, device='cuda:0', dtype=torch.float64), tensor(0.5186, device='cuda:0', dtype=torch.float64), tensor(0.5211, device='cuda:0', dtype=torch.float64), tensor(0.5354, device='cuda:0', dtype=torch.float64), tensor(0.5207, device='cuda:0', dtype=torch.float64), tensor(0.5166, device='cuda:0', dtype=torch.float64), tensor(0.5323, device='cuda:0', dtype=torch.float64), tensor(0.5388, device='cuda:0', dtype=torch.float64), tensor(0.5459, device='cuda:0', dtype=torch.float64), tensor(0.5407, device='cuda:0', dtype=torch.float64), tensor(0.5312, device='cuda:0', dtype=torch.float64), tensor(0.5540, device='cuda:0', dtype=torch.float64), tensor(0.5627, device='cuda:0', dtype=torch.float64), tensor(0.5689, device='cuda:0', dtype=torch.float64), tensor(0.5537, device='cuda:0', dtype=torch.float64), tensor(0.5466, device='cuda:0', dtype=torch.float64), tensor(0.5718, device='cuda:0', dtype=torch.float64), tensor(0.5637, device='cuda:0', dtype=torch.float64), tensor(0.5576, device='cuda:0', dtype=torch.float64), tensor(0.5647, device='cuda:0', dtype=torch.float64), tensor(0.5880, device='cuda:0', dtype=torch.float64), tensor(0.5632, device='cuda:0', dtype=torch.float64), tensor(0.5846, device='cuda:0', dtype=torch.float64), tensor(0.5773, device='cuda:0', dtype=torch.float64), tensor(0.5892, device='cuda:0', dtype=torch.float64), tensor(0.5818, device='cuda:0', dtype=torch.float64), tensor(0.5935, device='cuda:0', dtype=torch.float64), tensor(0.5630, device='cuda:0', dtype=torch.float64), tensor(0.5828, device='cuda:0', dtype=torch.float64), tensor(0.5808, device='cuda:0', dtype=torch.float64), tensor(0.5870, device='cuda:0', dtype=torch.float64), tensor(0.5835, device='cuda:0', dtype=torch.float64), tensor(0.5823, device='cuda:0', dtype=torch.float64)]
Val_loss: [2.977320049728325, 2.297435318720024, 2.0321086433722226, 1.9254597497574824, 1.8521297131071923, 2.203147019441267, 3.4712670904461747, 2.893480190952605, 2.7332161951361336, 2.576529248731071, 2.3488217872491286, 2.3464756183452264, 2.547183859356527, 2.312888882043529, 2.091792534168811, 2.1188958838637615, 2.05000196920414, 2.0407671707845942, 1.9767566883048762, 1.9306270630479188, 1.8811728228931597, 1.9269106978994837, 1.9426565505268454, 1.8949221576727004, 1.8668357686087884, 1.8156207831595577, 1.8548115638895961, 1.885448606252094, 1.855828046325249, 1.7581939637239201, 1.748657727216055, 1.7865615509113526, 1.840909099019228, 1.778464783568155, 1.7511604224166277, 1.9151295949394718, 1.8021865063756808, 1.6979117504185053, 1.7769250371980438, 1.705911593608787, 1.7287237167687592, 1.6817487886044993, 1.713712084008407, 1.7046474082945806, 1.852461535641766, 1.749505651997088, 1.749743934166526, 1.693847530991775, 1.7712669838419197, 1.7290460550665732]
Training:
Train_bal_acc: [0.10021264367816098, 0.30755172413793086, 0.41173563218390796, 0.4900689655172416, 0.5212068965517244, 0.55516091954023, 0.19899425287356315, 0.23764942528735641, 0.31696551724137906, 0.36355747126436777, 0.40920114942528735, 0.4349885057471265, 0.46958045977011464, 0.49109770114942597, 0.5116206896551726, 0.5174827586206895, 0.5268045977011496, 0.5458045977011496, 0.560683908045977, 0.5860402298850577, 0.5793620689655176, 0.6067183908045977, 0.6115402298850576, 0.6172931034482761, 0.6152241379310345, 0.6401091954022992, 0.6411206896551728, 0.6496264367816099, 0.6487586206896552, 0.6691436781609198, 0.6813505747126435, 0.6799022988505743, 0.6899540229885059, 0.6859252873563224, 0.6998045977011497, 0.709488505747127, 0.6975057471264375, 0.7074367816091959, 0.7146609195402297, 0.7096954022988506, 0.7100000000000004, 0.7380402298850575, 0.734051724137931, 0.7362241379310344, 0.7317126436781609, 0.7343793103448277, 0.7325919540229883, 0.7540287356321843, 0.7509080459770113, 0.7576954022988511]
Train_acc: [tensor(0.1003, device='cuda:0', dtype=torch.float64), tensor(0.3076, device='cuda:0', dtype=torch.float64), tensor(0.4117, device='cuda:0', dtype=torch.float64), tensor(0.4902, device='cuda:0', dtype=torch.float64), tensor(0.5214, device='cuda:0', dtype=torch.float64), tensor(0.5552, device='cuda:0', dtype=torch.float64), tensor(0.1990, device='cuda:0', dtype=torch.float64), tensor(0.2377, device='cuda:0', dtype=torch.float64), tensor(0.3170, device='cuda:0', dtype=torch.float64), tensor(0.3637, device='cuda:0', dtype=torch.float64), tensor(0.4092, device='cuda:0', dtype=torch.float64), tensor(0.4351, device='cuda:0', dtype=torch.float64), tensor(0.4696, device='cuda:0', dtype=torch.float64), tensor(0.4912, device='cuda:0', dtype=torch.float64), tensor(0.5117, device='cuda:0', dtype=torch.float64), tensor(0.5175, device='cuda:0', dtype=torch.float64), tensor(0.5269, device='cuda:0', dtype=torch.float64), tensor(0.5459, device='cuda:0', dtype=torch.float64), tensor(0.5607, device='cuda:0', dtype=torch.float64), tensor(0.5861, device='cuda:0', dtype=torch.float64), tensor(0.5794, device='cuda:0', dtype=torch.float64), tensor(0.6068, device='cuda:0', dtype=torch.float64), tensor(0.6116, device='cuda:0', dtype=torch.float64), tensor(0.6173, device='cuda:0', dtype=torch.float64), tensor(0.6153, device='cuda:0', dtype=torch.float64), tensor(0.6401, device='cuda:0', dtype=torch.float64), tensor(0.6411, device='cuda:0', dtype=torch.float64), tensor(0.6496, device='cuda:0', dtype=torch.float64), tensor(0.6488, device='cuda:0', dtype=torch.float64), tensor(0.6692, device='cuda:0', dtype=torch.float64), tensor(0.6813, device='cuda:0', dtype=torch.float64), tensor(0.6800, device='cuda:0', dtype=torch.float64), tensor(0.6900, device='cuda:0', dtype=torch.float64), tensor(0.6860, device='cuda:0', dtype=torch.float64), tensor(0.6999, device='cuda:0', dtype=torch.float64), tensor(0.7095, device='cuda:0', dtype=torch.float64), tensor(0.6975, device='cuda:0', dtype=torch.float64), tensor(0.7075, device='cuda:0', dtype=torch.float64), tensor(0.7147, device='cuda:0', dtype=torch.float64), tensor(0.7097, device='cuda:0', dtype=torch.float64), tensor(0.7100, device='cuda:0', dtype=torch.float64), tensor(0.7381, device='cuda:0', dtype=torch.float64), tensor(0.7341, device='cuda:0', dtype=torch.float64), tensor(0.7362, device='cuda:0', dtype=torch.float64), tensor(0.7317, device='cuda:0', dtype=torch.float64), tensor(0.7344, device='cuda:0', dtype=torch.float64), tensor(0.7326, device='cuda:0', dtype=torch.float64), tensor(0.7541, device='cuda:0', dtype=torch.float64), tensor(0.7509, device='cuda:0', dtype=torch.float64), tensor(0.7578, device='cuda:0', dtype=torch.float64)]
Train_loss: [4.422470678836057, 2.8336336995348517, 2.2985856626126857, 1.974897793900939, 1.8134234679473173, 1.6973897836826466, 3.5656763039551698, 3.243452703789707, 2.8309645981322458, 2.578307000167536, 2.352794117556837, 2.2434096125153093, 2.0962997579876883, 1.995669014341719, 1.8995170516095878, 1.8682983474489605, 1.8351274873958177, 1.7586293601576073, 1.709462801973383, 1.6088897700225429, 1.600679047870604, 1.5106224794963776, 1.4781041942837638, 1.4967475365828704, 1.4500736103958394, 1.3700497399817955, 1.3522720675409576, 1.3415425877034923, 1.3378856729518425, 1.2708611563917076, 1.2450955593470616, 1.199379632263769, 1.2085048517943782, 1.2167545402093771, 1.1649048543191172, 1.1258286600475673, 1.1503122373147532, 1.1293557279699438, 1.0979746252765725, 1.1006895811668347, 1.0816600746737746, 1.021667101520835, 1.0260388669189628, 1.0131153709537633, 1.007210212108649, 1.0196570056177674, 1.0248843078260068, 0.9627988154107744, 0.9565029631843955, 0.9050700526496829]


__________________________________________


traced_optimizer = nni.trace(optim.SGD)(model.parameters(), lr=0.001, momentum=0.9)
# Operation types to be pruned and operation partial names to be pruned in vgg16
#Prune 10% of the filters 
config_list = [{'op_types': ['Conv2d','Linear'], 
'sparsity_per_layer': 0.2}]
#criterion (Callable[[Tensor, Tensor], Tensor]) – The criterion function used in trainer. Take model output and target value as input, and return the loss.
criterion = nn.CrossEntropyLoss()
lr_scheduler = nni.trace(torch.optim.lr_scheduler.StepLR)(traced_optimizer, step_size=7, gamma=0.1)
evaluator = TorchEvaluator(training_func=training_model, optimizers=traced_optimizer,criterion=criterion, lr_schedulers=lr_scheduler)
# warm_up_step – The total optimizer.step() number before start pruning for warm up. Make sure warm_up_step is smaller than cool_down_beginning_step.
# cool_down_beginning_step – The number of steps at which sparsity stops growing, note that the sparsity stop growing doesn’t mean masks not changed.
warm_up_step = len(train_ds) // BATCH_SIZE * 6
cool_down_begin_step = len(train_ds) // BATCH_SIZE * 8
pruner = MovementPruner(model, config_list, evaluator, warm_up_step=warm_up_step, cool_down_beginning_step=cool_down_begin_step, training_epochs=50,  movement_mode='hard')

_, masks = pruner.compress()

Train Acc: 0.0961
Val Acc: 0.2855
Train Acc: 0.3048
Val Acc: 0.4139
Train Acc: 0.4206
Val Acc: 0.4686
Train Acc: 0.4960
Val Acc: 0.5067
Train Acc: 0.5285
Val Acc: 0.5364
Train Acc: 0.5659
Val Acc: 0.5330
Train Acc: 0.3937
Val Acc: 0.3417
Train Acc: 0.4556
Val Acc: 0.4610
Train Acc: 0.5354
Val Acc: 0.4793
Train Acc: 0.5661
Val Acc: 0.5166
Train Acc: 0.5933
Val Acc: 0.5526
Train Acc: 0.6013
Val Acc: 0.5538
Train Acc: 0.6175
Val Acc: 0.5606
Train Acc: 0.6456
Val Acc: 0.5450
Train Acc: 0.6560
Val Acc: 0.5727
Train Acc: 0.6705
Val Acc: 0.5647
Train Acc: 0.6695
Val Acc: 0.5590
Train Acc: 0.6823
Val Acc: 0.5918
Train Acc: 0.6897
Val Acc: 0.5661
Train Acc: 0.7070
Val Acc: 0.5880


__________________________________________


#If finetuning == False -> Feature extraction
def imageNetPruningCUBL1(pruningAmount, epochs, finetuning): 
  model = vgg16(weights='IMAGENET1K_V1')
  model.classifier[6] = nn.Linear(4096, 200) 

  features = []
  classifier = []

  for n,p in model.named_parameters():
    if n.split('.')[0] == 'features' and n.split('.')[2] == 'weight':
      features.append(int(n.split('.')[1]))
    elif n.split('.')[0] == 'classifier' and n.split('.')[2] == 'weight':
      classifier.append(int(n.split('.')[1]))

  for x in features:
    prune.l1_unstructured(model.features[x], 'weight', amount=pruningAmount)
  for x in classifier:
    prune.l1_unstructured(model.classifier[x], 'weight', amount=pruningAmount)

  if(finetuning):
    optimizer_ft = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)
  else:
    optimizer_ft = optim.SGD(model.classifier[6].parameters(), lr=0.001, momentum=0.9)

  criterion = nn.CrossEntropyLoss()

  exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)

  model = model.to(device)

  model_conv = train_model(model, criterion, optimizer_ft, exp_lr_scheduler, ds, ds_sizes, 200, num_epochs=epochs)


Pruning 10% ... with Fine Tuning
Epoch 0/24
----------
train Loss: 4.4232 Acc: 0.0968 Balanced Acc: 0.0967
val Loss: 2.9785 Acc: 0.3191 Balanced Acc: 0.3213

Epoch 1/24
----------
train Loss: 2.8508 Acc: 0.3086 Balanced Acc: 0.3086
val Loss: 2.2375 Acc: 0.4392 Balanced Acc: 0.4422

Epoch 2/24
----------
train Loss: 2.3090 Acc: 0.4209 Balanced Acc: 0.4209
val Loss: 2.0219 Acc: 0.4769 Balanced Acc: 0.4792

Epoch 3/24
----------
train Loss: 2.0309 Acc: 0.4750 Balanced Acc: 0.4749
val Loss: 1.9620 Acc: 0.4945 Balanced Acc: 0.4942

Epoch 4/24
----------
train Loss: 1.8396 Acc: 0.5229 Balanced Acc: 0.5228
val Loss: 1.8494 Acc: 0.5250 Balanced Acc: 0.5278

Epoch 5/24
----------
train Loss: 1.6502 Acc: 0.5654 Balanced Acc: 0.5653
val Loss: 1.7487 Acc: 0.5518 Balanced Acc: 0.5545

Epoch 6/24
----------
train Loss: 1.5623 Acc: 0.5966 Balanced Acc: 0.5966
val Loss: 1.6854 Acc: 0.5664 Balanced Acc: 0.5683

Epoch 7/24
----------
train Loss: 1.2697 Acc: 0.6577 Balanced Acc: 0.6576
val Loss: 1.5381 Acc: 0.6029 Balanced Acc: 0.6068

Epoch 8/24
----------
train Loss: 1.1964 Acc: 0.6820 Balanced Acc: 0.6820
val Loss: 1.4820 Acc: 0.6217 Balanced Acc: 0.6228

Epoch 9/24
----------
train Loss: 1.1605 Acc: 0.6919 Balanced Acc: 0.6918
val Loss: 1.4558 Acc: 0.6239 Balanced Acc: 0.6264

Epoch 10/24
----------
train Loss: 1.1088 Acc: 0.7020 Balanced Acc: 0.7020
val Loss: 1.4754 Acc: 0.6269 Balanced Acc: 0.6290

Epoch 11/24
----------
train Loss: 1.1403 Acc: 0.6970 Balanced Acc: 0.6970
val Loss: 1.4865 Acc: 0.6158 Balanced Acc: 0.6184

Epoch 12/24
----------
train Loss: 1.1216 Acc: 0.6964 Balanced Acc: 0.6964
val Loss: 1.4414 Acc: 0.6289 Balanced Acc: 0.6316

Epoch 13/24
----------
train Loss: 1.0855 Acc: 0.7080 Balanced Acc: 0.7080
val Loss: 1.4349 Acc: 0.6322 Balanced Acc: 0.6343

Epoch 14/24
----------
train Loss: 1.0685 Acc: 0.7105 Balanced Acc: 0.7105
val Loss: 1.4215 Acc: 0.6236 Balanced Acc: 0.6259

Epoch 15/24
----------
train Loss: 1.0728 Acc: 0.7212 Balanced Acc: 0.7212
val Loss: 1.4246 Acc: 0.6296 Balanced Acc: 0.6321

Epoch 16/24
----------
train Loss: 1.0599 Acc: 0.7192 Balanced Acc: 0.7192
val Loss: 1.4445 Acc: 0.6324 Balanced Acc: 0.6355

Epoch 17/24
----------
train Loss: 1.0329 Acc: 0.7150 Balanced Acc: 0.7150
val Loss: 1.4629 Acc: 0.6298 Balanced Acc: 0.6317

Epoch 18/24
----------
train Loss: 0.9958 Acc: 0.7344 Balanced Acc: 0.7344
val Loss: 1.4787 Acc: 0.6248 Balanced Acc: 0.6266

Epoch 19/24
----------
train Loss: 1.0674 Acc: 0.7145 Balanced Acc: 0.7146
val Loss: 1.4489 Acc: 0.6253 Balanced Acc: 0.6285

Epoch 20/24
----------
train Loss: 1.0319 Acc: 0.7219 Balanced Acc: 0.7219
val Loss: 1.4389 Acc: 0.6274 Balanced Acc: 0.6290

Epoch 21/24
----------
train Loss: 1.0115 Acc: 0.7231 Balanced Acc: 0.7231
val Loss: 1.4179 Acc: 0.6296 Balanced Acc: 0.6311

Epoch 22/24
----------
train Loss: 1.0640 Acc: 0.7199 Balanced Acc: 0.7198
val Loss: 1.4438 Acc: 0.6348 Balanced Acc: 0.6365

Epoch 23/24
----------
train Loss: 1.0799 Acc: 0.7154 Balanced Acc: 0.7154
val Loss: 1.4343 Acc: 0.6360 Balanced Acc: 0.6393

Epoch 24/24
----------
train Loss: 1.0358 Acc: 0.7199 Balanced Acc: 0.7198
val Loss: 1.4389 Acc: 0.6367 Balanced Acc: 0.6396

Training complete in 106m 59s
Best val Balanced Acc: 0.639599
Validation:
Val_bal_acc: [0.32126503514736376, 0.4422435432461076, 0.47923154787673844, 0.4941840699397514, 0.5278129861479105, 0.5545353369936538, 0.5683144852632545, 0.6068176000573419, 0.622798367361178, 0.6263509032138795, 0.6289772048875457, 0.6184422382453434, 0.6315976774948605, 0.6343370097248027, 0.6259343843488283, 0.6321153343063703, 0.6354739219532088, 0.6317134409577199, 0.6266173532214329, 0.6284773275674993, 0.6290494014192234, 0.631085367578463, 0.636514184805612, 0.6393244358397248, 0.6395986428789046]
Val_acc: [tensor(0.3191, device='cuda:0', dtype=torch.float64), tensor(0.4392, device='cuda:0', dtype=torch.float64), tensor(0.4769, device='cuda:0', dtype=torch.float64), tensor(0.4945, device='cuda:0', dtype=torch.float64), tensor(0.5250, device='cuda:0', dtype=torch.float64), tensor(0.5518, device='cuda:0', dtype=torch.float64), tensor(0.5664, device='cuda:0', dtype=torch.float64), tensor(0.6029, device='cuda:0', dtype=torch.float64), tensor(0.6217, device='cuda:0', dtype=torch.float64), tensor(0.6239, device='cuda:0', dtype=torch.float64), tensor(0.6269, device='cuda:0', dtype=torch.float64), tensor(0.6158, device='cuda:0', dtype=torch.float64), tensor(0.6289, device='cuda:0', dtype=torch.float64), tensor(0.6322, device='cuda:0', dtype=torch.float64), tensor(0.6236, device='cuda:0', dtype=torch.float64), tensor(0.6296, device='cuda:0', dtype=torch.float64), tensor(0.6324, device='cuda:0', dtype=torch.float64), tensor(0.6298, device='cuda:0', dtype=torch.float64), tensor(0.6248, device='cuda:0', dtype=torch.float64), tensor(0.6253, device='cuda:0', dtype=torch.float64), tensor(0.6274, device='cuda:0', dtype=torch.float64), tensor(0.6296, device='cuda:0', dtype=torch.float64), tensor(0.6348, device='cuda:0', dtype=torch.float64), tensor(0.6360, device='cuda:0', dtype=torch.float64), tensor(0.6367, device='cuda:0', dtype=torch.float64)]
Val_loss: [2.978539428506837, 2.23750788946583, 2.021928932771092, 1.96200857568927, 1.8493685882010047, 1.7486876025358726, 1.6854095396231814, 1.5381418541048082, 1.4819882779437543, 1.4557673407491576, 1.4753534347360615, 1.486452020856484, 1.4414283616206627, 1.4348768879591802, 1.4215319441450767, 1.4245571159360004, 1.4444796181893735, 1.4628714580226447, 1.478716853490401, 1.4488556342590282, 1.4388855038159956, 1.4178921359386727, 1.4437939266149888, 1.4343141613230443, 1.4388828666962383]
Training:
Train_bal_acc: [0.09672988505747128, 0.3085919540229884, 0.420896551724138, 0.4749252873563222, 0.5228160919540229, 0.5653333333333334, 0.5965919540229888, 0.6575919540229888, 0.6819885057471267, 0.6918160919540235, 0.7020057471264368, 0.6969655172413796, 0.6963678160919541, 0.7080287356321838, 0.7105287356321839, 0.7212011494252876, 0.719195402298851, 0.7150000000000007, 0.7344022988505747, 0.714580459770115, 0.7218563218390806, 0.7230517241379314, 0.7198390804597701, 0.7153505747126436, 0.7198390804597704]
Train_acc: [tensor(0.0968, device='cuda:0', dtype=torch.float64), tensor(0.3086, device='cuda:0', dtype=torch.float64), tensor(0.4209, device='cuda:0', dtype=torch.float64), tensor(0.4750, device='cuda:0', dtype=torch.float64), tensor(0.5229, device='cuda:0', dtype=torch.float64), tensor(0.5654, device='cuda:0', dtype=torch.float64), tensor(0.5966, device='cuda:0', dtype=torch.float64), tensor(0.6577, device='cuda:0', dtype=torch.float64), tensor(0.6820, device='cuda:0', dtype=torch.float64), tensor(0.6919, device='cuda:0', dtype=torch.float64), tensor(0.7020, device='cuda:0', dtype=torch.float64), tensor(0.6970, device='cuda:0', dtype=torch.float64), tensor(0.6964, device='cuda:0', dtype=torch.float64), tensor(0.7080, device='cuda:0', dtype=torch.float64), tensor(0.7105, device='cuda:0', dtype=torch.float64), tensor(0.7212, device='cuda:0', dtype=torch.float64), tensor(0.7192, device='cuda:0', dtype=torch.float64), tensor(0.7150, device='cuda:0', dtype=torch.float64), tensor(0.7344, device='cuda:0', dtype=torch.float64), tensor(0.7145, device='cuda:0', dtype=torch.float64), tensor(0.7219, device='cuda:0', dtype=torch.float64), tensor(0.7231, device='cuda:0', dtype=torch.float64), tensor(0.7199, device='cuda:0', dtype=torch.float64), tensor(0.7154, device='cuda:0', dtype=torch.float64), tensor(0.7199, device='cuda:0', dtype=torch.float64)]
Train_loss: [4.423200580013328, 2.850825318822393, 2.3090103345192547, 2.0309442674314178, 1.8395526836743386, 1.6501633266548257, 1.5623038605049446, 1.2696540889479058, 1.1963874748797667, 1.1605475112363264, 1.1088467131545634, 1.1403090492025152, 1.121595368689205, 1.0854928709086793, 1.068512813027477, 1.0727800075157428, 1.0599366231763367, 1.032886074668851, 0.9958093914342874, 1.0673940026406092, 1.0318601902699844, 1.0114774934482607, 1.0640298517934872, 1.0798672263249183, 1.035802326283536]
Pruning 10% ... with Feature Extraction
Epoch 0/24
----------
train Loss: 4.6109 Acc: 0.0921 Balanced Acc: 0.0921
val Loss: 3.7104 Acc: 0.2741 Balanced Acc: 0.2762

Epoch 1/24
----------
train Loss: 3.3972 Acc: 0.2719 Balanced Acc: 0.2719
val Loss: 3.0301 Acc: 0.3723 Balanced Acc: 0.3754

Epoch 2/24
----------
train Loss: 2.9103 Acc: 0.3483 Balanced Acc: 0.3483
val Loss: 2.7241 Acc: 0.4182 Balanced Acc: 0.4200

Epoch 3/24
----------
train Loss: 2.6449 Acc: 0.3882 Balanced Acc: 0.3882
val Loss: 2.5162 Acc: 0.4346 Balanced Acc: 0.4378

Epoch 4/24
----------
train Loss: 2.4467 Acc: 0.4281 Balanced Acc: 0.4281
val Loss: 2.3659 Acc: 0.4644 Balanced Acc: 0.4682

Epoch 5/24
----------
train Loss: 2.3120 Acc: 0.4431 Balanced Acc: 0.4431
val Loss: 2.2724 Acc: 0.4688 Balanced Acc: 0.4711

Epoch 6/24
----------
train Loss: 2.2146 Acc: 0.4630 Balanced Acc: 0.4630
val Loss: 2.2205 Acc: 0.4758 Balanced Acc: 0.4785

Epoch 7/24
----------
train Loss: 2.1603 Acc: 0.4778 Balanced Acc: 0.4777
val Loss: 2.2097 Acc: 0.4877 Balanced Acc: 0.4906

Epoch 8/24
----------
train Loss: 2.1085 Acc: 0.4917 Balanced Acc: 0.4916
val Loss: 2.1770 Acc: 0.4890 Balanced Acc: 0.4924

Epoch 9/24
----------
train Loss: 2.0957 Acc: 0.4915 Balanced Acc: 0.4915
val Loss: 2.1725 Acc: 0.4888 Balanced Acc: 0.4917

Epoch 10/24
----------
train Loss: 2.0867 Acc: 0.4957 Balanced Acc: 0.4957
val Loss: 2.1601 Acc: 0.4959 Balanced Acc: 0.4984

Epoch 11/24
----------
train Loss: 2.0569 Acc: 0.5040 Balanced Acc: 0.5040
val Loss: 2.1538 Acc: 0.4972 Balanced Acc: 0.5009

Epoch 12/24
----------
train Loss: 2.0553 Acc: 0.5075 Balanced Acc: 0.5075
val Loss: 2.1690 Acc: 0.4941 Balanced Acc: 0.4979

Epoch 13/24
----------
train Loss: 2.0704 Acc: 0.4992 Balanced Acc: 0.4991
val Loss: 2.1439 Acc: 0.5019 Balanced Acc: 0.5049

Epoch 14/24
----------
train Loss: 2.0477 Acc: 0.5077 Balanced Acc: 0.5077
val Loss: 2.1515 Acc: 0.4960 Balanced Acc: 0.5002

Epoch 15/24
----------
train Loss: 2.0454 Acc: 0.5120 Balanced Acc: 0.5120
val Loss: 2.1180 Acc: 0.5085 Balanced Acc: 0.5119

Epoch 16/24
----------
train Loss: 2.0500 Acc: 0.5035 Balanced Acc: 0.5035
val Loss: 2.1592 Acc: 0.4978 Balanced Acc: 0.5013

Epoch 17/24
----------
train Loss: 2.0508 Acc: 0.5080 Balanced Acc: 0.5081
val Loss: 2.1391 Acc: 0.5002 Balanced Acc: 0.5042

Epoch 18/24
----------
train Loss: 2.0276 Acc: 0.5102 Balanced Acc: 0.5102
val Loss: 2.1649 Acc: 0.4928 Balanced Acc: 0.4966

Epoch 19/24
----------
train Loss: 2.0343 Acc: 0.5040 Balanced Acc: 0.5041
val Loss: 2.1552 Acc: 0.4917 Balanced Acc: 0.4950

Epoch 20/24
----------
train Loss: 2.0724 Acc: 0.4953 Balanced Acc: 0.4953
val Loss: 2.1474 Acc: 0.4952 Balanced Acc: 0.4989

Epoch 21/24
----------
train Loss: 2.0354 Acc: 0.5042 Balanced Acc: 0.5042
val Loss: 2.1787 Acc: 0.4874 Balanced Acc: 0.4914

Epoch 22/24
----------
train Loss: 2.0497 Acc: 0.5005 Balanced Acc: 0.5004
val Loss: 2.1417 Acc: 0.4993 Balanced Acc: 0.5023

Epoch 23/24
----------
train Loss: 2.0514 Acc: 0.5047 Balanced Acc: 0.5047
val Loss: 2.1274 Acc: 0.5014 Balanced Acc: 0.5036

Epoch 24/24
----------
train Loss: 2.0447 Acc: 0.5077 Balanced Acc: 0.5076
val Loss: 2.1340 Acc: 0.4953 Balanced Acc: 0.5000

Training complete in 106m 60s
Best val Balanced Acc: 0.511890
Validation:
Val_bal_acc: [0.27624911784414274, 0.37535791065462426, 0.4199906955074438, 0.4378000654258051, 0.4682470945752328, 0.47109720643049363, 0.47853109592681903, 0.4905935708929278, 0.49240787234062317, 0.4916764127707073, 0.4983746326750746, 0.500905668131956, 0.49785100994043247, 0.5049424549056247, 0.5001618887971172, 0.5118897359417166, 0.5013255394053151, 0.50418921543312, 0.4966283098650537, 0.4950392804806913, 0.49888581972297324, 0.49144567995678623, 0.5022532436373083, 0.5035984911080981, 0.49995854846557103]
Val_acc: [tensor(0.2741, device='cuda:0', dtype=torch.float64), tensor(0.3723, device='cuda:0', dtype=torch.float64), tensor(0.4182, device='cuda:0', dtype=torch.float64), tensor(0.4346, device='cuda:0', dtype=torch.float64), tensor(0.4644, device='cuda:0', dtype=torch.float64), tensor(0.4688, device='cuda:0', dtype=torch.float64), tensor(0.4758, device='cuda:0', dtype=torch.float64), tensor(0.4877, device='cuda:0', dtype=torch.float64), tensor(0.4890, device='cuda:0', dtype=torch.float64), tensor(0.4888, device='cuda:0', dtype=torch.float64), tensor(0.4959, device='cuda:0', dtype=torch.float64), tensor(0.4972, device='cuda:0', dtype=torch.float64), tensor(0.4941, device='cuda:0', dtype=torch.float64), tensor(0.5019, device='cuda:0', dtype=torch.float64), tensor(0.4960, device='cuda:0', dtype=torch.float64), tensor(0.5085, device='cuda:0', dtype=torch.float64), tensor(0.4978, device='cuda:0', dtype=torch.float64), tensor(0.5002, device='cuda:0', dtype=torch.float64), tensor(0.4928, device='cuda:0', dtype=torch.float64), tensor(0.4917, device='cuda:0', dtype=torch.float64), tensor(0.4952, device='cuda:0', dtype=torch.float64), tensor(0.4874, device='cuda:0', dtype=torch.float64), tensor(0.4993, device='cuda:0', dtype=torch.float64), tensor(0.5014, device='cuda:0', dtype=torch.float64), tensor(0.4953, device='cuda:0', dtype=torch.float64)]
Val_loss: [3.710444220518877, 3.030149854212989, 2.724136730000855, 2.516234521747335, 2.365921157123222, 2.2724021815167323, 2.2205327509558113, 2.2096925873076625, 2.1770156105510274, 2.172523016266301, 2.1600752041839426, 2.1538021069047035, 2.1690495864333554, 2.1438568868347394, 2.1515004265954425, 2.1180239849391786, 2.1591748936364765, 2.1391485000751245, 2.164861446995549, 2.155217263054181, 2.147412993474216, 2.178722316987522, 2.1417094167035162, 2.127422937554329, 2.1340221496215968]
Training:
Train_bal_acc: [0.09206896551724142, 0.27191379310344854, 0.34830459770114947, 0.38817241379310347, 0.428051724137931, 0.4430632183908046, 0.4629540229885055, 0.4777356321839081, 0.49163218390804597, 0.49148850574712655, 0.4956896551724139, 0.503971264367816, 0.507494252873563, 0.4991436781609191, 0.5076724137931033, 0.5119770114942531, 0.5034999999999998, 0.508051724137931, 0.510183908045977, 0.5040517241379311, 0.49530459770114954, 0.5041896551724142, 0.5004482758620693, 0.5046666666666662, 0.5076379310344826]
Train_acc: [tensor(0.0921, device='cuda:0', dtype=torch.float64), tensor(0.2719, device='cuda:0', dtype=torch.float64), tensor(0.3483, device='cuda:0', dtype=torch.float64), tensor(0.3882, device='cuda:0', dtype=torch.float64), tensor(0.4281, device='cuda:0', dtype=torch.float64), tensor(0.4431, device='cuda:0', dtype=torch.float64), tensor(0.4630, device='cuda:0', dtype=torch.float64), tensor(0.4778, device='cuda:0', dtype=torch.float64), tensor(0.4917, device='cuda:0', dtype=torch.float64), tensor(0.4915, device='cuda:0', dtype=torch.float64), tensor(0.4957, device='cuda:0', dtype=torch.float64), tensor(0.5040, device='cuda:0', dtype=torch.float64), tensor(0.5075, device='cuda:0', dtype=torch.float64), tensor(0.4992, device='cuda:0', dtype=torch.float64), tensor(0.5077, device='cuda:0', dtype=torch.float64), tensor(0.5120, device='cuda:0', dtype=torch.float64), tensor(0.5035, device='cuda:0', dtype=torch.float64), tensor(0.5080, device='cuda:0', dtype=torch.float64), tensor(0.5102, device='cuda:0', dtype=torch.float64), tensor(0.5040, device='cuda:0', dtype=torch.float64), tensor(0.4953, device='cuda:0', dtype=torch.float64), tensor(0.5042, device='cuda:0', dtype=torch.float64), tensor(0.5005, device='cuda:0', dtype=torch.float64), tensor(0.5047, device='cuda:0', dtype=torch.float64), tensor(0.5077, device='cuda:0', dtype=torch.float64)]
Train_loss: [4.610900699117798, 3.397201139130591, 2.9103351031536655, 2.644899769468947, 2.4466972425853486, 2.312020414226406, 2.2146372236647047, 2.1603124323072613, 2.1085158271872286, 2.095747818181544, 2.086739398258147, 2.056948544543944, 2.055339138787072, 2.07040272329424, 2.047692892429707, 2.0454185373352733, 2.0499518885785912, 2.05083777425605, 2.027571548888951, 2.034337966490635, 2.0723689075784364, 2.0354172616232464, 2.0496863859034713, 2.0514123800559005, 2.044659054871993]
Pruning 20% ... with Fine Tuning
Epoch 0/24
----------
train Loss: 4.5337 Acc: 0.0911 Balanced Acc: 0.0911
val Loss: 3.0551 Acc: 0.2799 Balanced Acc: 0.2824

Epoch 1/24
----------
train Loss: 2.8797 Acc: 0.3001 Balanced Acc: 0.3001
val Loss: 2.2894 Acc: 0.4187 Balanced Acc: 0.4230

Epoch 2/24
----------
train Loss: 2.3721 Acc: 0.4044 Balanced Acc: 0.4044
val Loss: 2.0650 Acc: 0.4732 Balanced Acc: 0.4775

Epoch 3/24
----------
train Loss: 2.0443 Acc: 0.4673 Balanced Acc: 0.4672
val Loss: 1.9143 Acc: 0.5035 Balanced Acc: 0.5057

Epoch 4/24
----------
train Loss: 1.8558 Acc: 0.5133 Balanced Acc: 0.5133
val Loss: 1.8398 Acc: 0.5274 Balanced Acc: 0.5307

Epoch 5/24
----------
train Loss: 1.7220 Acc: 0.5546 Balanced Acc: 0.5546
val Loss: 1.7852 Acc: 0.5473 Balanced Acc: 0.5488

Epoch 6/24
----------
train Loss: 1.5907 Acc: 0.5858 Balanced Acc: 0.5857
val Loss: 1.7318 Acc: 0.5549 Balanced Acc: 0.5571

Epoch 7/24
----------
train Loss: 1.2936 Acc: 0.6547 Balanced Acc: 0.6546
val Loss: 1.5356 Acc: 0.6063 Balanced Acc: 0.6078

Epoch 8/24
----------
train Loss: 1.2051 Acc: 0.6718 Balanced Acc: 0.6717
val Loss: 1.5222 Acc: 0.6136 Balanced Acc: 0.6152

Epoch 9/24
----------
train Loss: 1.1952 Acc: 0.6760 Balanced Acc: 0.6759
val Loss: 1.5074 Acc: 0.6137 Balanced Acc: 0.6160

Epoch 10/24
----------
train Loss: 1.1753 Acc: 0.6807 Balanced Acc: 0.6806
val Loss: 1.4581 Acc: 0.6308 Balanced Acc: 0.6328

Epoch 11/24
----------
train Loss: 1.1495 Acc: 0.6924 Balanced Acc: 0.6923
val Loss: 1.4960 Acc: 0.6220 Balanced Acc: 0.6239

Epoch 12/24
----------
train Loss: 1.1453 Acc: 0.6930 Balanced Acc: 0.6930
val Loss: 1.4596 Acc: 0.6298 Balanced Acc: 0.6315

Epoch 13/24
----------
train Loss: 1.1327 Acc: 0.6957 Balanced Acc: 0.6957
val Loss: 1.4760 Acc: 0.6196 Balanced Acc: 0.6208

Epoch 14/24
----------
train Loss: 1.1152 Acc: 0.6979 Balanced Acc: 0.6979
val Loss: 1.4470 Acc: 0.6275 Balanced Acc: 0.6297

Epoch 15/24
----------
train Loss: 1.0642 Acc: 0.7114 Balanced Acc: 0.7113
val Loss: 1.4144 Acc: 0.6353 Balanced Acc: 0.6366

Epoch 16/24
----------
train Loss: 1.0809 Acc: 0.7072 Balanced Acc: 0.7072
val Loss: 1.4569 Acc: 0.6248 Balanced Acc: 0.6265

Epoch 17/24
----------
train Loss: 1.0666 Acc: 0.7120 Balanced Acc: 0.7120
val Loss: 1.4311 Acc: 0.6313 Balanced Acc: 0.6338

Epoch 18/24
----------
train Loss: 1.1172 Acc: 0.6984 Balanced Acc: 0.6983
val Loss: 1.4409 Acc: 0.6263 Balanced Acc: 0.6287

Epoch 19/24
----------
train Loss: 1.0975 Acc: 0.7019 Balanced Acc: 0.7018
val Loss: 1.4345 Acc: 0.6327 Balanced Acc: 0.6332

Epoch 20/24
----------
train Loss: 1.0752 Acc: 0.7100 Balanced Acc: 0.7100
val Loss: 1.4789 Acc: 0.6246 Balanced Acc: 0.6275

Epoch 21/24
----------
train Loss: 1.0887 Acc: 0.7075 Balanced Acc: 0.7075
val Loss: 1.4314 Acc: 0.6332 Balanced Acc: 0.6353

Epoch 22/24
----------
train Loss: 1.1107 Acc: 0.7055 Balanced Acc: 0.7055
val Loss: 1.4810 Acc: 0.6205 Balanced Acc: 0.6228

Epoch 23/24
----------
train Loss: 1.0864 Acc: 0.7064 Balanced Acc: 0.7063
val Loss: 1.4336 Acc: 0.6324 Balanced Acc: 0.6341

Epoch 24/24
----------
train Loss: 1.0694 Acc: 0.7110 Balanced Acc: 0.7110
val Loss: 1.4288 Acc: 0.6395 Balanced Acc: 0.6412

Training complete in 109m 28s
Best val Balanced Acc: 0.641222
Validation:
Val_bal_acc: [0.282384353073535, 0.4229866308002502, 0.477524390031413, 0.5057414816034915, 0.5306505302769806, 0.5487542926028093, 0.5571398339613047, 0.6078058982000434, 0.6152354169031358, 0.6159602748356788, 0.6328076470057847, 0.6238608702159956, 0.6314755376290334, 0.620811436139634, 0.6297475816346052, 0.6366280129896082, 0.6265256166791321, 0.6337915211755465, 0.6286841971881428, 0.6332115243219358, 0.6274557960268922, 0.6353360096961063, 0.6227970278141508, 0.6340865793143474, 0.6412218419390362]
Val_acc: [tensor(0.2799, device='cuda:0', dtype=torch.float64), tensor(0.4187, device='cuda:0', dtype=torch.float64), tensor(0.4732, device='cuda:0', dtype=torch.float64), tensor(0.5035, device='cuda:0', dtype=torch.float64), tensor(0.5274, device='cuda:0', dtype=torch.float64), tensor(0.5473, device='cuda:0', dtype=torch.float64), tensor(0.5549, device='cuda:0', dtype=torch.float64), tensor(0.6063, device='cuda:0', dtype=torch.float64), tensor(0.6136, device='cuda:0', dtype=torch.float64), tensor(0.6137, device='cuda:0', dtype=torch.float64), tensor(0.6308, device='cuda:0', dtype=torch.float64), tensor(0.6220, device='cuda:0', dtype=torch.float64), tensor(0.6298, device='cuda:0', dtype=torch.float64), tensor(0.6196, device='cuda:0', dtype=torch.float64), tensor(0.6275, device='cuda:0', dtype=torch.float64), tensor(0.6353, device='cuda:0', dtype=torch.float64), tensor(0.6248, device='cuda:0', dtype=torch.float64), tensor(0.6313, device='cuda:0', dtype=torch.float64), tensor(0.6263, device='cuda:0', dtype=torch.float64), tensor(0.6327, device='cuda:0', dtype=torch.float64), tensor(0.6246, device='cuda:0', dtype=torch.float64), tensor(0.6332, device='cuda:0', dtype=torch.float64), tensor(0.6205, device='cuda:0', dtype=torch.float64), tensor(0.6324, device='cuda:0', dtype=torch.float64), tensor(0.6395, device='cuda:0', dtype=torch.float64)]
Val_loss: [3.0550856432257993, 2.289407632085098, 2.064996408189952, 1.9142656022612703, 1.8397665651277875, 1.785154005145467, 1.7317826396479952, 1.5356323940407461, 1.5222446813145547, 1.507384904629744, 1.4581458432420518, 1.4959972881225623, 1.4595515066548836, 1.475999771818277, 1.447047611701723, 1.4143606233810777, 1.4569225630893001, 1.431116248230544, 1.4409464741312803, 1.4345402970658034, 1.4789377727054258, 1.431431499210501, 1.4810499500561716, 1.4336474679299869, 1.428796988871742]
Training:
Train_bal_acc: [0.09109770114942525, 0.3000689655172415, 0.4043620689655173, 0.46718390804597687, 0.513264367816092, 0.5545517241379313, 0.5857011494252875, 0.6546149425287356, 0.6717471264367816, 0.6759425287356325, 0.6806321839080464, 0.6923218390804603, 0.6929827586206896, 0.695683908045977, 0.6978620689655177, 0.7113218390804603, 0.7071896551724142, 0.7120057471264367, 0.6983448275862071, 0.7018218390804595, 0.7100000000000005, 0.7075114942528735, 0.7055229885057475, 0.7063160919540229, 0.7110000000000002]
Train_acc: [tensor(0.0911, device='cuda:0', dtype=torch.float64), tensor(0.3001, device='cuda:0', dtype=torch.float64), tensor(0.4044, device='cuda:0', dtype=torch.float64), tensor(0.4673, device='cuda:0', dtype=torch.float64), tensor(0.5133, device='cuda:0', dtype=torch.float64), tensor(0.5546, device='cuda:0', dtype=torch.float64), tensor(0.5858, device='cuda:0', dtype=torch.float64), tensor(0.6547, device='cuda:0', dtype=torch.float64), tensor(0.6718, device='cuda:0', dtype=torch.float64), tensor(0.6760, device='cuda:0', dtype=torch.float64), tensor(0.6807, device='cuda:0', dtype=torch.float64), tensor(0.6924, device='cuda:0', dtype=torch.float64), tensor(0.6930, device='cuda:0', dtype=torch.float64), tensor(0.6957, device='cuda:0', dtype=torch.float64), tensor(0.6979, device='cuda:0', dtype=torch.float64), tensor(0.7114, device='cuda:0', dtype=torch.float64), tensor(0.7072, device='cuda:0', dtype=torch.float64), tensor(0.7120, device='cuda:0', dtype=torch.float64), tensor(0.6984, device='cuda:0', dtype=torch.float64), tensor(0.7019, device='cuda:0', dtype=torch.float64), tensor(0.7100, device='cuda:0', dtype=torch.float64), tensor(0.7075, device='cuda:0', dtype=torch.float64), tensor(0.7055, device='cuda:0', dtype=torch.float64), tensor(0.7064, device='cuda:0', dtype=torch.float64), tensor(0.7110, device='cuda:0', dtype=torch.float64)]
Train_loss: [4.5337260254709415, 2.8797165402818767, 2.372140336521952, 2.0442513845982773, 1.8557759606921755, 1.721956516768004, 1.5906884968380233, 1.2936193370246314, 1.2050765227348677, 1.1952193733609275, 1.17526668733782, 1.1495067514814772, 1.1453282248389136, 1.1327067339096142, 1.11518807308969, 1.0641674901550517, 1.0809465531392777, 1.06660796448832, 1.1171769781593168, 1.0975360741088658, 1.0752485938815224, 1.0886999857517174, 1.1106645959116517, 1.0864315429527758, 1.0694397287365591]
Pruning 20% ... with Feature Extraction
Epoch 0/24
----------
train Loss: 4.6652 Acc: 0.0826 Balanced Acc: 0.0825
val Loss: 3.8028 Acc: 0.2594 Balanced Acc: 0.2629

Epoch 1/24
----------
train Loss: 3.5406 Acc: 0.2424 Balanced Acc: 0.2423
val Loss: 3.1275 Acc: 0.3761 Balanced Acc: 0.3783

Epoch 2/24
----------
train Loss: 2.9912 Acc: 0.3300 Balanced Acc: 0.3300
val Loss: 2.8191 Acc: 0.3980 Balanced Acc: 0.4015

Epoch 3/24
----------
train Loss: 2.7431 Acc: 0.3715 Balanced Acc: 0.3715
val Loss: 2.6307 Acc: 0.4172 Balanced Acc: 0.4215

Epoch 4/24
----------
train Loss: 2.5258 Acc: 0.4082 Balanced Acc: 0.4083
val Loss: 2.4756 Acc: 0.4427 Balanced Acc: 0.4445

Epoch 5/24
----------
train Loss: 2.4032 Acc: 0.4328 Balanced Acc: 0.4327
val Loss: 2.3401 Acc: 0.4679 Balanced Acc: 0.4723

Epoch 6/24
----------
train Loss: 2.3069 Acc: 0.4404 Balanced Acc: 0.4405
val Loss: 2.3082 Acc: 0.4610 Balanced Acc: 0.4654

Epoch 7/24
----------
train Loss: 2.2380 Acc: 0.4693 Balanced Acc: 0.4693
val Loss: 2.2530 Acc: 0.4834 Balanced Acc: 0.4869

Epoch 8/24
----------
train Loss: 2.2150 Acc: 0.4733 Balanced Acc: 0.4733
val Loss: 2.2580 Acc: 0.4798 Balanced Acc: 0.4830

Epoch 9/24
----------
train Loss: 2.1781 Acc: 0.4776 Balanced Acc: 0.4777
val Loss: 2.2334 Acc: 0.4829 Balanced Acc: 0.4854

Epoch 10/24
----------
train Loss: 2.1898 Acc: 0.4746 Balanced Acc: 0.4746
val Loss: 2.2253 Acc: 0.4862 Balanced Acc: 0.4895

Epoch 11/24
----------
train Loss: 2.1671 Acc: 0.4838 Balanced Acc: 0.4838
val Loss: 2.2333 Acc: 0.4900 Balanced Acc: 0.4925

Epoch 12/24
----------
train Loss: 2.1655 Acc: 0.4788 Balanced Acc: 0.4788
val Loss: 2.2250 Acc: 0.4921 Balanced Acc: 0.4953

Epoch 13/24
----------
train Loss: 2.1328 Acc: 0.4882 Balanced Acc: 0.4881
val Loss: 2.1988 Acc: 0.5000 Balanced Acc: 0.5028

Epoch 14/24
----------
train Loss: 2.1275 Acc: 0.4905 Balanced Acc: 0.4904
val Loss: 2.2081 Acc: 0.4838 Balanced Acc: 0.4861

Epoch 15/24
----------
train Loss: 2.1264 Acc: 0.4945 Balanced Acc: 0.4945
val Loss: 2.1764 Acc: 0.4993 Balanced Acc: 0.5041

Epoch 16/24
----------
train Loss: 2.1448 Acc: 0.4912 Balanced Acc: 0.4911
val Loss: 2.2231 Acc: 0.4821 Balanced Acc: 0.4857

Epoch 17/24
----------
train Loss: 2.1104 Acc: 0.4855 Balanced Acc: 0.4855
val Loss: 2.2241 Acc: 0.4862 Balanced Acc: 0.4898

Epoch 18/24
----------
train Loss: 2.1459 Acc: 0.4873 Balanced Acc: 0.4873
val Loss: 2.2035 Acc: 0.4933 Balanced Acc: 0.4956

Epoch 19/24
----------
train Loss: 2.1602 Acc: 0.4803 Balanced Acc: 0.4803
val Loss: 2.2275 Acc: 0.4821 Balanced Acc: 0.4851

Epoch 20/24
----------
train Loss: 2.1626 Acc: 0.4823 Balanced Acc: 0.4822
val Loss: 2.1935 Acc: 0.4877 Balanced Acc: 0.4923

Epoch 21/24
----------
train Loss: 2.1401 Acc: 0.4972 Balanced Acc: 0.4971
val Loss: 2.2216 Acc: 0.4860 Balanced Acc: 0.4897

Epoch 22/24
----------
train Loss: 2.1257 Acc: 0.4910 Balanced Acc: 0.4910
val Loss: 2.2197 Acc: 0.4874 Balanced Acc: 0.4901

Epoch 23/24
----------
train Loss: 2.1281 Acc: 0.4920 Balanced Acc: 0.4919
val Loss: 2.2393 Acc: 0.4839 Balanced Acc: 0.4875

Epoch 24/24
----------
train Loss: 2.1297 Acc: 0.4922 Balanced Acc: 0.4921
val Loss: 2.2111 Acc: 0.4864 Balanced Acc: 0.4891

Training complete in 107m 40s
Best val Balanced Acc: 0.504055
Validation:
Val_bal_acc: [0.2628580646589208, 0.3782961204030603, 0.40151099673915913, 0.4215260055014061, 0.44450644340914985, 0.4723242826134997, 0.46537741364112395, 0.48686580974794125, 0.4829742762979764, 0.48538497763856825, 0.4894949972624558, 0.49254254229686245, 0.49528915917805705, 0.5027866507467829, 0.4860528877318705, 0.5040549379311314, 0.48567973836398826, 0.48978067416201637, 0.49556100704883527, 0.4851079212491072, 0.49234822794887506, 0.4897401606316621, 0.49009756996395915, 0.48749417388608995, 0.4891417370800903]
Val_acc: [tensor(0.2594, device='cuda:0', dtype=torch.float64), tensor(0.3761, device='cuda:0', dtype=torch.float64), tensor(0.3980, device='cuda:0', dtype=torch.float64), tensor(0.4172, device='cuda:0', dtype=torch.float64), tensor(0.4427, device='cuda:0', dtype=torch.float64), tensor(0.4679, device='cuda:0', dtype=torch.float64), tensor(0.4610, device='cuda:0', dtype=torch.float64), tensor(0.4834, device='cuda:0', dtype=torch.float64), tensor(0.4798, device='cuda:0', dtype=torch.float64), tensor(0.4829, device='cuda:0', dtype=torch.float64), tensor(0.4862, device='cuda:0', dtype=torch.float64), tensor(0.4900, device='cuda:0', dtype=torch.float64), tensor(0.4921, device='cuda:0', dtype=torch.float64), tensor(0.5000, device='cuda:0', dtype=torch.float64), tensor(0.4838, device='cuda:0', dtype=torch.float64), tensor(0.4993, device='cuda:0', dtype=torch.float64), tensor(0.4821, device='cuda:0', dtype=torch.float64), tensor(0.4862, device='cuda:0', dtype=torch.float64), tensor(0.4933, device='cuda:0', dtype=torch.float64), tensor(0.4821, device='cuda:0', dtype=torch.float64), tensor(0.4877, device='cuda:0', dtype=torch.float64), tensor(0.4860, device='cuda:0', dtype=torch.float64), tensor(0.4874, device='cuda:0', dtype=torch.float64), tensor(0.4839, device='cuda:0', dtype=torch.float64), tensor(0.4864, device='cuda:0', dtype=torch.float64)]
Val_loss: [3.802791505095298, 3.127498525481904, 2.8191063748913714, 2.630711697713235, 2.4756298724396046, 2.3400968488101346, 2.308176515593708, 2.252999958804364, 2.2579766073677923, 2.2334498328754395, 2.225260882078058, 2.2333445296355023, 2.2249710247933887, 2.1988018274224954, 2.20811464920676, 2.1764330961682035, 2.223065810159111, 2.224106390475404, 2.203511325622535, 2.2275474688535235, 2.19350282224985, 2.2216405461564492, 2.219748478780173, 2.2393379806443496, 2.211051769078827]
Training:
Train_bal_acc: [0.08254022988505741, 0.24233908045977035, 0.33, 0.3715114942528735, 0.40827586206896577, 0.4327126436781612, 0.44047701149425256, 0.4693045977011495, 0.4732701149425288, 0.4776609195402296, 0.4745977011494251, 0.4838218390804595, 0.47878735632183905, 0.4881436781609195, 0.4904137931034483, 0.49451149425287366, 0.4911379310344825, 0.48551149425287343, 0.4873448275862068, 0.480310344827586, 0.4822471264367818, 0.4971264367816094, 0.4910287356321836, 0.4919367816091952, 0.4921436781609198]
Train_acc: [tensor(0.0826, device='cuda:0', dtype=torch.float64), tensor(0.2424, device='cuda:0', dtype=torch.float64), tensor(0.3300, device='cuda:0', dtype=torch.float64), tensor(0.3715, device='cuda:0', dtype=torch.float64), tensor(0.4082, device='cuda:0', dtype=torch.float64), tensor(0.4328, device='cuda:0', dtype=torch.float64), tensor(0.4404, device='cuda:0', dtype=torch.float64), tensor(0.4693, device='cuda:0', dtype=torch.float64), tensor(0.4733, device='cuda:0', dtype=torch.float64), tensor(0.4776, device='cuda:0', dtype=torch.float64), tensor(0.4746, device='cuda:0', dtype=torch.float64), tensor(0.4838, device='cuda:0', dtype=torch.float64), tensor(0.4788, device='cuda:0', dtype=torch.float64), tensor(0.4882, device='cuda:0', dtype=torch.float64), tensor(0.4905, device='cuda:0', dtype=torch.float64), tensor(0.4945, device='cuda:0', dtype=torch.float64), tensor(0.4912, device='cuda:0', dtype=torch.float64), tensor(0.4855, device='cuda:0', dtype=torch.float64), tensor(0.4873, device='cuda:0', dtype=torch.float64), tensor(0.4803, device='cuda:0', dtype=torch.float64), tensor(0.4823, device='cuda:0', dtype=torch.float64), tensor(0.4972, device='cuda:0', dtype=torch.float64), tensor(0.4910, device='cuda:0', dtype=torch.float64), tensor(0.4920, device='cuda:0', dtype=torch.float64), tensor(0.4922, device='cuda:0', dtype=torch.float64)]
Train_loss: [4.665163099666337, 3.5406095227441035, 2.991184495710157, 2.7431098690102966, 2.5257554946042795, 2.403239685971219, 2.3069429628285003, 2.23800052027723, 2.215044732884562, 2.1780872128985904, 2.1898186260913266, 2.167071460603594, 2.1654809074160015, 2.132840774120551, 2.1275496022240654, 2.1264413973869067, 2.1448094819361345, 2.110410271822153, 2.145891501420651, 2.1602111587056645, 2.1625502343411678, 2.140057932825378, 2.125678895233391, 2.1280644619747284, 2.1297194766807364]
Pruning 30% ... with Fine Tuning
Epoch 0/24
----------
train Loss: 4.6443 Acc: 0.0727 Balanced Acc: 0.0727
val Loss: 3.1282 Acc: 0.2668 Balanced Acc: 0.2710

Epoch 1/24
----------
train Loss: 3.0025 Acc: 0.2766 Balanced Acc: 0.2766
val Loss: 2.4256 Acc: 0.4103 Balanced Acc: 0.4142

Epoch 2/24
----------
train Loss: 2.4180 Acc: 0.3924 Balanced Acc: 0.3924
val Loss: 2.1016 Acc: 0.4705 Balanced Acc: 0.4751

Epoch 3/24
----------
train Loss: 2.1082 Acc: 0.4560 Balanced Acc: 0.4559
val Loss: 1.9493 Acc: 0.4924 Balanced Acc: 0.4944

Epoch 4/24
----------
train Loss: 1.9199 Acc: 0.4998 Balanced Acc: 0.4997
val Loss: 1.8377 Acc: 0.5288 Balanced Acc: 0.5336

Epoch 5/24
----------
train Loss: 1.7255 Acc: 0.5556 Balanced Acc: 0.5555
val Loss: 1.7646 Acc: 0.5412 Balanced Acc: 0.5430

Epoch 6/24
----------
train Loss: 1.6322 Acc: 0.5717 Balanced Acc: 0.5717
val Loss: 1.6925 Acc: 0.5687 Balanced Acc: 0.5696

Epoch 7/24
----------
train Loss: 1.3632 Acc: 0.6440 Balanced Acc: 0.6440
val Loss: 1.5767 Acc: 0.5934 Balanced Acc: 0.5955

Epoch 8/24
----------
train Loss: 1.2962 Acc: 0.6570 Balanced Acc: 0.6570
val Loss: 1.5200 Acc: 0.6048 Balanced Acc: 0.6072

Epoch 9/24
----------
train Loss: 1.2454 Acc: 0.6673 Balanced Acc: 0.6673
val Loss: 1.5277 Acc: 0.6103 Balanced Acc: 0.6126

Epoch 10/24
----------
train Loss: 1.2864 Acc: 0.6613 Balanced Acc: 0.6613
val Loss: 1.4828 Acc: 0.6120 Balanced Acc: 0.6150

Epoch 11/24
----------
train Loss: 1.2436 Acc: 0.6647 Balanced Acc: 0.6646
val Loss: 1.4839 Acc: 0.6189 Balanced Acc: 0.6195

Epoch 12/24
----------
train Loss: 1.2180 Acc: 0.6682 Balanced Acc: 0.6682
val Loss: 1.4670 Acc: 0.6239 Balanced Acc: 0.6259

Epoch 13/24
----------
train Loss: 1.1665 Acc: 0.6828 Balanced Acc: 0.6829
val Loss: 1.4997 Acc: 0.6174 Balanced Acc: 0.6191

Epoch 14/24
----------
train Loss: 1.1664 Acc: 0.6859 Balanced Acc: 0.6858
val Loss: 1.4400 Acc: 0.6294 Balanced Acc: 0.6311

Epoch 15/24
----------
train Loss: 1.1817 Acc: 0.6839 Balanced Acc: 0.6838
val Loss: 1.4417 Acc: 0.6272 Balanced Acc: 0.6295

Epoch 16/24
----------
train Loss: 1.1572 Acc: 0.6890 Balanced Acc: 0.6890
val Loss: 1.4696 Acc: 0.6272 Balanced Acc: 0.6286

Epoch 17/24
----------
train Loss: 1.1815 Acc: 0.6874 Balanced Acc: 0.6874
val Loss: 1.4531 Acc: 0.6281 Balanced Acc: 0.6295

Epoch 18/24
----------
train Loss: 1.1386 Acc: 0.6990 Balanced Acc: 0.6990
val Loss: 1.4692 Acc: 0.6217 Balanced Acc: 0.6244

Epoch 19/24
----------
train Loss: 1.1157 Acc: 0.6927 Balanced Acc: 0.6927
val Loss: 1.4355 Acc: 0.6244 Balanced Acc: 0.6264

Epoch 20/24
----------
train Loss: 1.1587 Acc: 0.6899 Balanced Acc: 0.6898
val Loss: 1.4480 Acc: 0.6243 Balanced Acc: 0.6264

Epoch 21/24
----------
train Loss: 1.1329 Acc: 0.6929 Balanced Acc: 0.6928
val Loss: 1.4715 Acc: 0.6198 Balanced Acc: 0.6210

Epoch 22/24
----------
train Loss: 1.2108 Acc: 0.6877 Balanced Acc: 0.6877
val Loss: 1.4512 Acc: 0.6137 Balanced Acc: 0.6164

Epoch 23/24
----------
train Loss: 1.1339 Acc: 0.6939 Balanced Acc: 0.6939
val Loss: 1.4596 Acc: 0.6251 Balanced Acc: 0.6270

Epoch 24/24
----------
train Loss: 1.1008 Acc: 0.7025 Balanced Acc: 0.7025
val Loss: 1.4365 Acc: 0.6336 Balanced Acc: 0.6353

Training complete in 109m 57s
Best val Balanced Acc: 0.635252
Validation:
Val_bal_acc: [0.27096278135096896, 0.414246301823967, 0.4750518457084382, 0.4944479726788377, 0.5335587229023673, 0.5430134224247104, 0.5695918580463873, 0.5955232029923895, 0.6072432675078064, 0.6125535569192756, 0.6150011225874872, 0.6194892015692736, 0.625878622729664, 0.6190565102276022, 0.631111333559491, 0.6295498351171436, 0.628579908466498, 0.6295444442894141, 0.6243960929746262, 0.6263870871391982, 0.6264478471092138, 0.6209776854603063, 0.6163566492863422, 0.6270409360797983, 0.6352517841897822]
Val_acc: [tensor(0.2668, device='cuda:0', dtype=torch.float64), tensor(0.4103, device='cuda:0', dtype=torch.float64), tensor(0.4705, device='cuda:0', dtype=torch.float64), tensor(0.4924, device='cuda:0', dtype=torch.float64), tensor(0.5288, device='cuda:0', dtype=torch.float64), tensor(0.5412, device='cuda:0', dtype=torch.float64), tensor(0.5687, device='cuda:0', dtype=torch.float64), tensor(0.5934, device='cuda:0', dtype=torch.float64), tensor(0.6048, device='cuda:0', dtype=torch.float64), tensor(0.6103, device='cuda:0', dtype=torch.float64), tensor(0.6120, device='cuda:0', dtype=torch.float64), tensor(0.6189, device='cuda:0', dtype=torch.float64), tensor(0.6239, device='cuda:0', dtype=torch.float64), tensor(0.6174, device='cuda:0', dtype=torch.float64), tensor(0.6294, device='cuda:0', dtype=torch.float64), tensor(0.6272, device='cuda:0', dtype=torch.float64), tensor(0.6272, device='cuda:0', dtype=torch.float64), tensor(0.6281, device='cuda:0', dtype=torch.float64), tensor(0.6217, device='cuda:0', dtype=torch.float64), tensor(0.6244, device='cuda:0', dtype=torch.float64), tensor(0.6243, device='cuda:0', dtype=torch.float64), tensor(0.6198, device='cuda:0', dtype=torch.float64), tensor(0.6137, device='cuda:0', dtype=torch.float64), tensor(0.6251, device='cuda:0', dtype=torch.float64), tensor(0.6336, device='cuda:0', dtype=torch.float64)]
Val_loss: [3.128235624953307, 2.4256327902813966, 2.10159277782734, 1.9492828929870756, 1.8376581348519943, 1.764612184739088, 1.6924795517643278, 1.5766549779383179, 1.5200153126319935, 1.52770359926813, 1.4828067364098494, 1.483882421362421, 1.4670282357386328, 1.4996604703849457, 1.4400198053893938, 1.441719127968751, 1.469583146346744, 1.4530605761142057, 1.4691793232652126, 1.4355150150268734, 1.4480414603917846, 1.4715066590765282, 1.4512168355118953, 1.4596037933303794, 1.436537484156497]
Training:
Train_bal_acc: [0.07270114942528727, 0.2765632183908046, 0.3923850574712644, 0.45587931034482765, 0.49974712643678204, 0.5554942528735635, 0.5717011494252876, 0.6439655172413795, 0.6569885057471265, 0.667298850574713, 0.6612873563218392, 0.6646034482758619, 0.6681781609195402, 0.6828678160919539, 0.6858275862068968, 0.6838448275862067, 0.6890057471264369, 0.6873678160919541, 0.6990000000000003, 0.6926666666666672, 0.6898333333333336, 0.6928160919540232, 0.6876551724137937, 0.6938908045977016, 0.7025402298850573]
Train_acc: [tensor(0.0727, device='cuda:0', dtype=torch.float64), tensor(0.2766, device='cuda:0', dtype=torch.float64), tensor(0.3924, device='cuda:0', dtype=torch.float64), tensor(0.4560, device='cuda:0', dtype=torch.float64), tensor(0.4998, device='cuda:0', dtype=torch.float64), tensor(0.5556, device='cuda:0', dtype=torch.float64), tensor(0.5717, device='cuda:0', dtype=torch.float64), tensor(0.6440, device='cuda:0', dtype=torch.float64), tensor(0.6570, device='cuda:0', dtype=torch.float64), tensor(0.6673, device='cuda:0', dtype=torch.float64), tensor(0.6613, device='cuda:0', dtype=torch.float64), tensor(0.6647, device='cuda:0', dtype=torch.float64), tensor(0.6682, device='cuda:0', dtype=torch.float64), tensor(0.6828, device='cuda:0', dtype=torch.float64), tensor(0.6859, device='cuda:0', dtype=torch.float64), tensor(0.6839, device='cuda:0', dtype=torch.float64), tensor(0.6890, device='cuda:0', dtype=torch.float64), tensor(0.6874, device='cuda:0', dtype=torch.float64), tensor(0.6990, device='cuda:0', dtype=torch.float64), tensor(0.6927, device='cuda:0', dtype=torch.float64), tensor(0.6899, device='cuda:0', dtype=torch.float64), tensor(0.6929, device='cuda:0', dtype=torch.float64), tensor(0.6877, device='cuda:0', dtype=torch.float64), tensor(0.6939, device='cuda:0', dtype=torch.float64), tensor(0.7025, device='cuda:0', dtype=torch.float64)]
Train_loss: [4.6443128748897875, 3.0025418429204453, 2.418048745519048, 2.108237055487978, 1.9198943073048687, 1.7255305852896379, 1.632221415315742, 1.363150235172268, 1.296215117435118, 1.245436953511841, 1.2863929794755107, 1.2436268517165172, 1.2180325385527409, 1.166542783434247, 1.1664450653083809, 1.1817057167326246, 1.1572211114096171, 1.1814770161966344, 1.1385953755608629, 1.115698046890306, 1.1587411419805145, 1.1328922001409418, 1.2108245698618896, 1.133917025816532, 1.1008059401133476]
Pruning 30% ... with Feature Extraction
Epoch 0/24
----------
train Loss: 4.7568 Acc: 0.0721 Balanced Acc: 0.0720
val Loss: 3.9691 Acc: 0.2465 Balanced Acc: 0.2499

Epoch 1/24
----------
train Loss: 3.7101 Acc: 0.2132 Balanced Acc: 0.2132
val Loss: 3.3211 Acc: 0.3464 Balanced Acc: 0.3495

Epoch 2/24
----------
train Loss: 3.1752 Acc: 0.3036 Balanced Acc: 0.3036
val Loss: 2.9631 Acc: 0.3949 Balanced Acc: 0.3988

Epoch 3/24
----------
train Loss: 2.9010 Acc: 0.3502 Balanced Acc: 0.3502
val Loss: 2.7389 Acc: 0.4194 Balanced Acc: 0.4215

Epoch 4/24
----------
train Loss: 2.6941 Acc: 0.3866 Balanced Acc: 0.3866
val Loss: 2.5832 Acc: 0.4441 Balanced Acc: 0.4483

Epoch 5/24
----------
train Loss: 2.5407 Acc: 0.4079 Balanced Acc: 0.4078
val Loss: 2.4971 Acc: 0.4425 Balanced Acc: 0.4467

Epoch 6/24
----------
train Loss: 2.4206 Acc: 0.4201 Balanced Acc: 0.4200
val Loss: 2.4339 Acc: 0.4468 Balanced Acc: 0.4502

Epoch 7/24
----------
train Loss: 2.3481 Acc: 0.4383 Balanced Acc: 0.4382
val Loss: 2.3732 Acc: 0.4641 Balanced Acc: 0.4686

Epoch 8/24
----------
train Loss: 2.3101 Acc: 0.4540 Balanced Acc: 0.4540
val Loss: 2.3723 Acc: 0.4676 Balanced Acc: 0.4696

Epoch 9/24
----------
train Loss: 2.3307 Acc: 0.4481 Balanced Acc: 0.4481
val Loss: 2.3506 Acc: 0.4689 Balanced Acc: 0.4721

Epoch 10/24
----------
train Loss: 2.3098 Acc: 0.4666 Balanced Acc: 0.4666
val Loss: 2.3473 Acc: 0.4746 Balanced Acc: 0.4777

Epoch 11/24
----------
train Loss: 2.2865 Acc: 0.4630 Balanced Acc: 0.4629
val Loss: 2.3312 Acc: 0.4753 Balanced Acc: 0.4776

Epoch 12/24
----------
train Loss: 2.2748 Acc: 0.4641 Balanced Acc: 0.4641
val Loss: 2.3510 Acc: 0.4812 Balanced Acc: 0.4847

Epoch 13/24
----------
train Loss: 2.2779 Acc: 0.4563 Balanced Acc: 0.4562
val Loss: 2.3349 Acc: 0.4726 Balanced Acc: 0.4764

Epoch 14/24
----------
train Loss: 2.2772 Acc: 0.4616 Balanced Acc: 0.4616
val Loss: 2.3046 Acc: 0.4888 Balanced Acc: 0.4913

Epoch 15/24
----------
train Loss: 2.2698 Acc: 0.4546 Balanced Acc: 0.4545
val Loss: 2.3103 Acc: 0.4824 Balanced Acc: 0.4867

Epoch 16/24
----------
train Loss: 2.2494 Acc: 0.4688 Balanced Acc: 0.4688
val Loss: 2.3019 Acc: 0.4824 Balanced Acc: 0.4846

Epoch 17/24
----------
train Loss: 2.2701 Acc: 0.4580 Balanced Acc: 0.4579
val Loss: 2.3400 Acc: 0.4738 Balanced Acc: 0.4779

Epoch 18/24
----------
train Loss: 2.2979 Acc: 0.4610 Balanced Acc: 0.4609
val Loss: 2.3466 Acc: 0.4762 Balanced Acc: 0.4792

Epoch 19/24
----------
train Loss: 2.2545 Acc: 0.4650 Balanced Acc: 0.4649
val Loss: 2.3544 Acc: 0.4724 Balanced Acc: 0.4754

Epoch 20/24
----------
train Loss: 2.2620 Acc: 0.4585 Balanced Acc: 0.4584
val Loss: 2.3506 Acc: 0.4672 Balanced Acc: 0.4711

Epoch 21/24
----------
train Loss: 2.2893 Acc: 0.4648 Balanced Acc: 0.4648
val Loss: 2.2978 Acc: 0.4865 Balanced Acc: 0.4898

Epoch 22/24
----------
train Loss: 2.2850 Acc: 0.4580 Balanced Acc: 0.4579
val Loss: 2.3365 Acc: 0.4715 Balanced Acc: 0.4753

Epoch 23/24
----------
train Loss: 2.2824 Acc: 0.4605 Balanced Acc: 0.4604
val Loss: 2.3244 Acc: 0.4791 Balanced Acc: 0.4813

Epoch 24/24
----------
train Loss: 2.2762 Acc: 0.4605 Balanced Acc: 0.4604
val Loss: 2.3524 Acc: 0.4777 Balanced Acc: 0.4813

Training complete in 106m 47s
Best val Balanced Acc: 0.491336
Validation:
Val_bal_acc: [0.24990154712327847, 0.34945424236255135, 0.3988079208857636, 0.4214765577664459, 0.44830233476773357, 0.4466578230395993, 0.4502001358379881, 0.4686434988627049, 0.4696225523083607, 0.4721384812687976, 0.47769939977870185, 0.47764791511623383, 0.4846508445072317, 0.47641021845989073, 0.4913355033739707, 0.4866580749802757, 0.48464128682222246, 0.47790952226549555, 0.47924891945907083, 0.4753939605062657, 0.4711446446179145, 0.48981628805232147, 0.4752595561098878, 0.48133685577597857, 0.4812761146373553]
Val_acc: [tensor(0.2465, device='cuda:0', dtype=torch.float64), tensor(0.3464, device='cuda:0', dtype=torch.float64), tensor(0.3949, device='cuda:0', dtype=torch.float64), tensor(0.4194, device='cuda:0', dtype=torch.float64), tensor(0.4441, device='cuda:0', dtype=torch.float64), tensor(0.4425, device='cuda:0', dtype=torch.float64), tensor(0.4468, device='cuda:0', dtype=torch.float64), tensor(0.4641, device='cuda:0', dtype=torch.float64), tensor(0.4676, device='cuda:0', dtype=torch.float64), tensor(0.4689, device='cuda:0', dtype=torch.float64), tensor(0.4746, device='cuda:0', dtype=torch.float64), tensor(0.4753, device='cuda:0', dtype=torch.float64), tensor(0.4812, device='cuda:0', dtype=torch.float64), tensor(0.4726, device='cuda:0', dtype=torch.float64), tensor(0.4888, device='cuda:0', dtype=torch.float64), tensor(0.4824, device='cuda:0', dtype=torch.float64), tensor(0.4824, device='cuda:0', dtype=torch.float64), tensor(0.4738, device='cuda:0', dtype=torch.float64), tensor(0.4762, device='cuda:0', dtype=torch.float64), tensor(0.4724, device='cuda:0', dtype=torch.float64), tensor(0.4672, device='cuda:0', dtype=torch.float64), tensor(0.4865, device='cuda:0', dtype=torch.float64), tensor(0.4715, device='cuda:0', dtype=torch.float64), tensor(0.4791, device='cuda:0', dtype=torch.float64), tensor(0.4777, device='cuda:0', dtype=torch.float64)]
Val_loss: [3.9691142568433535, 3.3211340637588895, 2.963062707086576, 2.7388989514715307, 2.583234938910224, 2.4970516792280244, 2.4339413094775693, 2.3732423193091807, 2.3722693386018627, 2.350572860771761, 2.347343646784589, 2.331200338948954, 2.3509516964956196, 2.3349489455063424, 2.3045522993006786, 2.3102634259244677, 2.301874878532441, 2.3400012003293233, 2.3465744805492363, 2.354427953897199, 2.3505560382711996, 2.2977656898393195, 2.3365411886396923, 2.324449638024996, 2.352448450107594]
Training:
Train_bal_acc: [0.07201724137931026, 0.21315517241379334, 0.3036034482758621, 0.3501551724137935, 0.3865804597701149, 0.40782183908046, 0.42002298850574715, 0.4381896551724136, 0.4539712643678165, 0.4480689655172413, 0.4665747126436781, 0.4629137931034481, 0.464126436781609, 0.4562241379310348, 0.46157471264367833, 0.45454022988505727, 0.46878160919540224, 0.457942528735632, 0.4608850574712644, 0.4649195402298848, 0.45844827586206927, 0.46475287356321837, 0.4579252873563218, 0.4604425287356321, 0.46038505747126424]
Train_acc: [tensor(0.0721, device='cuda:0', dtype=torch.float64), tensor(0.2132, device='cuda:0', dtype=torch.float64), tensor(0.3036, device='cuda:0', dtype=torch.float64), tensor(0.3502, device='cuda:0', dtype=torch.float64), tensor(0.3866, device='cuda:0', dtype=torch.float64), tensor(0.4079, device='cuda:0', dtype=torch.float64), tensor(0.4201, device='cuda:0', dtype=torch.float64), tensor(0.4383, device='cuda:0', dtype=torch.float64), tensor(0.4540, device='cuda:0', dtype=torch.float64), tensor(0.4481, device='cuda:0', dtype=torch.float64), tensor(0.4666, device='cuda:0', dtype=torch.float64), tensor(0.4630, device='cuda:0', dtype=torch.float64), tensor(0.4641, device='cuda:0', dtype=torch.float64), tensor(0.4563, device='cuda:0', dtype=torch.float64), tensor(0.4616, device='cuda:0', dtype=torch.float64), tensor(0.4546, device='cuda:0', dtype=torch.float64), tensor(0.4688, device='cuda:0', dtype=torch.float64), tensor(0.4580, device='cuda:0', dtype=torch.float64), tensor(0.4610, device='cuda:0', dtype=torch.float64), tensor(0.4650, device='cuda:0', dtype=torch.float64), tensor(0.4585, device='cuda:0', dtype=torch.float64), tensor(0.4648, device='cuda:0', dtype=torch.float64), tensor(0.4580, device='cuda:0', dtype=torch.float64), tensor(0.4605, device='cuda:0', dtype=torch.float64), tensor(0.4605, device='cuda:0', dtype=torch.float64)]
Train_loss: [4.756776578035759, 3.7100770746345, 3.1751572502665733, 2.9009775384330814, 2.6940876945957646, 2.540661310489312, 2.420615329796527, 2.3481361994076697, 2.310122264239006, 2.330651831857594, 2.30979786366274, 2.2865027416376895, 2.2747629310116593, 2.2778928072244913, 2.2772484597341993, 2.2698374418246576, 2.2494118482540717, 2.2700969145064915, 2.2978947059210038, 2.2544819702495924, 2.2620416959604106, 2.28934918795978, 2.2850099586827937, 2.2824250632697516, 2.276215751687407]
Pruning 40% ... with Fine Tuning
Epoch 0/24
----------
train Loss: 4.7745 Acc: 0.0572 Balanced Acc: 0.0572
val Loss: 3.3690 Acc: 0.2280 Balanced Acc: 0.2281

Epoch 1/24
----------
train Loss: 3.1419 Acc: 0.2506 Balanced Acc: 0.2506
val Loss: 2.5551 Acc: 0.3852 Balanced Acc: 0.3890

Epoch 2/24
----------
train Loss: 2.4939 Acc: 0.3767 Balanced Acc: 0.3767
val Loss: 2.1531 Acc: 0.4536 Balanced Acc: 0.4552

Epoch 3/24
----------
train Loss: 2.1894 Acc: 0.4411 Balanced Acc: 0.4410
val Loss: 2.0578 Acc: 0.4814 Balanced Acc: 0.4847

Epoch 4/24
----------
train Loss: 1.9483 Acc: 0.4987 Balanced Acc: 0.4986
val Loss: 1.8411 Acc: 0.5307 Balanced Acc: 0.5342

Epoch 5/24
----------
train Loss: 1.8060 Acc: 0.5299 Balanced Acc: 0.5298
val Loss: 1.7705 Acc: 0.5551 Balanced Acc: 0.5570

Epoch 6/24
----------
train Loss: 1.7061 Acc: 0.5527 Balanced Acc: 0.5527
val Loss: 1.7104 Acc: 0.5683 Balanced Acc: 0.5714

Epoch 7/24
----------
train Loss: 1.4127 Acc: 0.6273 Balanced Acc: 0.6273
val Loss: 1.5814 Acc: 0.5989 Balanced Acc: 0.6016

Epoch 8/24
----------
train Loss: 1.3277 Acc: 0.6490 Balanced Acc: 0.6489
val Loss: 1.5140 Acc: 0.6061 Balanced Acc: 0.6087

Epoch 9/24
----------
train Loss: 1.3395 Acc: 0.6455 Balanced Acc: 0.6454
val Loss: 1.5185 Acc: 0.6163 Balanced Acc: 0.6191

Epoch 10/24
----------
train Loss: 1.2847 Acc: 0.6598 Balanced Acc: 0.6598
val Loss: 1.5592 Acc: 0.6067 Balanced Acc: 0.6096

Epoch 11/24
----------
train Loss: 1.2954 Acc: 0.6517 Balanced Acc: 0.6516
val Loss: 1.5064 Acc: 0.6125 Balanced Acc: 0.6159

Epoch 12/24
----------
train Loss: 1.2372 Acc: 0.6720 Balanced Acc: 0.6720
val Loss: 1.5425 Acc: 0.6046 Balanced Acc: 0.6062

Epoch 13/24
----------
train Loss: 1.2318 Acc: 0.6650 Balanced Acc: 0.6650
val Loss: 1.4822 Acc: 0.6181 Balanced Acc: 0.6191

Epoch 14/24
----------
train Loss: 1.2170 Acc: 0.6753 Balanced Acc: 0.6752
val Loss: 1.5019 Acc: 0.6151 Balanced Acc: 0.6176

Epoch 15/24
----------
train Loss: 1.2221 Acc: 0.6727 Balanced Acc: 0.6727
val Loss: 1.4839 Acc: 0.6189 Balanced Acc: 0.6205

Epoch 16/24
----------
train Loss: 1.1914 Acc: 0.6835 Balanced Acc: 0.6835
val Loss: 1.4576 Acc: 0.6244 Balanced Acc: 0.6254

Epoch 17/24
----------
train Loss: 1.2204 Acc: 0.6773 Balanced Acc: 0.6774
val Loss: 1.4755 Acc: 0.6201 Balanced Acc: 0.6217

Epoch 18/24
----------
train Loss: 1.1923 Acc: 0.6778 Balanced Acc: 0.6778
val Loss: 1.4782 Acc: 0.6225 Balanced Acc: 0.6251

Epoch 19/24
----------
train Loss: 1.2040 Acc: 0.6770 Balanced Acc: 0.6771
val Loss: 1.4569 Acc: 0.6272 Balanced Acc: 0.6305

Epoch 20/24
----------
train Loss: 1.1738 Acc: 0.6798 Balanced Acc: 0.6798
val Loss: 1.5040 Acc: 0.6165 Balanced Acc: 0.6188

Epoch 21/24
----------
train Loss: 1.1685 Acc: 0.6785 Balanced Acc: 0.6785
val Loss: 1.4995 Acc: 0.6170 Balanced Acc: 0.6185

Epoch 22/24
----------
train Loss: 1.2380 Acc: 0.6760 Balanced Acc: 0.6759
val Loss: 1.4849 Acc: 0.6198 Balanced Acc: 0.6235

Epoch 23/24
----------
train Loss: 1.2115 Acc: 0.6745 Balanced Acc: 0.6745
val Loss: 1.4849 Acc: 0.6231 Balanced Acc: 0.6253

Epoch 24/24
----------
train Loss: 1.1708 Acc: 0.6869 Balanced Acc: 0.6868
val Loss: 1.4772 Acc: 0.6262 Balanced Acc: 0.6285

Training complete in 105m 6s
Best val Balanced Acc: 0.630477
Validation:
Val_bal_acc: [0.22809913650882638, 0.3890069170953139, 0.45516215245413155, 0.48467726208914164, 0.5341933192205622, 0.5569566278978217, 0.5714374840624742, 0.6016341844106774, 0.6087253092604429, 0.619132522692098, 0.60959042085348, 0.6158922050547947, 0.6062254293726127, 0.6191328941683233, 0.617633587529981, 0.6205064036201692, 0.6254186480736822, 0.6217282888217747, 0.6251092535285306, 0.6304768051693407, 0.6188081238049677, 0.6185051970271528, 0.6234832890652547, 0.6252711437920149, 0.6285323809160901]
Val_acc: [tensor(0.2280, device='cuda:0', dtype=torch.float64), tensor(0.3852, device='cuda:0', dtype=torch.float64), tensor(0.4536, device='cuda:0', dtype=torch.float64), tensor(0.4814, device='cuda:0', dtype=torch.float64), tensor(0.5307, device='cuda:0', dtype=torch.float64), tensor(0.5551, device='cuda:0', dtype=torch.float64), tensor(0.5683, device='cuda:0', dtype=torch.float64), tensor(0.5989, device='cuda:0', dtype=torch.float64), tensor(0.6061, device='cuda:0', dtype=torch.float64), tensor(0.6163, device='cuda:0', dtype=torch.float64), tensor(0.6067, device='cuda:0', dtype=torch.float64), tensor(0.6125, device='cuda:0', dtype=torch.float64), tensor(0.6046, device='cuda:0', dtype=torch.float64), tensor(0.6181, device='cuda:0', dtype=torch.float64), tensor(0.6151, device='cuda:0', dtype=torch.float64), tensor(0.6189, device='cuda:0', dtype=torch.float64), tensor(0.6244, device='cuda:0', dtype=torch.float64), tensor(0.6201, device='cuda:0', dtype=torch.float64), tensor(0.6225, device='cuda:0', dtype=torch.float64), tensor(0.6272, device='cuda:0', dtype=torch.float64), tensor(0.6165, device='cuda:0', dtype=torch.float64), tensor(0.6170, device='cuda:0', dtype=torch.float64), tensor(0.6198, device='cuda:0', dtype=torch.float64), tensor(0.6231, device='cuda:0', dtype=torch.float64), tensor(0.6262, device='cuda:0', dtype=torch.float64)]
Val_loss: [3.36895577710869, 2.5551021500295468, 2.153063202998866, 2.0578181574910026, 1.8411433768732242, 1.7704915861654764, 1.7103865941434142, 1.5814473329431482, 1.5139941766605418, 1.5185272865020534, 1.559182419380238, 1.5063863664731758, 1.5424891177984872, 1.4822482822264724, 1.5018530133975014, 1.4839177699923556, 1.457639591916907, 1.4754742784338817, 1.4781921152385518, 1.4568526108469846, 1.503987831404014, 1.4995475714890596, 1.4848964694947837, 1.484902786182913, 1.4771796094318277]
Training:
Train_bal_acc: [0.05721264367816085, 0.25056321839080486, 0.37665517241379304, 0.4410402298850576, 0.49863793103448273, 0.5297758620689653, 0.5526954022988501, 0.6272528735632185, 0.6488908045977013, 0.6454482758620692, 0.6597758620689653, 0.6516264367816091, 0.6719540229885061, 0.6649770114942534, 0.6752471264367815, 0.6726551724137931, 0.6834712643678165, 0.6773505747126438, 0.6777816091954024, 0.6770574712643674, 0.6797873563218393, 0.678540229885058, 0.6759137931034485, 0.674471264367816, 0.686816091954023]
Train_acc: [tensor(0.0572, device='cuda:0', dtype=torch.float64), tensor(0.2506, device='cuda:0', dtype=torch.float64), tensor(0.3767, device='cuda:0', dtype=torch.float64), tensor(0.4411, device='cuda:0', dtype=torch.float64), tensor(0.4987, device='cuda:0', dtype=torch.float64), tensor(0.5299, device='cuda:0', dtype=torch.float64), tensor(0.5527, device='cuda:0', dtype=torch.float64), tensor(0.6273, device='cuda:0', dtype=torch.float64), tensor(0.6490, device='cuda:0', dtype=torch.float64), tensor(0.6455, device='cuda:0', dtype=torch.float64), tensor(0.6598, device='cuda:0', dtype=torch.float64), tensor(0.6517, device='cuda:0', dtype=torch.float64), tensor(0.6720, device='cuda:0', dtype=torch.float64), tensor(0.6650, device='cuda:0', dtype=torch.float64), tensor(0.6753, device='cuda:0', dtype=torch.float64), tensor(0.6727, device='cuda:0', dtype=torch.float64), tensor(0.6835, device='cuda:0', dtype=torch.float64), tensor(0.6773, device='cuda:0', dtype=torch.float64), tensor(0.6778, device='cuda:0', dtype=torch.float64), tensor(0.6770, device='cuda:0', dtype=torch.float64), tensor(0.6798, device='cuda:0', dtype=torch.float64), tensor(0.6785, device='cuda:0', dtype=torch.float64), tensor(0.6760, device='cuda:0', dtype=torch.float64), tensor(0.6745, device='cuda:0', dtype=torch.float64), tensor(0.6869, device='cuda:0', dtype=torch.float64)]
Train_loss: [4.774459305627369, 3.1418639952475362, 2.4938760546791503, 2.1894111873549065, 1.9483033157325722, 1.8059820865924492, 1.7060912504648025, 1.4126798906921345, 1.3276508344305646, 1.339528265737636, 1.2847018900576297, 1.295427277917897, 1.237178257496547, 1.2317554963442496, 1.217010928345872, 1.2220670656538184, 1.1913502495170316, 1.2203924900140213, 1.1923194237060852, 1.2040159662127057, 1.1738312068803334, 1.1684894543470523, 1.2380363811124433, 1.2115441430080403, 1.1708238874827777]
Pruning 40% ... with Feature Extraction
Epoch 0/24
----------
train Loss: 4.8976 Acc: 0.0576 Balanced Acc: 0.0575
val Loss: 4.2066 Acc: 0.2019 Balanced Acc: 0.2031

Epoch 1/24
----------
train Loss: 3.9583 Acc: 0.1904 Balanced Acc: 0.1903
val Loss: 3.5765 Acc: 0.3131 Balanced Acc: 0.3147

Epoch 2/24
----------
train Loss: 3.4281 Acc: 0.2636 Balanced Acc: 0.2635
val Loss: 3.1988 Acc: 0.3668 Balanced Acc: 0.3685

Epoch 3/24
----------
train Loss: 3.1329 Acc: 0.3128 Balanced Acc: 0.3129
val Loss: 2.9634 Acc: 0.3947 Balanced Acc: 0.3988

Epoch 4/24
----------
train Loss: 2.9171 Acc: 0.3412 Balanced Acc: 0.3411
val Loss: 2.7849 Acc: 0.4210 Balanced Acc: 0.4239

Epoch 5/24
----------
train Loss: 2.7655 Acc: 0.3667 Balanced Acc: 0.3667
val Loss: 2.6735 Acc: 0.4258 Balanced Acc: 0.4290

Epoch 6/24
----------
train Loss: 2.6405 Acc: 0.3861 Balanced Acc: 0.3861
val Loss: 2.5683 Acc: 0.4398 Balanced Acc: 0.4423

Epoch 7/24
----------
train Loss: 2.5641 Acc: 0.4021 Balanced Acc: 0.4020
val Loss: 2.5593 Acc: 0.4512 Balanced Acc: 0.4547

Epoch 8/24
----------
train Loss: 2.5464 Acc: 0.4041 Balanced Acc: 0.4040
val Loss: 2.5563 Acc: 0.4493 Balanced Acc: 0.4506

Epoch 9/24
----------
train Loss: 2.5183 Acc: 0.4106 Balanced Acc: 0.4105
val Loss: 2.5467 Acc: 0.4505 Balanced Acc: 0.4530

Epoch 10/24
----------
train Loss: 2.5103 Acc: 0.4188 Balanced Acc: 0.4187
val Loss: 2.5389 Acc: 0.4503 Balanced Acc: 0.4533

Epoch 11/24
----------
train Loss: 2.5051 Acc: 0.4261 Balanced Acc: 0.4261
val Loss: 2.5267 Acc: 0.4496 Balanced Acc: 0.4521

Epoch 12/24
----------
train Loss: 2.4961 Acc: 0.4174 Balanced Acc: 0.4174
val Loss: 2.5324 Acc: 0.4460 Balanced Acc: 0.4493

Epoch 13/24
----------
train Loss: 2.4939 Acc: 0.4199 Balanced Acc: 0.4199
val Loss: 2.4999 Acc: 0.4600 Balanced Acc: 0.4638

Epoch 14/24
----------
train Loss: 2.4562 Acc: 0.4288 Balanced Acc: 0.4287
val Loss: 2.5152 Acc: 0.4486 Balanced Acc: 0.4524

Epoch 15/24
----------
train Loss: 2.4707 Acc: 0.4246 Balanced Acc: 0.4246
val Loss: 2.5214 Acc: 0.4482 Balanced Acc: 0.4511

Epoch 16/24
----------
train Loss: 2.4693 Acc: 0.4236 Balanced Acc: 0.4236
val Loss: 2.5042 Acc: 0.4582 Balanced Acc: 0.4614

Epoch 17/24
----------
train Loss: 2.4809 Acc: 0.4224 Balanced Acc: 0.4224
val Loss: 2.5084 Acc: 0.4574 Balanced Acc: 0.4600

Epoch 18/24
----------
train Loss: 2.4777 Acc: 0.4223 Balanced Acc: 0.4223
val Loss: 2.5169 Acc: 0.4486 Balanced Acc: 0.4512

Epoch 19/24
----------
train Loss: 2.4599 Acc: 0.4274 Balanced Acc: 0.4274
val Loss: 2.4614 Acc: 0.4705 Balanced Acc: 0.4742

Epoch 20/24
----------
train Loss: 2.4707 Acc: 0.4309 Balanced Acc: 0.4310
val Loss: 2.5051 Acc: 0.4584 Balanced Acc: 0.4613

Epoch 21/24
----------
train Loss: 2.4909 Acc: 0.4146 Balanced Acc: 0.4145
val Loss: 2.4933 Acc: 0.4574 Balanced Acc: 0.4614

Epoch 22/24
----------
train Loss: 2.4655 Acc: 0.4358 Balanced Acc: 0.4358
val Loss: 2.4854 Acc: 0.4596 Balanced Acc: 0.4626

Epoch 23/24
----------
train Loss: 2.4948 Acc: 0.4159 Balanced Acc: 0.4159
val Loss: 2.5113 Acc: 0.4524 Balanced Acc: 0.4558

Epoch 24/24
----------
train Loss: 2.4592 Acc: 0.4178 Balanced Acc: 0.4177
val Loss: 2.4875 Acc: 0.4579 Balanced Acc: 0.4609

Training complete in 102m 45s
Best val Balanced Acc: 0.474196
Validation:
Val_bal_acc: [0.20312578816494628, 0.31465469306799304, 0.3685291943532296, 0.39878411561799476, 0.4239025673584974, 0.4290083831754311, 0.44229883784746493, 0.45465841864443207, 0.4506105040001514, 0.4530042541371349, 0.45334369268386476, 0.4521022043273482, 0.44926807808874003, 0.4638438682953599, 0.45238014554190714, 0.4511057101115098, 0.4614366184986796, 0.4600296487131296, 0.45124777014414424, 0.47419623617919204, 0.46131872328933016, 0.46136713165331095, 0.4625705510737865, 0.4558038158523837, 0.46085695141268035]
Val_acc: [tensor(0.2019, device='cuda:0', dtype=torch.float64), tensor(0.3131, device='cuda:0', dtype=torch.float64), tensor(0.3668, device='cuda:0', dtype=torch.float64), tensor(0.3947, device='cuda:0', dtype=torch.float64), tensor(0.4210, device='cuda:0', dtype=torch.float64), tensor(0.4258, device='cuda:0', dtype=torch.float64), tensor(0.4398, device='cuda:0', dtype=torch.float64), tensor(0.4512, device='cuda:0', dtype=torch.float64), tensor(0.4493, device='cuda:0', dtype=torch.float64), tensor(0.4505, device='cuda:0', dtype=torch.float64), tensor(0.4503, device='cuda:0', dtype=torch.float64), tensor(0.4496, device='cuda:0', dtype=torch.float64), tensor(0.4460, device='cuda:0', dtype=torch.float64), tensor(0.4600, device='cuda:0', dtype=torch.float64), tensor(0.4486, device='cuda:0', dtype=torch.float64), tensor(0.4482, device='cuda:0', dtype=torch.float64), tensor(0.4582, device='cuda:0', dtype=torch.float64), tensor(0.4574, device='cuda:0', dtype=torch.float64), tensor(0.4486, device='cuda:0', dtype=torch.float64), tensor(0.4705, device='cuda:0', dtype=torch.float64), tensor(0.4584, device='cuda:0', dtype=torch.float64), tensor(0.4574, device='cuda:0', dtype=torch.float64), tensor(0.4596, device='cuda:0', dtype=torch.float64), tensor(0.4524, device='cuda:0', dtype=torch.float64), tensor(0.4579, device='cuda:0', dtype=torch.float64)]
Val_loss: [4.20662898543295, 3.5765069965498175, 3.1988141835455983, 2.9633634176013963, 2.7849456507212382, 2.673474533129117, 2.568271474629219, 2.559314023061666, 2.556329295270641, 2.546678662341259, 2.5388509805012376, 2.526736880731369, 2.532369954445299, 2.4999401089566553, 2.5151661469025326, 2.52142450428437, 2.5042230133192267, 2.508441490019277, 2.516912841286625, 2.4613600482930797, 2.5050629209887787, 2.493327409677107, 2.485358007866883, 2.5113459892588974, 2.4874659639866468]
Training:
Train_bal_acc: [0.057534482758620636, 0.19033908045977013, 0.26351149425287385, 0.3128563218390805, 0.34114367816091956, 0.3667126436781609, 0.3860574712643676, 0.40202873563218383, 0.40402298850574697, 0.41047701149425303, 0.41874712643678164, 0.42605747126436766, 0.41737931034482806, 0.41991379310344834, 0.4287471264367821, 0.42455172413793124, 0.42356321839080435, 0.4223965517241378, 0.42226436781609195, 0.4274482758620692, 0.43095402298850594, 0.4145114942528737, 0.4357816091954023, 0.4158850574712644, 0.41772413793103447]
Train_acc: [tensor(0.0576, device='cuda:0', dtype=torch.float64), tensor(0.1904, device='cuda:0', dtype=torch.float64), tensor(0.2636, device='cuda:0', dtype=torch.float64), tensor(0.3128, device='cuda:0', dtype=torch.float64), tensor(0.3412, device='cuda:0', dtype=torch.float64), tensor(0.3667, device='cuda:0', dtype=torch.float64), tensor(0.3861, device='cuda:0', dtype=torch.float64), tensor(0.4021, device='cuda:0', dtype=torch.float64), tensor(0.4041, device='cuda:0', dtype=torch.float64), tensor(0.4106, device='cuda:0', dtype=torch.float64), tensor(0.4188, device='cuda:0', dtype=torch.float64), tensor(0.4261, device='cuda:0', dtype=torch.float64), tensor(0.4174, device='cuda:0', dtype=torch.float64), tensor(0.4199, device='cuda:0', dtype=torch.float64), tensor(0.4288, device='cuda:0', dtype=torch.float64), tensor(0.4246, device='cuda:0', dtype=torch.float64), tensor(0.4236, device='cuda:0', dtype=torch.float64), tensor(0.4224, device='cuda:0', dtype=torch.float64), tensor(0.4223, device='cuda:0', dtype=torch.float64), tensor(0.4274, device='cuda:0', dtype=torch.float64), tensor(0.4309, device='cuda:0', dtype=torch.float64), tensor(0.4146, device='cuda:0', dtype=torch.float64), tensor(0.4358, device='cuda:0', dtype=torch.float64), tensor(0.4159, device='cuda:0', dtype=torch.float64), tensor(0.4178, device='cuda:0', dtype=torch.float64)]
Train_loss: [4.897624557719137, 3.9583040225653, 3.428063197417541, 3.1328860089744692, 2.917135179221809, 2.765494261815781, 2.640489376661258, 2.564073542097548, 2.546361255375274, 2.518296275172267, 2.510323889620669, 2.5050750988739745, 2.496082327466907, 2.493868443979436, 2.4562073213719193, 2.4706946168695247, 2.4693001976878715, 2.4808858481096276, 2.477682949106893, 2.4599398565085524, 2.470692511514938, 2.490890761872789, 2.46549421133182, 2.4947608922933555, 2.459228762078373]


Pruned 50% ... local training with Fine Tuning
Downloading https://s3.us-west-2.amazonaws.com/caltechdata/96/97/8384-3670-482e-a3dd-97ac171e8a10/data?response-content-type=application%2Foctet-stream&response-content-disposition=attachment%3B%20filename%3DCUB_200_2011.tgz&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIARCVIVNNAP7NNDVEA%2F20221124%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20221124T193156Z&X-Amz-Expires=60&X-Amz-SignedHeaders=host&X-Amz-Signature=a4f7a774655b2a1912719e8d1143fb1d488522553838f4ba787898ed81769b2c to ./cub2011/train/CUB_200_2011.tgz

100%
1150585339/1150585339 [00:39<00:00, 25134451.40it/s]

Downloading https://s3.us-west-2.amazonaws.com/caltechdata/96/97/8384-3670-482e-a3dd-97ac171e8a10/data?response-content-type=application%2Foctet-stream&response-content-disposition=attachment%3B%20filename%3DCUB_200_2011.tgz&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIARCVIVNNAP7NNDVEA%2F20221124%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20221124T193248Z&X-Amz-Expires=60&X-Amz-SignedHeaders=host&X-Amz-Signature=e7feb82fcf46cb44eca3b5575a8d746a2aff9b938ff7bc4e966d3b639e64b641 to ./cub2011/val/CUB_200_2011.tgz

100%
1150585339/1150585339 [00:31<00:00, 36373552.46it/s]

/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:566: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  cpuset_checked))
Downloading: "https://download.pytorch.org/models/vgg16-397923af.pth" to /root/.cache/torch/hub/checkpoints/vgg16-397923af.pth

100%
528M/528M [00:04<00:00, 141MB/s]

Epoch 0/24
----------
train Loss: 4.9887 Acc: 0.0355 Balanced Acc: 0.0355
val Loss: 3.7906 Acc: 0.1859 Balanced Acc: 0.1884

Epoch 1/24
----------
train Loss: 3.3942 Acc: 0.2224 Balanced Acc: 0.2224
val Loss: 2.5721 Acc: 0.3738 Balanced Acc: 0.3769

Epoch 2/24
----------
train Loss: 2.7027 Acc: 0.3293 Balanced Acc: 0.3293
val Loss: 2.3253 Acc: 0.4108 Balanced Acc: 0.4134

Epoch 3/24
----------
train Loss: 2.3427 Acc: 0.4054 Balanced Acc: 0.4054
val Loss: 1.9834 Acc: 0.4948 Balanced Acc: 0.4975

Epoch 4/24
----------
train Loss: 2.0657 Acc: 0.4640 Balanced Acc: 0.4639
val Loss: 1.9230 Acc: 0.5035 Balanced Acc: 0.5057

Epoch 5/24
----------
train Loss: 1.9484 Acc: 0.4947 Balanced Acc: 0.4946
val Loss: 1.8121 Acc: 0.5362 Balanced Acc: 0.5391

Epoch 6/24
----------
train Loss: 1.7808 Acc: 0.5269 Balanced Acc: 0.5268
val Loss: 1.7695 Acc: 0.5440 Balanced Acc: 0.5465

Epoch 7/24
----------
train Loss: 1.5602 Acc: 0.5919 Balanced Acc: 0.5919
val Loss: 1.6161 Acc: 0.5846 Balanced Acc: 0.5876

Epoch 8/24
----------
train Loss: 1.4499 Acc: 0.6103 Balanced Acc: 0.6103
val Loss: 1.5813 Acc: 0.5882 Balanced Acc: 0.5913

Epoch 9/24
----------
train Loss: 1.4245 Acc: 0.6210 Balanced Acc: 0.6209
val Loss: 1.5801 Acc: 0.5880 Balanced Acc: 0.5908

Epoch 10/24
----------
train Loss: 1.4255 Acc: 0.6243 Balanced Acc: 0.6242
val Loss: 1.5467 Acc: 0.6017 Balanced Acc: 0.6040

Epoch 11/24
----------
train Loss: 1.4058 Acc: 0.6251 Balanced Acc: 0.6251
val Loss: 1.5563 Acc: 0.5970 Balanced Acc: 0.6001

Epoch 12/24
----------
train Loss: 1.4077 Acc: 0.6215 Balanced Acc: 0.6214
val Loss: 1.5233 Acc: 0.6056 Balanced Acc: 0.6090

Epoch 13/24
----------
train Loss: 1.3665 Acc: 0.6313 Balanced Acc: 0.6312
val Loss: 1.5120 Acc: 0.6168 Balanced Acc: 0.6182

Epoch 14/24
----------
train Loss: 1.3458 Acc: 0.6401 Balanced Acc: 0.6401
val Loss: 1.5299 Acc: 0.6048 Balanced Acc: 0.6075

Epoch 15/24
----------
train Loss: 1.3056 Acc: 0.6550 Balanced Acc: 0.6550
val Loss: 1.5037 Acc: 0.6127 Balanced Acc: 0.6141

Epoch 16/24
----------
train Loss: 1.2968 Acc: 0.6530 Balanced Acc: 0.6530
val Loss: 1.5099 Acc: 0.6170 Balanced Acc: 0.6190

Epoch 17/24
----------
train Loss: 1.3256 Acc: 0.6468 Balanced Acc: 0.6467
val Loss: 1.5147 Acc: 0.6101 Balanced Acc: 0.6116

Epoch 18/24
----------
train Loss: 1.3161 Acc: 0.6491 Balanced Acc: 0.6491
val Loss: 1.4826 Acc: 0.6172 Balanced Acc: 0.6198

Epoch 19/24
----------
train Loss: 1.3577 Acc: 0.6441 Balanced Acc: 0.6441
val Loss: 1.5221 Acc: 0.6058 Balanced Acc: 0.6076

Epoch 20/24
----------
train Loss: 1.2935 Acc: 0.6513 Balanced Acc: 0.6513
val Loss: 1.4926 Acc: 0.6146 Balanced Acc: 0.6177

Epoch 21/24
----------
train Loss: 1.3213 Acc: 0.6491 Balanced Acc: 0.6492
val Loss: 1.5414 Acc: 0.6041 Balanced Acc: 0.6059

Epoch 22/24
----------
train Loss: 1.3476 Acc: 0.6445 Balanced Acc: 0.6444
val Loss: 1.5202 Acc: 0.6048 Balanced Acc: 0.6077

Epoch 23/24
----------
train Loss: 1.3380 Acc: 0.6380 Balanced Acc: 0.6379
val Loss: 1.5299 Acc: 0.6144 Balanced Acc: 0.6161

Epoch 24/24
----------
train Loss: 1.3452 Acc: 0.6391 Balanced Acc: 0.6391
val Loss: 1.5153 Acc: 0.6137 Balanced Acc: 0.6166

Training complete in 51m 53s
Best val Balanced Acc: 0.619778
Validation:
Val_bal_acc: [0.1883546396118005, 0.37694438192057156, 0.4133952137456633, 0.4975494333690896, 0.5056677450595035, 0.5390570788575604, 0.546470330875977, 0.5875656856966536, 0.5913442066349033, 0.5907995389343728, 0.6039729729327692, 0.6001353445317581, 0.6089917957446956, 0.6182119158849806, 0.6074992902367835, 0.6140693161898875, 0.6190471014821207, 0.6116473613338794, 0.6197775629787388, 0.6076354125254937, 0.617669552173951, 0.6059340757293097, 0.6076732212802994, 0.6160686934729585, 0.6166004767945116]
Val_acc: [tensor(0.1859, device='cuda:0', dtype=torch.float64), tensor(0.3738, device='cuda:0', dtype=torch.float64), tensor(0.4108, device='cuda:0', dtype=torch.float64), tensor(0.4948, device='cuda:0', dtype=torch.float64), tensor(0.5035, device='cuda:0', dtype=torch.float64), tensor(0.5362, device='cuda:0', dtype=torch.float64), tensor(0.5440, device='cuda:0', dtype=torch.float64), tensor(0.5846, device='cuda:0', dtype=torch.float64), tensor(0.5882, device='cuda:0', dtype=torch.float64), tensor(0.5880, device='cuda:0', dtype=torch.float64), tensor(0.6017, device='cuda:0', dtype=torch.float64), tensor(0.5970, device='cuda:0', dtype=torch.float64), tensor(0.6056, device='cuda:0', dtype=torch.float64), tensor(0.6168, device='cuda:0', dtype=torch.float64), tensor(0.6048, device='cuda:0', dtype=torch.float64), tensor(0.6127, device='cuda:0', dtype=torch.float64), tensor(0.6170, device='cuda:0', dtype=torch.float64), tensor(0.6101, device='cuda:0', dtype=torch.float64), tensor(0.6172, device='cuda:0', dtype=torch.float64), tensor(0.6058, device='cuda:0', dtype=torch.float64), tensor(0.6146, device='cuda:0', dtype=torch.float64), tensor(0.6041, device='cuda:0', dtype=torch.float64), tensor(0.6048, device='cuda:0', dtype=torch.float64), tensor(0.6144, device='cuda:0', dtype=torch.float64), tensor(0.6137, device='cuda:0', dtype=torch.float64)]
Val_loss: [3.7905891902042663, 2.5721387474710546, 2.3253453995879454, 1.9834200879150643, 1.9230050925740219, 1.8121162925274321, 1.7694665559374303, 1.6160906074034331, 1.5812742436059721, 1.5800732287861508, 1.5466648636499603, 1.5562564141541708, 1.5233171907095076, 1.5119781960772776, 1.529936266629026, 1.503678201924409, 1.5098882246560625, 1.5147364418548595, 1.482596167689527, 1.5221028038868627, 1.4925593960725483, 1.5414143422214088, 1.520246400724496, 1.5298936862985884, 1.5152678087914364]
Training:
Train_bal_acc: [0.03553448275862065, 0.2223908045977012, 0.3292701149425287, 0.4053620689655172, 0.4638965517241378, 0.4946436781609196, 0.5268045977011494, 0.5918620689655174, 0.6102643678160922, 0.6208678160919541, 0.6242471264367812, 0.6250919540229884, 0.6213735632183907, 0.6312471264367818, 0.6401034482758625, 0.6549540229885057, 0.6530229885057477, 0.6467298850574714, 0.6490804597701151, 0.644080459770115, 0.6512873563218391, 0.6491666666666667, 0.6444080459770114, 0.637913793103448, 0.6390862068965518]
Train_acc: [tensor(0.0355, device='cuda:0', dtype=torch.float64), tensor(0.2224, device='cuda:0', dtype=torch.float64), tensor(0.3293, device='cuda:0', dtype=torch.float64), tensor(0.4054, device='cuda:0', dtype=torch.float64), tensor(0.4640, device='cuda:0', dtype=torch.float64), tensor(0.4947, device='cuda:0', dtype=torch.float64), tensor(0.5269, device='cuda:0', dtype=torch.float64), tensor(0.5919, device='cuda:0', dtype=torch.float64), tensor(0.6103, device='cuda:0', dtype=torch.float64), tensor(0.6210, device='cuda:0', dtype=torch.float64), tensor(0.6243, device='cuda:0', dtype=torch.float64), tensor(0.6251, device='cuda:0', dtype=torch.float64), tensor(0.6215, device='cuda:0', dtype=torch.float64), tensor(0.6313, device='cuda:0', dtype=torch.float64), tensor(0.6401, device='cuda:0', dtype=torch.float64), tensor(0.6550, device='cuda:0', dtype=torch.float64), tensor(0.6530, device='cuda:0', dtype=torch.float64), tensor(0.6468, device='cuda:0', dtype=torch.float64), tensor(0.6491, device='cuda:0', dtype=torch.float64), tensor(0.6441, device='cuda:0', dtype=torch.float64), tensor(0.6513, device='cuda:0', dtype=torch.float64), tensor(0.6491, device='cuda:0', dtype=torch.float64), tensor(0.6445, device='cuda:0', dtype=torch.float64), tensor(0.6380, device='cuda:0', dtype=torch.float64), tensor(0.6391, device='cuda:0', dtype=torch.float64)]
Train_loss: [4.988653529195496, 3.394213299533308, 2.702684884076123, 2.3427231423966997, 2.0657190474264215, 1.9483663183790785, 1.7808279767607624, 1.5602346442125223, 1.4499405898131408, 1.4245228838196666, 1.4254628218210734, 1.4058440126177865, 1.4077105970433603, 1.366494124815549, 1.3457687621041858, 1.3056026929451856, 1.296787363551321, 1.325622426536746, 1.3161454168724782, 1.357730827929777, 1.2934543931329094, 1.3213470015161468, 1.347635770663445, 1.337992943681635, 1.345185232074968]




Pruning 50% ... with Feature Extraction
Epoch 0/24
----------
train Loss: 5.0136 Acc: 0.0447 Balanced Acc: 0.0447
val Loss: 4.4636 Acc: 0.1645 Balanced Acc: 0.1650

Epoch 1/24
----------
train Loss: 4.2304 Acc: 0.1445 Balanced Acc: 0.1445
val Loss: 3.8819 Acc: 0.2615 Balanced Acc: 0.2619

Epoch 2/24
----------
train Loss: 3.7468 Acc: 0.2187 Balanced Acc: 0.2187
val Loss: 3.5058 Acc: 0.3253 Balanced Acc: 0.3283

Epoch 3/24
----------
train Loss: 3.4311 Acc: 0.2728 Balanced Acc: 0.2728
val Loss: 3.2711 Acc: 0.3490 Balanced Acc: 0.3527

Epoch 4/24
----------
train Loss: 3.1865 Acc: 0.3040 Balanced Acc: 0.3040
val Loss: 3.0731 Acc: 0.3828 Balanced Acc: 0.3862

Epoch 5/24
----------
train Loss: 3.0393 Acc: 0.3253 Balanced Acc: 0.3253
val Loss: 2.9733 Acc: 0.3856 Balanced Acc: 0.3901

Epoch 6/24
----------
train Loss: 2.9146 Acc: 0.3353 Balanced Acc: 0.3353
val Loss: 2.8369 Acc: 0.4058 Balanced Acc: 0.4098

Epoch 7/24
----------
train Loss: 2.8434 Acc: 0.3609 Balanced Acc: 0.3608
val Loss: 2.8105 Acc: 0.4070 Balanced Acc: 0.4095

Epoch 8/24
----------
train Loss: 2.8140 Acc: 0.3627 Balanced Acc: 0.3627
val Loss: 2.7840 Acc: 0.4199 Balanced Acc: 0.4227

Epoch 9/24
----------
train Loss: 2.8053 Acc: 0.3637 Balanced Acc: 0.3636
val Loss: 2.7812 Acc: 0.4225 Balanced Acc: 0.4260

Epoch 10/24
----------
train Loss: 2.7855 Acc: 0.3674 Balanced Acc: 0.3673
val Loss: 2.8050 Acc: 0.4149 Balanced Acc: 0.4173

Epoch 11/24
----------
train Loss: 2.7688 Acc: 0.3700 Balanced Acc: 0.3700
val Loss: 2.7641 Acc: 0.4208 Balanced Acc: 0.4244

Epoch 12/24
----------
train Loss: 2.7825 Acc: 0.3695 Balanced Acc: 0.3696
val Loss: 2.7626 Acc: 0.4235 Balanced Acc: 0.4281

Epoch 13/24
----------
train Loss: 2.7490 Acc: 0.3790 Balanced Acc: 0.3791
val Loss: 2.7762 Acc: 0.4127 Balanced Acc: 0.4159

Epoch 14/24
----------
train Loss: 2.7358 Acc: 0.3762 Balanced Acc: 0.3762
val Loss: 2.7789 Acc: 0.4115 Balanced Acc: 0.4156

Epoch 15/24
----------
train Loss: 2.7330 Acc: 0.3849 Balanced Acc: 0.3849
val Loss: 2.7445 Acc: 0.4210 Balanced Acc: 0.4241

Epoch 16/24
----------
train Loss: 2.7408 Acc: 0.3774 Balanced Acc: 0.3773
val Loss: 2.7489 Acc: 0.4270 Balanced Acc: 0.4291

Epoch 17/24
----------
train Loss: 2.7380 Acc: 0.3830 Balanced Acc: 0.3830
val Loss: 2.7588 Acc: 0.4134 Balanced Acc: 0.4151

Epoch 18/24
----------
train Loss: 2.7239 Acc: 0.3834 Balanced Acc: 0.3833
val Loss: 2.7487 Acc: 0.4244 Balanced Acc: 0.4276

Epoch 19/24
----------
train Loss: 2.7225 Acc: 0.3795 Balanced Acc: 0.3795
val Loss: 2.7412 Acc: 0.4194 Balanced Acc: 0.4233

Epoch 20/24
----------
train Loss: 2.7448 Acc: 0.3760 Balanced Acc: 0.3760
val Loss: 2.7495 Acc: 0.4197 Balanced Acc: 0.4241

Epoch 21/24
----------
train Loss: 2.7489 Acc: 0.3754 Balanced Acc: 0.3754
val Loss: 2.7590 Acc: 0.4197 Balanced Acc: 0.4224

Epoch 22/24
----------
train Loss: 2.7478 Acc: 0.3737 Balanced Acc: 0.3737
val Loss: 2.7661 Acc: 0.4082 Balanced Acc: 0.4114

Epoch 23/24
----------
train Loss: 2.7401 Acc: 0.3737 Balanced Acc: 0.3737
val Loss: 2.7574 Acc: 0.4178 Balanced Acc: 0.4208

Epoch 24/24
----------
train Loss: 2.7449 Acc: 0.3795 Balanced Acc: 0.3796
val Loss: 2.7597 Acc: 0.4234 Balanced Acc: 0.4269

Training complete in 98m 38s
Best val Balanced Acc: 0.429101
Validation:
Val_bal_acc: [0.1649668480310792, 0.2619213062247467, 0.32827654112168425, 0.3526990886424919, 0.386216091536083, 0.39010549723356475, 0.4097832382261877, 0.40951848060902757, 0.42271952600619805, 0.42604207908693825, 0.4173272705460425, 0.4243540712176001, 0.42814543361093127, 0.41588301513739473, 0.4155814182682921, 0.4241199465377638, 0.4291006554431157, 0.4150971934025272, 0.4276235308955464, 0.4233148910790353, 0.42411827197953206, 0.4223910488729794, 0.4114248077039644, 0.4207853631225762, 0.42689763671161146]
Val_acc: [tensor(0.1645, device='cuda:0', dtype=torch.float64), tensor(0.2615, device='cuda:0', dtype=torch.float64), tensor(0.3253, device='cuda:0', dtype=torch.float64), tensor(0.3490, device='cuda:0', dtype=torch.float64), tensor(0.3828, device='cuda:0', dtype=torch.float64), tensor(0.3856, device='cuda:0', dtype=torch.float64), tensor(0.4058, device='cuda:0', dtype=torch.float64), tensor(0.4070, device='cuda:0', dtype=torch.float64), tensor(0.4199, device='cuda:0', dtype=torch.float64), tensor(0.4225, device='cuda:0', dtype=torch.float64), tensor(0.4149, device='cuda:0', dtype=torch.float64), tensor(0.4208, device='cuda:0', dtype=torch.float64), tensor(0.4235, device='cuda:0', dtype=torch.float64), tensor(0.4127, device='cuda:0', dtype=torch.float64), tensor(0.4115, device='cuda:0', dtype=torch.float64), tensor(0.4210, device='cuda:0', dtype=torch.float64), tensor(0.4270, device='cuda:0', dtype=torch.float64), tensor(0.4134, device='cuda:0', dtype=torch.float64), tensor(0.4244, device='cuda:0', dtype=torch.float64), tensor(0.4194, device='cuda:0', dtype=torch.float64), tensor(0.4197, device='cuda:0', dtype=torch.float64), tensor(0.4197, device='cuda:0', dtype=torch.float64), tensor(0.4082, device='cuda:0', dtype=torch.float64), tensor(0.4178, device='cuda:0', dtype=torch.float64), tensor(0.4234, device='cuda:0', dtype=torch.float64)]
Val_loss: [4.463620949416972, 3.88186104872904, 3.5058268869503393, 3.2711254034447266, 3.0731206594519174, 2.973333214660081, 2.8369361872832695, 2.8105179180883315, 2.7839849116187447, 2.781216657363344, 2.804971053183058, 2.764077628773988, 2.762615073496694, 2.7761559274059184, 2.778858488248799, 2.744477939967826, 2.748893891608752, 2.7587780602685243, 2.7487230376617884, 2.7412095633629563, 2.7494764565022436, 2.7590001063137826, 2.766060491409802, 2.757428402155072, 2.7597128399331115]
Training:
Train_bal_acc: [0.044712643678160885, 0.14445402298850588, 0.2187011494252874, 0.2727528735632186, 0.30397701149425305, 0.3252873563218392, 0.3353275862068966, 0.3608390804597704, 0.36266091954023005, 0.3636436781609196, 0.367344827586207, 0.37004022988505736, 0.36956321839080464, 0.3790862068965517, 0.3761896551724137, 0.3848620689655173, 0.3773103448275862, 0.38300000000000006, 0.3833160919540232, 0.37951149425287356, 0.3760229885057471, 0.3753793103448272, 0.37366666666666665, 0.37371264367816037, 0.3795632183908044]
Train_acc: [tensor(0.0447, device='cuda:0', dtype=torch.float64), tensor(0.1445, device='cuda:0', dtype=torch.float64), tensor(0.2187, device='cuda:0', dtype=torch.float64), tensor(0.2728, device='cuda:0', dtype=torch.float64), tensor(0.3040, device='cuda:0', dtype=torch.float64), tensor(0.3253, device='cuda:0', dtype=torch.float64), tensor(0.3353, device='cuda:0', dtype=torch.float64), tensor(0.3609, device='cuda:0', dtype=torch.float64), tensor(0.3627, device='cuda:0', dtype=torch.float64), tensor(0.3637, device='cuda:0', dtype=torch.float64), tensor(0.3674, device='cuda:0', dtype=torch.float64), tensor(0.3700, device='cuda:0', dtype=torch.float64), tensor(0.3695, device='cuda:0', dtype=torch.float64), tensor(0.3790, device='cuda:0', dtype=torch.float64), tensor(0.3762, device='cuda:0', dtype=torch.float64), tensor(0.3849, device='cuda:0', dtype=torch.float64), tensor(0.3774, device='cuda:0', dtype=torch.float64), tensor(0.3830, device='cuda:0', dtype=torch.float64), tensor(0.3834, device='cuda:0', dtype=torch.float64), tensor(0.3795, device='cuda:0', dtype=torch.float64), tensor(0.3760, device='cuda:0', dtype=torch.float64), tensor(0.3754, device='cuda:0', dtype=torch.float64), tensor(0.3737, device='cuda:0', dtype=torch.float64), tensor(0.3737, device='cuda:0', dtype=torch.float64), tensor(0.3795, device='cuda:0', dtype=torch.float64)]
Train_loss: [5.0135770407286255, 4.230393984415629, 3.746796096528734, 3.431077343804223, 3.186455680721793, 3.0392816310172326, 2.9146396157580057, 2.8434091976256144, 2.813984290018931, 2.8053303393356632, 2.785481482535392, 2.76875860260692, 2.782469346438164, 2.7489708577309764, 2.735750537416638, 2.7330390069736894, 2.7407948613604347, 2.7379604764568595, 2.7238504083147834, 2.7225174194262114, 2.744775694531125, 2.7488592188876195, 2.7477874247519463, 2.7400952363992714, 2.744885865791583]
Pruning 60% ... with Fine Tuning
Epoch 0/24
----------
train Loss: 5.2120 Acc: 0.0163 Balanced Acc: 0.0164
val Loss: 4.6426 Acc: 0.0901 Balanced Acc: 0.0897

Epoch 1/24
----------
train Loss: 4.0500 Acc: 0.1206 Balanced Acc: 0.1206
val Loss: 3.0083 Acc: 0.3091 Balanced Acc: 0.3121

Epoch 2/24
----------
train Loss: 3.0486 Acc: 0.2708 Balanced Acc: 0.2707
val Loss: 2.5431 Acc: 0.4033 Balanced Acc: 0.4070

Epoch 3/24
----------
train Loss: 2.6207 Acc: 0.3475 Balanced Acc: 0.3475
val Loss: 2.2458 Acc: 0.4487 Balanced Acc: 0.4531

Epoch 4/24
----------
train Loss: 2.3063 Acc: 0.4122 Balanced Acc: 0.4122
val Loss: 2.1410 Acc: 0.4674 Balanced Acc: 0.4726

Epoch 5/24
----------
train Loss: 2.1185 Acc: 0.4578 Balanced Acc: 0.4577
val Loss: 1.9772 Acc: 0.4997 Balanced Acc: 0.5028

Epoch 6/24
----------
train Loss: 1.9826 Acc: 0.4852 Balanced Acc: 0.4851
val Loss: 1.8535 Acc: 0.5302 Balanced Acc: 0.5318

Epoch 7/24
----------
train Loss: 1.7152 Acc: 0.5447 Balanced Acc: 0.5446
val Loss: 1.6777 Acc: 0.5713 Balanced Acc: 0.5735

Epoch 8/24
----------
train Loss: 1.6182 Acc: 0.5759 Balanced Acc: 0.5758
val Loss: 1.6554 Acc: 0.5761 Balanced Acc: 0.5779

Epoch 9/24
----------
train Loss: 1.6105 Acc: 0.5791 Balanced Acc: 0.5791
val Loss: 1.6181 Acc: 0.5827 Balanced Acc: 0.5857

Epoch 10/24
----------
train Loss: 1.5707 Acc: 0.5863 Balanced Acc: 0.5862
val Loss: 1.6033 Acc: 0.5884 Balanced Acc: 0.5897

Epoch 11/24
----------
train Loss: 1.5785 Acc: 0.5784 Balanced Acc: 0.5784
val Loss: 1.6003 Acc: 0.5941 Balanced Acc: 0.5966

Epoch 12/24
----------
train Loss: 1.5284 Acc: 0.5888 Balanced Acc: 0.5887
val Loss: 1.6141 Acc: 0.5934 Balanced Acc: 0.5957

Epoch 13/24
----------
train Loss: 1.5140 Acc: 0.5986 Balanced Acc: 0.5985
val Loss: 1.5945 Acc: 0.5910 Balanced Acc: 0.5926

Epoch 14/24
----------
train Loss: 1.5029 Acc: 0.5909 Balanced Acc: 0.5909
val Loss: 1.5788 Acc: 0.5975 Balanced Acc: 0.6000

Epoch 15/24
----------
train Loss: 1.5083 Acc: 0.5961 Balanced Acc: 0.5960
val Loss: 1.5685 Acc: 0.5970 Balanced Acc: 0.5988

Epoch 16/24
----------
train Loss: 1.4767 Acc: 0.6089 Balanced Acc: 0.6089
val Loss: 1.5891 Acc: 0.5918 Balanced Acc: 0.5936

Epoch 17/24
----------
train Loss: 1.4791 Acc: 0.6051 Balanced Acc: 0.6050
val Loss: 1.5842 Acc: 0.5979 Balanced Acc: 0.6001

Epoch 18/24
----------
train Loss: 1.4607 Acc: 0.6091 Balanced Acc: 0.6091
val Loss: 1.5635 Acc: 0.5999 Balanced Acc: 0.6018

Epoch 19/24
----------
train Loss: 1.4999 Acc: 0.6131 Balanced Acc: 0.6130
val Loss: 1.5651 Acc: 0.5986 Balanced Acc: 0.6007

Epoch 20/24
----------
train Loss: 1.4854 Acc: 0.5999 Balanced Acc: 0.5999
val Loss: 1.5535 Acc: 0.5982 Balanced Acc: 0.6008

Epoch 21/24
----------
train Loss: 1.4724 Acc: 0.6054 Balanced Acc: 0.6054
val Loss: 1.5850 Acc: 0.5965 Balanced Acc: 0.5985

Epoch 22/24
----------
train Loss: 1.4924 Acc: 0.5984 Balanced Acc: 0.5984
val Loss: 1.5549 Acc: 0.6001 Balanced Acc: 0.6025

Epoch 23/24
----------
train Loss: 1.4837 Acc: 0.6024 Balanced Acc: 0.6023
val Loss: 1.5685 Acc: 0.6048 Balanced Acc: 0.6070

Epoch 24/24
----------
train Loss: 1.4940 Acc: 0.6044 Balanced Acc: 0.6044
val Loss: 1.5603 Acc: 0.5951 Balanced Acc: 0.5969

Training complete in 103m 21s
Best val Balanced Acc: 0.607013
Validation:
Val_bal_acc: [0.08968076879735527, 0.31213279415100226, 0.4069562604849435, 0.453051938560342, 0.472590138208145, 0.5028382978796457, 0.5317721391260014, 0.5735317213407248, 0.577906363963592, 0.5857472969353347, 0.5897024084447545, 0.5966081423654214, 0.5956644148715092, 0.5926224462744822, 0.5999835710587896, 0.5987900999177928, 0.5936277903543743, 0.6000641656127731, 0.6017945275548254, 0.6006852266959979, 0.6008307458729817, 0.5985284285001989, 0.6024706756028659, 0.6070134125462578, 0.5969166466170727]
Val_acc: [tensor(0.0901, device='cuda:0', dtype=torch.float64), tensor(0.3091, device='cuda:0', dtype=torch.float64), tensor(0.4033, device='cuda:0', dtype=torch.float64), tensor(0.4487, device='cuda:0', dtype=torch.float64), tensor(0.4674, device='cuda:0', dtype=torch.float64), tensor(0.4997, device='cuda:0', dtype=torch.float64), tensor(0.5302, device='cuda:0', dtype=torch.float64), tensor(0.5713, device='cuda:0', dtype=torch.float64), tensor(0.5761, device='cuda:0', dtype=torch.float64), tensor(0.5827, device='cuda:0', dtype=torch.float64), tensor(0.5884, device='cuda:0', dtype=torch.float64), tensor(0.5941, device='cuda:0', dtype=torch.float64), tensor(0.5934, device='cuda:0', dtype=torch.float64), tensor(0.5910, device='cuda:0', dtype=torch.float64), tensor(0.5975, device='cuda:0', dtype=torch.float64), tensor(0.5970, device='cuda:0', dtype=torch.float64), tensor(0.5918, device='cuda:0', dtype=torch.float64), tensor(0.5979, device='cuda:0', dtype=torch.float64), tensor(0.5999, device='cuda:0', dtype=torch.float64), tensor(0.5986, device='cuda:0', dtype=torch.float64), tensor(0.5982, device='cuda:0', dtype=torch.float64), tensor(0.5965, device='cuda:0', dtype=torch.float64), tensor(0.6001, device='cuda:0', dtype=torch.float64), tensor(0.6048, device='cuda:0', dtype=torch.float64), tensor(0.5951, device='cuda:0', dtype=torch.float64)]
Val_loss: [4.642582161903052, 3.008300892846684, 2.543128395722494, 2.2457721644860116, 2.141029157678053, 1.9771711307432802, 1.8535395015878515, 1.6776902833795975, 1.6553863007157352, 1.6181156232845055, 1.6033149104633535, 1.600338350596986, 1.6140961803915093, 1.5945481451779275, 1.5788085153059566, 1.5685032791707858, 1.5891489231220228, 1.5842128577708046, 1.5634524736258957, 1.5650913596029812, 1.5535235676141619, 1.5850259238866267, 1.5548963648352925, 1.5684679735080307, 1.5603183085728316]
Training:
Train_bal_acc: [0.016350574712643676, 0.12057471264367838, 0.27071839080459775, 0.34745977011494256, 0.412206896551724, 0.4577356321839078, 0.48514367816091947, 0.54464367816092, 0.5758103448275865, 0.579051724137931, 0.586206896551724, 0.5783563218390805, 0.5886896551724139, 0.5985402298850573, 0.590879310344828, 0.5960459770114944, 0.6089195402298855, 0.605045977011494, 0.6090574712643678, 0.6130287356321843, 0.5998563218390801, 0.6053850574712643, 0.5984022988505752, 0.6022988505747126, 0.6043850574712644]
Train_acc: [tensor(0.0163, device='cuda:0', dtype=torch.float64), tensor(0.1206, device='cuda:0', dtype=torch.float64), tensor(0.2708, device='cuda:0', dtype=torch.float64), tensor(0.3475, device='cuda:0', dtype=torch.float64), tensor(0.4122, device='cuda:0', dtype=torch.float64), tensor(0.4578, device='cuda:0', dtype=torch.float64), tensor(0.4852, device='cuda:0', dtype=torch.float64), tensor(0.5447, device='cuda:0', dtype=torch.float64), tensor(0.5759, device='cuda:0', dtype=torch.float64), tensor(0.5791, device='cuda:0', dtype=torch.float64), tensor(0.5863, device='cuda:0', dtype=torch.float64), tensor(0.5784, device='cuda:0', dtype=torch.float64), tensor(0.5888, device='cuda:0', dtype=torch.float64), tensor(0.5986, device='cuda:0', dtype=torch.float64), tensor(0.5909, device='cuda:0', dtype=torch.float64), tensor(0.5961, device='cuda:0', dtype=torch.float64), tensor(0.6089, device='cuda:0', dtype=torch.float64), tensor(0.6051, device='cuda:0', dtype=torch.float64), tensor(0.6091, device='cuda:0', dtype=torch.float64), tensor(0.6131, device='cuda:0', dtype=torch.float64), tensor(0.5999, device='cuda:0', dtype=torch.float64), tensor(0.6054, device='cuda:0', dtype=torch.float64), tensor(0.5984, device='cuda:0', dtype=torch.float64), tensor(0.6024, device='cuda:0', dtype=torch.float64), tensor(0.6044, device='cuda:0', dtype=torch.float64)]
Train_loss: [5.212049501118041, 4.050038076377846, 3.0485581790044542, 2.6206602802823933, 2.306305157251266, 2.1185358435382913, 1.982627365841323, 1.7152484112037274, 1.6181636536801542, 1.6105412851781657, 1.570710003753881, 1.5785250298611753, 1.5284112151320632, 1.514031837334186, 1.5029275987917556, 1.5083360957987992, 1.4766652780570386, 1.4790921621733122, 1.460662325263063, 1.499886472065289, 1.4854187706847728, 1.4724406302671333, 1.4924111092213912, 1.4837164638795812, 1.4939895520657351]
Pruning 60% ... with Feature Extraction
Epoch 0/24
----------
train Loss: 5.1743 Acc: 0.0242 Balanced Acc: 0.0242
val Loss: 4.8411 Acc: 0.1096 Balanced Acc: 0.1105

Epoch 1/24
----------
train Loss: 4.7049 Acc: 0.0858 Balanced Acc: 0.0857
val Loss: 4.4533 Acc: 0.1819 Balanced Acc: 0.1840

Epoch 2/24
----------
train Loss: 4.3501 Acc: 0.1386 Balanced Acc: 0.1387
val Loss: 4.1491 Acc: 0.2380 Balanced Acc: 0.2406

Epoch 3/24
----------
train Loss: 4.0714 Acc: 0.1803 Balanced Acc: 0.1803
val Loss: 3.9279 Acc: 0.2610 Balanced Acc: 0.2638

Epoch 4/24
----------
train Loss: 3.8637 Acc: 0.2014 Balanced Acc: 0.2013
val Loss: 3.7476 Acc: 0.2839 Balanced Acc: 0.2856

Epoch 5/24
----------
train Loss: 3.6982 Acc: 0.2372 Balanced Acc: 0.2372
val Loss: 3.6014 Acc: 0.3055 Balanced Acc: 0.3085

Epoch 6/24
----------
train Loss: 3.5385 Acc: 0.2462 Balanced Acc: 0.2463
val Loss: 3.4640 Acc: 0.3141 Balanced Acc: 0.3163

Epoch 7/24
----------
train Loss: 3.4588 Acc: 0.2609 Balanced Acc: 0.2608
val Loss: 3.4555 Acc: 0.3300 Balanced Acc: 0.3323

Epoch 8/24
----------
train Loss: 3.4415 Acc: 0.2708 Balanced Acc: 0.2707
val Loss: 3.4576 Acc: 0.3205 Balanced Acc: 0.3241

Epoch 9/24
----------
train Loss: 3.4548 Acc: 0.2633 Balanced Acc: 0.2632
val Loss: 3.4457 Acc: 0.3265 Balanced Acc: 0.3305

Epoch 10/24
----------
train Loss: 3.4247 Acc: 0.2801 Balanced Acc: 0.2800
val Loss: 3.4343 Acc: 0.3290 Balanced Acc: 0.3320

Epoch 11/24
----------
train Loss: 3.3988 Acc: 0.2806 Balanced Acc: 0.2805
val Loss: 3.4343 Acc: 0.3217 Balanced Acc: 0.3238

Epoch 12/24
----------
train Loss: 3.4104 Acc: 0.2739 Balanced Acc: 0.2739
val Loss: 3.4332 Acc: 0.3333 Balanced Acc: 0.3356

Epoch 13/24
----------
train Loss: 3.3837 Acc: 0.2784 Balanced Acc: 0.2785
val Loss: 3.3946 Acc: 0.3347 Balanced Acc: 0.3374

Epoch 14/24
----------
train Loss: 3.4126 Acc: 0.2754 Balanced Acc: 0.2754
val Loss: 3.4012 Acc: 0.3309 Balanced Acc: 0.3339

Epoch 15/24
----------
train Loss: 3.3735 Acc: 0.2771 Balanced Acc: 0.2771
val Loss: 3.4128 Acc: 0.3281 Balanced Acc: 0.3302

Epoch 16/24
----------
train Loss: 3.3813 Acc: 0.2819 Balanced Acc: 0.2819
val Loss: 3.3964 Acc: 0.3302 Balanced Acc: 0.3330

Epoch 17/24
----------
train Loss: 3.3847 Acc: 0.2801 Balanced Acc: 0.2801
val Loss: 3.3972 Acc: 0.3333 Balanced Acc: 0.3375

Epoch 18/24
----------
train Loss: 3.3534 Acc: 0.2881 Balanced Acc: 0.2881
val Loss: 3.3923 Acc: 0.3404 Balanced Acc: 0.3428

Epoch 19/24
----------
train Loss: 3.3790 Acc: 0.2783 Balanced Acc: 0.2783
val Loss: 3.4061 Acc: 0.3297 Balanced Acc: 0.3316

Epoch 20/24
----------
train Loss: 3.3778 Acc: 0.2824 Balanced Acc: 0.2824
val Loss: 3.4089 Acc: 0.3295 Balanced Acc: 0.3328

Epoch 21/24
----------
train Loss: 3.3879 Acc: 0.2784 Balanced Acc: 0.2784
val Loss: 3.3924 Acc: 0.3369 Balanced Acc: 0.3411

Epoch 22/24
----------
train Loss: 3.3522 Acc: 0.2958 Balanced Acc: 0.2958
val Loss: 3.3946 Acc: 0.3300 Balanced Acc: 0.3333

Epoch 23/24
----------
train Loss: 3.3937 Acc: 0.2786 Balanced Acc: 0.2786
val Loss: 3.4137 Acc: 0.3350 Balanced Acc: 0.3372

Epoch 24/24
----------
train Loss: 3.3731 Acc: 0.2875 Balanced Acc: 0.2874
val Loss: 3.4145 Acc: 0.3322 Balanced Acc: 0.3343

Training complete in 105m 8s
Best val Balanced Acc: 0.342813
Validation:
Val_bal_acc: [0.11051996674384831, 0.18396203400233638, 0.24057286694531238, 0.26376200720979015, 0.28561148975640416, 0.30854890480328423, 0.31633541811374605, 0.33234657985020966, 0.3241321444307318, 0.33045285420425485, 0.33198544413639497, 0.3237944094360099, 0.335642286600564, 0.3373974685450856, 0.33387753252998276, 0.3301562571786475, 0.33301094495647876, 0.3375383957103361, 0.34281314314479316, 0.3316009915498988, 0.33276006501294525, 0.3410550245241521, 0.333274304092382, 0.3372077772755197, 0.3343373278986983]
Val_acc: [tensor(0.1096, device='cuda:0', dtype=torch.float64), tensor(0.1819, device='cuda:0', dtype=torch.float64), tensor(0.2380, device='cuda:0', dtype=torch.float64), tensor(0.2610, device='cuda:0', dtype=torch.float64), tensor(0.2839, device='cuda:0', dtype=torch.float64), tensor(0.3055, device='cuda:0', dtype=torch.float64), tensor(0.3141, device='cuda:0', dtype=torch.float64), tensor(0.3300, device='cuda:0', dtype=torch.float64), tensor(0.3205, device='cuda:0', dtype=torch.float64), tensor(0.3265, device='cuda:0', dtype=torch.float64), tensor(0.3290, device='cuda:0', dtype=torch.float64), tensor(0.3217, device='cuda:0', dtype=torch.float64), tensor(0.3333, device='cuda:0', dtype=torch.float64), tensor(0.3347, device='cuda:0', dtype=torch.float64), tensor(0.3309, device='cuda:0', dtype=torch.float64), tensor(0.3281, device='cuda:0', dtype=torch.float64), tensor(0.3302, device='cuda:0', dtype=torch.float64), tensor(0.3333, device='cuda:0', dtype=torch.float64), tensor(0.3404, device='cuda:0', dtype=torch.float64), tensor(0.3297, device='cuda:0', dtype=torch.float64), tensor(0.3295, device='cuda:0', dtype=torch.float64), tensor(0.3369, device='cuda:0', dtype=torch.float64), tensor(0.3300, device='cuda:0', dtype=torch.float64), tensor(0.3350, device='cuda:0', dtype=torch.float64), tensor(0.3322, device='cuda:0', dtype=torch.float64)]
Val_loss: [4.841137687872721, 4.453293808913042, 4.149068500570153, 3.927938690916357, 3.747597177233579, 3.6014336899390993, 3.4640147999561695, 3.4554651217251595, 3.4575538832770163, 3.445690011171625, 3.4343227023373895, 3.4343298547301657, 3.4332174568452793, 3.3945847704857433, 3.4011927825399706, 3.412791324476559, 3.3964435999912768, 3.397215423725538, 3.3923385386060425, 3.40606988682021, 3.4089314683289373, 3.392397634975656, 3.39460205784904, 3.4137137091731793, 3.414548358713136]
Training:
Train_bal_acc: [0.024189655172413786, 0.08574712643678156, 0.13867241379310372, 0.1803390804597703, 0.2013160919540233, 0.23715517241379327, 0.24626436781609212, 0.2608448275862071, 0.27067816091954017, 0.2632356321839082, 0.280028735632184, 0.2805172413793102, 0.2738563218390806, 0.27845402298850586, 0.27541954022988513, 0.2770689655172415, 0.281919540229885, 0.28009195402298853, 0.288080459770115, 0.2782816091954024, 0.2824080459770114, 0.2784080459770115, 0.29576436781609206, 0.27858045977011486, 0.2874137931034484]
Train_acc: [tensor(0.0242, device='cuda:0', dtype=torch.float64), tensor(0.0858, device='cuda:0', dtype=torch.float64), tensor(0.1386, device='cuda:0', dtype=torch.float64), tensor(0.1803, device='cuda:0', dtype=torch.float64), tensor(0.2014, device='cuda:0', dtype=torch.float64), tensor(0.2372, device='cuda:0', dtype=torch.float64), tensor(0.2462, device='cuda:0', dtype=torch.float64), tensor(0.2609, device='cuda:0', dtype=torch.float64), tensor(0.2708, device='cuda:0', dtype=torch.float64), tensor(0.2633, device='cuda:0', dtype=torch.float64), tensor(0.2801, device='cuda:0', dtype=torch.float64), tensor(0.2806, device='cuda:0', dtype=torch.float64), tensor(0.2739, device='cuda:0', dtype=torch.float64), tensor(0.2784, device='cuda:0', dtype=torch.float64), tensor(0.2754, device='cuda:0', dtype=torch.float64), tensor(0.2771, device='cuda:0', dtype=torch.float64), tensor(0.2819, device='cuda:0', dtype=torch.float64), tensor(0.2801, device='cuda:0', dtype=torch.float64), tensor(0.2881, device='cuda:0', dtype=torch.float64), tensor(0.2783, device='cuda:0', dtype=torch.float64), tensor(0.2824, device='cuda:0', dtype=torch.float64), tensor(0.2784, device='cuda:0', dtype=torch.float64), tensor(0.2958, device='cuda:0', dtype=torch.float64), tensor(0.2786, device='cuda:0', dtype=torch.float64), tensor(0.2875, device='cuda:0', dtype=torch.float64)]
Train_loss: [5.1742686533235815, 4.704943942196336, 4.350140652260384, 4.071428866158894, 3.8636766075093547, 3.6981652012895974, 3.5384992778321127, 3.458783242954665, 3.441540679654798, 3.4548256801373567, 3.424697813130157, 3.3988462772853065, 3.4103615946637658, 3.3837389463100744, 3.4125517074927356, 3.3734764313753502, 3.381338533736246, 3.384651686853276, 3.3533831734159287, 3.379029927748539, 3.3778260630052968, 3.3878699313969785, 3.352230746466834, 3.3936898567535736, 3.3731338189449316]